{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense, Layer, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "rn.seed(123)\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.keras.backend.shape(z_mean)[0]\n",
    "    dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim), seed=123)\n",
    "    \n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50\n",
    "\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "NUM_CLASSES = y_train_one_hot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(16), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "tfpl.MultivariateNormalTriL.params_size(encoded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tfpl = tfp.layers\n",
    "# tfkl = tf.keras.layers\n",
    "# base_depth = 100\n",
    "# encoded_size = 16\n",
    "# encoder = tf.keras.Sequential([\n",
    "#     tfkl.InputLayer(input_shape=(28,28,1)),\n",
    "#     tfkl.Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n",
    "#     tfkl.Conv2D(base_depth, 5, strides=1,\n",
    "#                 padding='same', activation=tf.nn.leaky_relu),\n",
    "#     tfkl.Conv2D(base_depth, 5, strides=2,\n",
    "#                 padding='same', activation=tf.nn.leaky_relu),\n",
    "#     tfkl.Conv2D(2 * base_depth, 5, strides=1,\n",
    "#                 padding='same', activation=tf.nn.leaky_relu),\n",
    "#     tfkl.Conv2D(2 * base_depth, 5, strides=2,\n",
    "#                 padding='same', activation=tf.nn.leaky_relu),\n",
    "#     tfkl.Conv2D(4 * encoded_size, 7, strides=1,\n",
    "#                 padding='valid', activation=tf.nn.leaky_relu),\n",
    "#     tfkl.Flatten(),\n",
    "#     tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "#                activation=None,name='dense'),\n",
    "#     tfpl.MultivariateNormalTriL(\n",
    "#         encoded_size,\n",
    "#         activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=1.0)),\n",
    "# ])\n",
    "\n",
    "# encoder.get_layer('dense').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = Input(shape=(x_train.shape[1],)) \n",
    "cond = Input(shape=(NUM_CLASSES,))\n",
    "\n",
    "inputs = Concatenate()([X_input, cond])\n",
    "\n",
    "encoder_h = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "#mu = Dense(latent_dim, activation='linear', name='mu')(encoder_h)\n",
    "#l_sigma = Dense(latent_dim, activation='linear')(encoder_h)\n",
    "\n",
    "#z = Lambda(sampling, output_shape = (latent_dim, ))([mu, l_sigma])\n",
    "\n",
    "zc = Concatenate(name='z_condition')([z, cond])\n",
    "\n",
    "decoder_hidden = Dense(intermediate_dim, activation='relu', name='decoder_hidden')\n",
    "decoder_output = Dense(original_dim, activation='sigmoid', name='decoder_output')\n",
    "\n",
    "decoder_intermediate = decoder_hidden(zc)\n",
    "decoder_output_layer = decoder_output(decoder_intermediate)\n",
    "\n",
    "cvae = Model([X_input, cond], [decoder_output_layer,mu, l_sigma])\n",
    "encoder = Model(cvae.inputs, cvae.get_layer('mu').output)\n",
    "\n",
    "\n",
    "decoder_z = Input(shape=(latent_dim,))\n",
    "decoder_cond = Input(shape=(NUM_CLASSES,))\n",
    "\n",
    "decoder_input = Concatenate()([decoder_z, decoder_cond])\n",
    "\n",
    "slice_decoder_hidden = decoder_hidden(decoder_input)\n",
    "slice_decoder_output = decoder_output(slice_decoder_hidden)\n",
    "decoder = Model([decoder_z, decoder_cond], slice_decoder_output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_prior = tfd.MultivariateNormalDiag(loc=tf.zeros([latent_dim]),\n",
    "        scale_identity_multiplier=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss_metric = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for x_batch_train, y_batch_train in train_dataset:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            reconstructed, z_mu, z_sigma = cvae((x_batch_train, y_batch_train))\n",
    "            \n",
    "            mse_loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            mse_loss *= original_dim\n",
    "\n",
    "            kl_loss = -0.5 * tf.reduce_sum(1 + z_sigma - tf.square(z_mu) - tf.exp(z_sigma), axis=-1)\n",
    "            \n",
    "            loss = tf.reduce_mean(mse_loss + kl_loss)\n",
    "\n",
    "        grads = tape.gradient(loss, cvae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, cvae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "    if not epoch % 10:\n",
    "        \n",
    "        print('Epoch %s: mean loss = %s' % (epoch, loss_metric.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
