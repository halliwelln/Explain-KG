{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense, Layer, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "rn.seed(123)\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# def sampling(args):\n",
    "#     \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "#     # Arguments\n",
    "#         args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "#     # Returns\n",
    "#         z (tensor): sampled latent vector\n",
    "#     \"\"\"\n",
    "\n",
    "#     z_mean, z_log_var = args\n",
    "#     batch = tf.keras.backend.shape(z_mean)[0]\n",
    "#     dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "#     # by default, random_normal has mean = 0 and std = 1.0\n",
    "#     epsilon = tf.keras.backend.random_normal(shape=(batch, dim), seed=123)\n",
    "#     return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.keras.backend.shape(z_mean)[0]\n",
    "    dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim), seed=123)\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = y_train_one_hot.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'decoder_output_15/Identity:0' shape=(None, 784) dtype=float32>,\n",
       " <tf.Tensor 'mu_24/Identity:0' shape=(None, 2) dtype=float32>,\n",
       " <tf.Tensor 'dense_103/Identity:0' shape=(None, 2) dtype=float32>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input = Input(shape=(x_train.shape[1],)) \n",
    "cond = Input(shape=(NUM_CLASSES,))\n",
    "\n",
    "inputs = Concatenate()([X_input, cond])\n",
    "\n",
    "encoder_h = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "mu = Dense(latent_dim, activation='linear', name='mu')(encoder_h)\n",
    "l_sigma = Dense(latent_dim, activation='linear')(encoder_h)\n",
    "\n",
    "z = Lambda(sampling, output_shape = (latent_dim, ))([mu, l_sigma])\n",
    "\n",
    "zc = Concatenate(name='z_condition')([z, cond])\n",
    "\n",
    "#decoder_hidden = Dense(intermediate_dim, activation='relu', name='decoder_hidden')(zc)\n",
    "#decoder_output = Dense(original_dim, activation='sigmoid', name='decoder_output')(decoder_hidden)\n",
    "decoder_hidden = Dense(intermediate_dim, activation='relu', name='decoder_hidden')\n",
    "decoder_output = Dense(original_dim, activation='sigmoid', name='decoder_output')\n",
    "\n",
    "decoder_intermediate = decoder_hidden(zc)\n",
    "decoder_output_layer = decoder_output(decoder_intermediate)\n",
    "    \n",
    "cvae = Model([X_input, cond], [decoder_output_layer,mu, l_sigma])\n",
    "encoder = Model(cvae.inputs, cvae.get_layer('mu').output)\n",
    "#decoder = Model(cvae.get_layer('z_condition').input,cvae.output)\n",
    "\n",
    "decoder_input = Input(shape=(NUM_CLASSES+latent_dim,),name='decoder_input')\n",
    "slice_decoder_hidden = decoder_hidden(decoder_input)\n",
    "slice_decoder_output = decoder_output(slice_decoder_hidden)\n",
    "decoder = Model(decoder_input, slice_decoder_output)\n",
    "\n",
    "#cvae.get_layer('z_condition').input[0]\n",
    "#decoder\n",
    "#z_cond = Concatenate(axis=1)([z, cond])\n",
    "\n",
    "# X_input = Input(shape=input_shape, name='X_input')\n",
    "# cond_input = Input(shape=(y_train_one_hot.shape[1],), name='cond_input')\n",
    "\n",
    "# inputs = Concatenate(axis=1)([X_input, cond_input])\n",
    "\n",
    "# x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "# z_mean_layer = Dense(latent_dim, name='z_mean')(x)\n",
    "# z_log_var_layer = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# z_layer = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean_layer, z_log_var_layer])\n",
    "# z_cond = Concatenate(axis=1)([z_layer, cond_input])\n",
    "# encoder = Model([X_input, cond_input], [z_mean_layer, z_log_var_layer, z_cond], name='encoder')\n",
    "\n",
    "# latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "# decoder_x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "# outputs = Dense(original_dim, activation='sigmoid')(decoder_x)\n",
    "\n",
    "# decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "# outputs = decoder(encoder([X_input, cond_input])[2])\n",
    "# vae = Model([X_input, cond_input], [z_mean_layer, z_log_var_layer, outputs], name='vae_mlp')\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(128)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    loss_metric = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for x_batch_train in train_dataset:\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            z_mean, z_log_var, reconstructed = vae(x_batch_train)\n",
    "            \n",
    "            mse_loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            mse_loss *= original_dim\n",
    "\n",
    "            kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "            \n",
    "            loss = tf.reduce_mean(mse_loss + kl_loss)\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "    if not epoch % 10:\n",
    "        \n",
    "        print('Epoch %s: mean loss = %s' % (epoch, loss_metric.result()))\n",
    "\n",
    "\n",
    "#z_mean_test, _, _ = encoder(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 10))\n",
    "# plt.scatter(z_mean_test[:, 0], z_mean_test[:, 1], c=y_test)\n",
    "# plt.colorbar()\n",
    "# plt.xlabel(\"z[0]\")\n",
    "# plt.ylabel(\"z[1]\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 30\n",
    "# digit_size = 28\n",
    "# figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# # linearly spaced coordinates corresponding to the 2D plot\n",
    "# # of digit classes in the latent space\n",
    "# grid_x = np.linspace(-4, 4, n)\n",
    "# grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "# for i, yi in enumerate(grid_y):\n",
    "#     for j, xi in enumerate(grid_x):\n",
    "#         z_sample = np.array([[xi, yi]])\n",
    "#         x_decoded = decoder.predict(z_sample)\n",
    "#         digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "#         figure[i * digit_size: (i + 1) * digit_size,\n",
    "#                j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# start_range = digit_size // 2\n",
    "# end_range = (n - 1) * digit_size + start_range + 1\n",
    "# pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "# sample_range_x = np.round(grid_x, 1)\n",
    "# sample_range_y = np.round(grid_y, 1)\n",
    "# plt.xticks(pixel_range, sample_range_x)\n",
    "# plt.yticks(pixel_range, sample_range_y)\n",
    "# plt.xlabel(\"z[0]\")\n",
    "# plt.ylabel(\"z[1]\")\n",
    "# plt.imshow(figure, cmap='Greys_r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
