{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_loss(loss, smap_loss):\n",
    "\n",
    "    total_loss = (loss) + (smap_loss) + \\\n",
    "                ((loss) * (1-smap_loss)) + ((smap_loss) * (1-loss))\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r1(loss, smap_loss):\n",
    "    return ((loss) * (1-smap_loss))\n",
    "def r2(loss, smap_loss):\n",
    "    return ((smap_loss) * (1-loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb15k_237 = np.load('./data/fb15k_237.npz', allow_pickle=True)\n",
    "# fb_train = fb15k_237['train']\n",
    "# fb_valid = fb15k_237['valid']\n",
    "# fb_test = fb15k_237['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# data = np.load('./data/human_data.npz')\n",
    "# train = data['X']\n",
    "\n",
    "# g=Graph()\n",
    "# g.parse(\"../CORESE-DATA/human-data.rdf\", format=\"xml\")\n",
    "\n",
    "# triples = []\n",
    "\n",
    "# for i,j,k in g:\n",
    "    \n",
    "#     head = str(i).split('#')\n",
    "#     rel = str(j).split('#')\n",
    "#     tail = str(k).split('#')\n",
    "    \n",
    "#     if head[0] == 'http://www.inria.fr/2015/humans-instances':\n",
    "        \n",
    "#         triples.append((head[-1], rel[-1], tail[-1]))\n",
    "\n",
    "triples = [('Eve', 'type', 'Lecturer'),\n",
    "           #('Eve', 'type', 'Person'), \n",
    "           ('Lecturer', 'subClassOf', 'Person'), \n",
    "           #('David', 'type', 'Person'),\n",
    "           ('David', 'type', 'Researcher'),\n",
    "           ('Researcher', 'subClassOf', 'Person'),\n",
    "           ('Flora', 'hasSpouse', 'Gaston'),\n",
    "           ('Gaston', 'type', 'Person'),\n",
    "           #('Flora', 'type', 'Person')\n",
    "          ]\n",
    "\n",
    "train = np.array(triples)\n",
    "\n",
    "entities = np.unique(np.concatenate((train[:,0], train[:,2]), axis=0)).tolist()\n",
    "relations = np.unqiue(train[:,1]).tolist()\n",
    "\n",
    "num_entities = len(entities)\n",
    "num_relations = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(num_entities)))\n",
    "rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "idx2rel = {idx:rel for rel,idx in rel2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.zeros(shape=(num_entities,num_entities,num_relations))\n",
    "\n",
    "for h,r,t in train:\n",
    "    \n",
    "    h_idx = entities.index(h)\n",
    "    r_idx = relations.index(r)\n",
    "    t_idx = entities.index(t)\n",
    "    \n",
    "    A[h_idx,t_idx,r_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2idx = []\n",
    "\n",
    "for head, rel, tail in train:\n",
    "    \n",
    "    head_idx = ent2idx[head]\n",
    "    tail_idx = ent2idx[tail]\n",
    "    rel_idx = rel2idx[rel]\n",
    "\n",
    "    train2idx.append([head_idx, rel_idx, tail_idx])\n",
    "    \n",
    "train2idx = np.array(train2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## valid2idx = []\n",
    "\n",
    "# for head, rel, tail in valid:\n",
    "    \n",
    "#     head_idx = ent2idx[head]\n",
    "#     tail_idx = ent2idx[tail]\n",
    "#     rel_idx = rel2idx[rel]\n",
    "\n",
    "#     valid2idx.append([head_idx, rel_idx, tail_idx])\n",
    "    \n",
    "# valid2idx = np.array(valid2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transE\n",
    "EMBEDDING_SIZE = 50\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 200\n",
    "MARGIN = 2\n",
    "SQRT_SIZE = 6 / np.sqrt(EMBEDDING_SIZE)\n",
    "\n",
    "pos_head_input = tf.keras.layers.Input(shape=(1,), name='pos_head_input')\n",
    "neg_head_input = tf.keras.layers.Input(shape=(1,), name='neg_head_input')\n",
    "pos_tail_input = tf.keras.layers.Input(shape=(1,), name='pos_tail_input')\n",
    "neg_tail_input = tf.keras.layers.Input(shape=(1,), name='neg_tail_input')\n",
    "relation_input = tf.keras.layers.Input(shape=(1,), name='relation_input')\n",
    "\n",
    "entity_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=num_entities,\n",
    "    output_dim=EMBEDDING_SIZE,\n",
    "    name='entity_embeddings',\n",
    "    embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "                                                               seed=tf.random.set_seed(SEED))\n",
    "    )\n",
    "\n",
    "relation_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=num_relations,\n",
    "    output_dim=EMBEDDING_SIZE,\n",
    "    name='relation_embeddings',\n",
    "    embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "                                                               seed=tf.random.set_seed(SEED)),\n",
    "    )\n",
    "\n",
    "pos_head_e = entity_embedding(pos_head_input)\n",
    "neg_head_e = entity_embedding(neg_head_input)\n",
    "pos_tail_e = entity_embedding(pos_tail_input)\n",
    "neg_tail_e = entity_embedding(neg_tail_input)\n",
    "rel_e = relation_embedding(relation_input)\n",
    "\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[\n",
    "        pos_head_input,\n",
    "        neg_head_input, \n",
    "        pos_tail_input, \n",
    "        neg_tail_input, \n",
    "        relation_input\n",
    "        ], \n",
    "    outputs=[\n",
    "        pos_head_e,\n",
    "        neg_head_e, \n",
    "        pos_tail_e, \n",
    "        neg_tail_e, \n",
    "        rel_e\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex\n",
    "# EMBEDDING_SIZE = 30\n",
    "# BATCH_SIZE = 3\n",
    "# NUM_EPOCHS = 200\n",
    "# MARGIN = 1\n",
    "# SQRT_SIZE = 6 / np.sqrt(EMBEDDING_SIZE)\n",
    "\n",
    "# real_head_input = tf.keras.layers.Input(shape=(1,), name='real_head_input')\n",
    "# img_head_input = tf.keras.layers.Input(shape=(1,), name='img_head_input')\n",
    "# real_tail_input = tf.keras.layers.Input(shape=(1,), name='real_tail_input')\n",
    "# img_tail_input = tf.keras.layers.Input(shape=(1,), name='img_tail_input')\n",
    "# real_rel_input = tf.keras.layers.Input(shape=(1,), name='real_rel_input')\n",
    "# img_rel_input = tf.keras.layers.Input(shape=(1,), name='img_rel_input')\n",
    "\n",
    "# real_entity_embeddings = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_entities,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='real_entity_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED))\n",
    "#     )\n",
    "\n",
    "# img_entity_embeddings = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_entities,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='img_entity_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED))\n",
    "#     )\n",
    "\n",
    "# real_relation_embedding = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_relations,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='real_relation_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED)),\n",
    "#     )\n",
    "\n",
    "# img_relation_embedding = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_relations,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='img_relation_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED)),\n",
    "#     )\n",
    "\n",
    "# real_head = real_entity_embeddings(real_head_input)\n",
    "# img_head = img_entity_embeddings(img_head_input)\n",
    "# real_tail = real_entity_embeddings(real_tail_input)\n",
    "# img_tail = img_entity_embeddings(img_tail_input)\n",
    "# real_rel = real_relation_embedding(real_rel_input)\n",
    "# img_rel = real_relation_embedding(img_rel_input)\n",
    "\n",
    "# model = tf.keras.models.Model(\n",
    "#     inputs=[\n",
    "#         real_head_input,\n",
    "#         img_head_input, \n",
    "#         real_tail_input, \n",
    "#         img_tail_input, \n",
    "#         real_rel_input,\n",
    "#         img_rel_input\n",
    "#         ], \n",
    "#     outputs=[\n",
    "#         real_head,\n",
    "#         img_head, \n",
    "#         real_tail, \n",
    "#         img_tail, \n",
    "#         real_rel,\n",
    "#         img_rel\n",
    "#         ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_triples(head, rel, tail, seed):\n",
    "    \n",
    "    cond = tf.random.uniform(head.shape, 0, 2, dtype=tf.int64, seed=seed) #1 means keep entity\n",
    "    rnd = tf.random.uniform(head.shape, 0, num_entities-1, dtype=tf.int64, seed=seed)\n",
    "    \n",
    "    neg_head = tf.where(cond == 1, head, rnd)\n",
    "    neg_tail = tf.where(cond == 1, rnd, tail)   \n",
    "    \n",
    "    return neg_head, neg_tail\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0], train2idx[:,1], train2idx[:,2])).batch(BATCH_SIZE)\n",
    "#train_data = train_data.shuffle(buffer_size=50000, seed=tf.random.set_seed(SEED)).batch(BATCH_SIZE)\n",
    "\n",
    "#exp_decay = tf.keras.optimizers.schedules.ExponentialDecay(.01, 1000, .05)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(h,r,t):\n",
    "    return tf.reduce_sum(tf.square(h + r - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "#     for head, rel, tail in train_data:\n",
    "                \n",
    "#         neg_head, neg_tail = get_negative_triples(head, rel, tail,seed=tf.random.set_seed(SEED))\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "            \n",
    "#             real_head_e,img_head_e, real_tail_e, img_tail_e, real_rel_e,img_rel_e = model([head, \n",
    "#                                                                            neg_head, tail, neg_tail, rel, rel])\n",
    "\n",
    "            \n",
    "#             dot1 = tf.reduce_sum(tf.multiply(real_rel_e, tf.multiply(real_head_e, real_tail_e)),1)\n",
    "#             dot2 = tf.reduce_sum(tf.multiply(real_rel_e, tf.multiply(img_head_e, img_tail_e)),1)\n",
    "#             dot3 = tf.reduce_sum(tf.multiply(img_rel_e, tf.multiply(real_head_e, img_tail_e)),1)\n",
    "#             dot4 = tf.reduce_sum(tf.multiply(img_rel_e, tf.multiply(img_head_e, real_tail_e)),1)\n",
    "            \n",
    "#             embedding_loss = tf.reduce_sum(dot1+dot2+dot3-dot4)\n",
    "\n",
    "#         grads = tape.gradient(embedding_loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "#     if not epoch % 10:\n",
    "        \n",
    "#         print('Current loss' , embedding_loss.numpy(),'at epoch', epoch)\n",
    "    \n",
    "#     losses.append(embedding_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    for head, rel, tail in train_data:\n",
    "                \n",
    "        neg_head, neg_tail = get_negative_triples(head, rel, tail,seed=tf.random.set_seed(SEED))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "                        \n",
    "            pos_head_e,neg_head_e, pos_tail_e, neg_tail_e, rel_e= model([head, \n",
    "                                                                           neg_head, tail, neg_tail, rel])\n",
    "            \n",
    "#             pos_head_0,neg_head_0, pos_tail_0, neg_tail_0, rel_0 = model([head[0], \n",
    "#                                                                           neg_head[0], tail[0], \n",
    "#                                                                           neg_tail[0], rel[0]])\n",
    "            \n",
    "#             pos_head_1,neg_head_1, pos_tail_1, neg_tail_1, rel_1 = model([head[1], \n",
    "#                                                                           neg_head[1], tail[1], neg_tail[1], rel[1]])\n",
    "            \n",
    "#             pos_head_2,neg_head_2, pos_tail_2, neg_tail_2, rel_2 = model([head[2], \n",
    "#                                                                           neg_head[2], tail[2], neg_tail[2], rel[2]])\n",
    "            \n",
    "            #pos = score(pos_head_0,rel_0, pos_tail_0) + \\\n",
    "            #score(pos_head_1,rel_1, pos_tail_1) #+score(pos_head_2,rel_2, pos_tail_2)\n",
    "            \n",
    "            #neg = score(neg_head_0,rel_0, neg_tail_0) + \\\n",
    "            #score(neg_head_1,rel_1, neg_tail_1) #+\\score(neg_head_2,rel_2, neg_tail_2)\n",
    "            \n",
    "            \n",
    "            pos = tf.reduce_sum(tf.square(pos_head_e + rel_e - pos_tail_e), axis=1)\n",
    "            neg = tf.reduce_sum(tf.square(neg_head_e + rel_e - neg_tail_e), axis=1)    \n",
    "            \n",
    "            embedding_loss = tf.reduce_sum(tf.maximum(pos - neg + MARGIN, 0))\n",
    "            #pred_loss = tf.reduce_sum(tf.maximum(pos - neg + MARGIN, 0))\n",
    "            \n",
    " \n",
    "        grads = tape.gradient(embedding_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "    if not epoch % 10:\n",
    "        \n",
    "        print('Current loss' , embedding_loss.numpy(),'at epoch', epoch)\n",
    "    \n",
    "    losses.append(embedding_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = np.concatenate((entities,relations))\n",
    "\n",
    "all_entities = model.get_layer('entity_embeddings').get_weights()[0]\n",
    "all_relations = model.get_layer('relation_embeddings').get_weights()[0]\n",
    "\n",
    "all_embeddings = np.concatenate((all_entities,all_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2d = PCA(n_components=2, random_state=SEED).fit_transform(all_embeddings)\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.scatter(embeddings_2d[:,0], embeddings_2d[:,1])\n",
    "for i, txt in enumerate(all_names):\n",
    "    ax.annotate(txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_names = list(all_names)\n",
    "all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_head = 'Eve'\n",
    "true_tail = 'Person'\n",
    "\n",
    "head_idx = all_names.index(true_head)\n",
    "tail_idx = all_names.index(true_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = -100000000\n",
    "min_idx = -100000000\n",
    "\n",
    "for rel in relations:\n",
    "    \n",
    "    rel_idx = all_names.index(rel)\n",
    "    \n",
    "    current_score = -score(all_embeddings[head_idx],\n",
    "                           all_embeddings[rel_idx],\n",
    "                           all_embeddings[tail_idx]).numpy()\n",
    "    \n",
    "    if current_score > min_score:\n",
    "        min_score = current_score\n",
    "        min_idx = rel_idx\n",
    "        \n",
    "    print(all_names[rel_idx], current_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_head, all_names[min_idx], true_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h,r,t in train:\n",
    "    \n",
    "#     h_idx = all_names.index(h)\n",
    "#     r_idx = all_names.index(r)\n",
    "#     t_idx = all_names.index(t)\n",
    "    \n",
    "#     print(h,r,t,-score(all_embeddings[h_idx], all_embeddings[r_idx], all_embeddings[t_idx]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find closest l2 triple\n",
    "eve = all_embeddings[head_idx]\n",
    "person = all_embeddings[tail_idx]\n",
    "rel = all_embeddings[min_idx]\n",
    "\n",
    "for h,r,t in train:\n",
    "    \n",
    "    h_idx = all_names.index(h)\n",
    "    r_idx = all_names.index(r)\n",
    "    t_idx = all_names.index(t)\n",
    "    \n",
    "    l2 = np.sqrt(np.sum((eve - all_embeddings[h_idx])**2 + (rel- all_embeddings[r_idx])**2\n",
    "                + (person-all_embeddings[t_idx])**2))\n",
    "    \n",
    "    print(h,r,t,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eve = all_embeddings[all_names.index('Eve')]\n",
    "person = all_embeddings[all_names.index('Person')]\n",
    "rel = all_embeddings[all_names.index('type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx, rel_idx, tail_idx = train2idx[0]\n",
    "\n",
    "# def get_grad(head_idx, tail_idx, rel_idx, A):\n",
    "    \n",
    "#     with tf.GradientTape(persistent=True) as g:\n",
    "\n",
    "# #         head_idx = tf.convert_to_tensor(head_idx)\n",
    "# #         tail_idx = tf.convert_to_tensor(tail_idx)\n",
    "# #         rel_idx = tf.convert_to_tensor(rel_idx)    \n",
    "#         A = tf.convert_to_tensor(A)\n",
    "\n",
    "#         head = tf.argmax(A[:,tail_idx, rel_idx])\n",
    "#         tail = tf.argmax(A[head_idx,:, rel_idx])\n",
    "#         rel = tf.argmax(A[head_idx,tail_idx,:])\n",
    " \n",
    "#         head_e,_,tail_e,_,rel_e= model([head,head,tail,tail,rel])\n",
    "\n",
    "#         get_score = score(head_e,rel_e,tail_e)\n",
    "        \n",
    "        \n",
    "#     nabla = g.gradient(get_score, head_e)\n",
    "#     return nabla\n",
    "#get_grad(head_idx, tail_idx, rel_idx, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('entity2wikidata.json','r') as f:\n",
    "    \n",
    "#     entities_dict = json.load(f)\n",
    "\n",
    "# for k, d in entities_dict.items():\n",
    "    \n",
    "#     if 'France' in d['label']:\n",
    "        \n",
    "#         print(k)\n",
    "\n",
    "# embeddings = model.get_layer('entity_embeddings').get_weights()[0]\n",
    "# relations = model.get_layer('relation_embeddings').get_weights()[0]\n",
    "\n",
    "# paris = '/m/05qtj'\n",
    "# france = '/m/0f8l9c'\n",
    "\n",
    "# paris_idx = ent2idx[paris]\n",
    "# france_idx = ent2idx[france]\n",
    "# capital = rel2idx['/location/country/capital']\n",
    "\n",
    "# head, tail = embeddings[[paris_idx, france_idx], :]\n",
    "# rel = relations[capital]\n",
    "\n",
    "# #-np.linalg.norm((head+rel - tail),ord=2)\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# for i in range(len(relations)):\n",
    "    \n",
    "#     temp_rel = relations[i]\n",
    "    \n",
    "#     score = -np.linalg.norm((head+temp_rel - tail),ord=2)\n",
    "    \n",
    "#     scores.append(score)\n",
    "\n",
    "# idx2rel[np.argmax(scores)]\n",
    "\n",
    "# for i in np.argsort(scores)[-10:]:\n",
    "    \n",
    "#     print(idx2rel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
