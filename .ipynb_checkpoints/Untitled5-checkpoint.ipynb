{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import os\n",
    "import cne\n",
    "import maxent\n",
    "import utils\n",
    "import joblib\n",
    "import explaiNE\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "\n",
    "print(f'CPU count: {joblib.cpu_count()}')\n",
    "\n",
    "data = np.load(os.path.join('.','data','royalty_spouse.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['X_train']\n",
    "test = data['X_test']\n",
    "\n",
    "train_exp = data['train_exp']\n",
    "test_exp = data['test_exp']\n",
    "\n",
    "full_train = np.concatenate((train,train_exp.reshape(-1,3)), axis=0)\n",
    "\n",
    "entities = data['entities'].tolist()\n",
    "relations = data['relations'].tolist()\n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "train2idx = utils.array2idx(train,ent2idx,rel2idx)\n",
    "test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "\n",
    "data2idx = np.concatenate([train2idx,test2idx], axis=0)\n",
    "\n",
    "trainexp2idx = utils.array2idx(train_exp,ent2idx,rel2idx)\n",
    "testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)\n",
    "\n",
    "adjacency_data = np.concatenate((train,train_exp.reshape(-1,3)), axis=0)\n",
    "adjacency_data = np.concatenate((adjacency_data,test), axis=0)\n",
    "\n",
    "A = utils.get_adjacency_matrix(adjacency_data,entities,NUM_ENTITIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainexp2idx = np.concatenate([trainexp2idx[:,:,0],\n",
    "    trainexp2idx[:,:,2]],axis=1).reshape(-1,1,2)\n",
    "\n",
    "testexp2idx = np.concatenate([testexp2idx[:,:,0],\n",
    "    testexp2idx[:,:,2]],axis=1).reshape(-1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explaiNE import get_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = joblib.Parallel(n_jobs=-2, verbose=0)(\n",
    "    joblib.delayed(get_explanations)(\n",
    "        i,j,S1,S2,EMBEDDING_DIM,GAMMA,X,A,TOP_K,train2idx,HESSIANS,PROBS,seed=SEED\n",
    "        ) for i,_,j in train2idx[0:10]#test2idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = np.array(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip, trace = utils.parse_ttl(\n",
    "    file_name=os.path.join('.','data','traces','spouse.ttl'),\n",
    "    max_padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('.','data','royalty_uncle.npz'))\n",
    "\n",
    "train = data['X_train']\n",
    "test = data['X_test']\n",
    "\n",
    "train_exp = data['train_exp']\n",
    "test_exp = data['test_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Pōmare_III' 'parent' 'Pōmare_II']\n",
      " ['Pōmare_III' 'gender' '\"male\"@en']\n",
      " ['Pōmare_IV' 'parent' 'Pōmare_II']\n",
      " ['Teriitapunui_Pōmare' 'parent' 'Pōmare_IV']]\n"
     ]
    }
   ],
   "source": [
    "print(test_exp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[['Pōmare_IV', 'parent', 'Pōmare_II'],\n",
       "        ['Pōmare_III', 'parent', 'Pōmare_II'],\n",
       "        ['Teriitapunui_Pōmare', 'parent', 'Pōmare_IV'],\n",
       "        ['Pōmare_III', 'gender', '\"male\"@en']],\n",
       "\n",
       "       [['Pōmare_IV', 'parent', 'Pōmare_II'],\n",
       "        ['Pōmare_III', 'parent', 'Pōmare_II'],\n",
       "        ['Teriitapunui_Pōmare', 'parent', 'Pōmare_IV'],\n",
       "        ['Pōmare_III', 'gender', '\"male\"@en']]], dtype='<U19')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([['Pōmare_IV', 'parent', 'Pōmare_II'],['Pōmare_III', 'parent', 'Pōmare_II'],\n",
    "          ['Teriitapunui_Pōmare', 'parent', 'Pōmare_IV'],['Pōmare_III', 'gender', '\"male\"@en']])\n",
    "a = np.concatenate([a,a],axis=0).reshape(-1,4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.jaccard_score(test_exp[0:2],a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_split_no_unseen(X, exp,test_size=100, seed=0, allow_duplication=False, filtered_test_predicates=None):\n",
    "#     \"\"\"Split into train and test sets.\n",
    "#      This function carves out a test set that contains only entities\n",
    "#      and relations which also occur in the training set.\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     X : ndarray, size[n, 3]\n",
    "#         The dataset to split.\n",
    "#     test_size : int, float\n",
    "#         If int, the number of triples in the test set.\n",
    "#         If float, the percentage of total triples.\n",
    "#     seed : int\n",
    "#         A random seed used to split the dataset.\n",
    "#     allow_duplication: boolean\n",
    "#         Flag to indicate if the test set can contain duplicated triples.\n",
    "#     filtered_test_predicates: None, list\n",
    "#         If None, all predicate types will be considered for the test set.\n",
    "#         If list, only the predicate types in the list will be considered for\n",
    "#         the test set.\n",
    "#     Returns\n",
    "#     -------\n",
    "#     X_train : ndarray, size[n, 3]\n",
    "#         The training set.\n",
    "#     X_test : ndarray, size[n, 3]\n",
    "#         The test set.\n",
    "#     Examples\n",
    "#     --------\n",
    "#     >>> import numpy as np\n",
    "#     >>> from ampligraph.evaluation import train_test_split_no_unseen\n",
    "#     >>> # load your dataset to X\n",
    "#     >>> X = np.array([['a', 'y', 'b'],\n",
    "#     >>>               ['f', 'y', 'e'],\n",
    "#     >>>               ['b', 'y', 'a'],\n",
    "#     >>>               ['a', 'y', 'c'],\n",
    "#     >>>               ['c', 'y', 'a'],\n",
    "#     >>>               ['a', 'y', 'd'],\n",
    "#     >>>               ['c', 'y', 'd'],\n",
    "#     >>>               ['b', 'y', 'c'],\n",
    "#     >>>               ['f', 'y', 'e']])\n",
    "#     >>> # if you want to split into train/test datasets\n",
    "#     >>> X_train, X_test = train_test_split_no_unseen(X, test_size=2)\n",
    "#     >>> X_train\n",
    "#     array([['a', 'y', 'b'],\n",
    "#         ['f', 'y', 'e'],\n",
    "#         ['b', 'y', 'a'],\n",
    "#         ['c', 'y', 'a'],\n",
    "#         ['c', 'y', 'd'],\n",
    "#         ['b', 'y', 'c'],\n",
    "#         ['f', 'y', 'e']], dtype='<U1')\n",
    "#     >>> X_test\n",
    "#     array([['a', 'y', 'c'],\n",
    "#         ['a', 'y', 'd']], dtype='<U1')\n",
    "#     >>> # if you want to split into train/valid/test datasets, call it 2 times\n",
    "#     >>> X_train_valid, X_test = train_test_split_no_unseen(X, test_size=2)\n",
    "#     >>> X_train, X_valid = train_test_split_no_unseen(X_train_valid, test_size=2)\n",
    "#     >>> X_train\n",
    "#     array([['a', 'y', 'b'],\n",
    "#         ['b', 'y', 'a'],\n",
    "#         ['c', 'y', 'd'],\n",
    "#         ['b', 'y', 'c'],\n",
    "#         ['f', 'y', 'e']], dtype='<U1')\n",
    "#     >>> X_valid\n",
    "#     array([['f', 'y', 'e'],\n",
    "#         ['c', 'y', 'a']], dtype='<U1')\n",
    "#     >>> X_test\n",
    "#     array([['a', 'y', 'c'],\n",
    "#         ['a', 'y', 'd']], dtype='<U1')\n",
    "#     \"\"\"\n",
    "\n",
    "#     if type(test_size) is float:\n",
    "#         test_size = int(len(X) * test_size)\n",
    "\n",
    "#     rnd = np.random.RandomState(seed)\n",
    "\n",
    "#     subs, subs_cnt = np.unique(X[:, 0], return_counts=True)\n",
    "#     objs, objs_cnt = np.unique(X[:, 2], return_counts=True)\n",
    "#     rels, rels_cnt = np.unique(X[:, 1], return_counts=True)\n",
    "#     dict_subs = dict(zip(subs, subs_cnt))\n",
    "#     dict_objs = dict(zip(objs, objs_cnt))\n",
    "#     dict_rels = dict(zip(rels, rels_cnt))\n",
    "\n",
    "#     idx_test = np.array([], dtype=int)\n",
    "\n",
    "#     loop_count = 0\n",
    "#     tolerance = len(X) * 10\n",
    "#     # Set the indices of test set triples. If filtered, reduce candidate triples to certain predicate types.\n",
    "#     if filtered_test_predicates:\n",
    "#         test_triples_idx = np.where(np.isin(X[:, 1], filtered_test_predicates))[0]\n",
    "#     else:\n",
    "#         test_triples_idx = np.arange(len(X))\n",
    "\n",
    "#     while idx_test.shape[0] < test_size:\n",
    "#         i = rnd.choice(test_triples_idx)\n",
    "#         if dict_subs[X[i, 0]] > 1 and dict_objs[X[i, 2]] > 1 and dict_rels[X[i, 1]] > 1:\n",
    "#             dict_subs[X[i, 0]] -= 1\n",
    "#             dict_objs[X[i, 2]] -= 1\n",
    "#             dict_rels[X[i, 1]] -= 1\n",
    "#             if allow_duplication:\n",
    "#                 idx_test = np.append(idx_test, i)\n",
    "#             else:\n",
    "#                 idx_test = np.unique(np.append(idx_test, i))\n",
    "\n",
    "#         loop_count += 1\n",
    "\n",
    "#         # in case can't find solution\n",
    "#         if loop_count == tolerance:\n",
    "#             if allow_duplication:\n",
    "#                 raise Exception(\"Cannot create a test split of the desired size. \"\n",
    "#                                 \"Some entities will not occur in both training and test set. \"\n",
    "#                                 \"Change seed values, remove filter on test predicates or set \"\n",
    "#                                 \"test_size to a smaller value.\")\n",
    "#             else:\n",
    "#                 raise Exception(\"Cannot create a test split of the desired size. \"\n",
    "#                                 \"Some entities will not occur in both training and test set. \"\n",
    "#                                 \"Set allow_duplication=True,\"\n",
    "#                                 \"change seed values, remove filter on test predicates or \"\n",
    "#                                 \"set test_size to a smaller value.\")\n",
    "\n",
    "\n",
    "#     idx = np.arange(len(X))\n",
    "#     idx_train = np.setdiff1d(idx, idx_test)\n",
    "\n",
    "#     return X[idx_train, :], X[idx_test, :], exp[idx_train,:], exp[idx_test,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
