{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "import utils\n",
    "import rdflib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rdflib.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/Users/nhalliwe/Downloads')\n",
    "\n",
    "for file in files:\n",
    "    if file.startswith('sparql'):\n",
    "        g.parse('/Users/nhalliwe/Downloads/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.parse('/Users/nhalliwe/Downloads/sparql-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g.serialize(destination='/Users/nhalliwe/Downloads/output', format='xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g=rdflib.Graph()\n",
    "#g.parse(\"/Users/nhalliwe/Downloads/sparql\",format=\"xml\")\n",
    "\n",
    "#write evaluation method down and compare to state of the art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = rdflib.Graph()\n",
    "g.parse(os.path.join('.','data','sparql'),format=\"xml\")\n",
    "\n",
    "traces = utils.parse_traces(os.path.join('.','data','traces','spouse.ttl'))\n",
    "\n",
    "full_royalty = []\n",
    "\n",
    "for subj,pred,obj in g:\n",
    "    \n",
    "    s = subj.split('/')[-1]\n",
    "    p = pred.split('/')[-1]\n",
    "    o = obj.split('/')[-1]\n",
    "    \n",
    "    if 'with_no_name_entry' in s or 'with_no_name_entry' in o:\n",
    "        continue\n",
    "        \n",
    "    full_royalty.append((s,p,o))\n",
    "\n",
    "royalty = []\n",
    "explanations = []\n",
    "\n",
    "for k in full_royalty:\n",
    "    if traces[k]:\n",
    "        royalty.append(k)\n",
    "        explanations.append(traces[k])\n",
    "\n",
    "royalty = np.array(royalty)\n",
    "explanations = np.array(explanations).reshape(-1,3)\n",
    "\n",
    "print(f\"number of total observations {len(full_royalty)}\")\n",
    "print(f\"number of observations with explanations {len(royalty)}\")\n",
    "\n",
    "X_train, X_test, train_exp, test_exp = train_test_split(royalty, explanations, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h,r,t in royalty:\n",
    "    if h == 'Kalanipauahi' or t == 'Kalanipauahi':\n",
    "        print(h,r,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h,r,t in explanations:\n",
    "    if h == 'Kalanipauahi' or t == 'Kalanipauahi':\n",
    "        print(h,r,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entities = np.unique(np.concatenate((full_train[:,0], full_train[:,2]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entities =  np.unique(np.concatenate((X_test[:,0],X_test[:,2],test_exp[:,0], test_exp[:,2]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_royalty = np.array(full_royalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_ent = np.unique(np.concatenate((full_royalty[:,0],full_royalty[:,2]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen = []\n",
    "for h,r,t in X_test:\n",
    "    if h not in train_entities or t not in train_entities:\n",
    "        unseen.append(True)\n",
    "    else:\n",
    "        unseen.append(False)\n",
    "        \n",
    "unseen = np.array(unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,X_test[unseen]),axis=0)\n",
    "train_exp = np.concatenate((train_exp,test_exp[unseen]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[~unseen]\n",
    "test_exp = test_exp[~unseen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('.','data','royalty.npz'))\n",
    "\n",
    "train = data['X_train']\n",
    "test = data['X_test']\n",
    "\n",
    "train_exp = data['train_exp']\n",
    "test_exp = data['test_exp']\n",
    "\n",
    "full_train = np.concatenate((train,train_exp), axis=0)\n",
    "\n",
    "entities = data['entities'].tolist()\n",
    "relations = data['relations'].tolist()\n",
    "\n",
    "num_entities = len(entities)\n",
    "num_relations = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(num_entities)))\n",
    "rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "idx2rel = {idx:rel for rel,idx in rel2idx.items()}\n",
    "\n",
    "train2idx = utils.array2idx(full_train,ent2idx,rel2idx)\n",
    "test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "\n",
    "#train_entities = np.unique(np.concatenate((full_train[:,0], full_train[:,2]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = 'spouse'\n",
    "min_score = -100000000\n",
    "min_idx = -100000000\n",
    "\n",
    "for rel in relations:\n",
    "    rel_idx =  rel2idx[rel]\n",
    "    relation_embedding = relation_embeddings[rel_idx]\n",
    "    \n",
    "    current_score = -score(head_embedding,\n",
    "                           relation_embedding,\n",
    "                           tail_embedding).numpy()\n",
    "    \n",
    "    if current_score > min_score:\n",
    "        min_score = current_score\n",
    "        min_idx = rel_idx\n",
    "        \n",
    "    print(all_names[rel_idx], current_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = np.load(os.path.join('.','data','transE_embeddings.npz'))['entity_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.tensor([entity_embeddings[ent2idx[h]] for h,_,_ in full_train])\n",
    "y = torch.tensor([(rel2idx[r]) for _,r,_ in full_train])\n",
    "ents = np.array([(ent2idx[h],ent2idx[t]) for h,_,t in full_train]).T\n",
    "ents_flipped = np.stack((ents[1,:], ents[0,:]))\n",
    "edge_index = torch.tensor(np.concatenate((ents,ents_flipped), axis=1), dtype=torch.long).contiguous()\n",
    "\n",
    "\n",
    "x, edge_index = data.x, data.edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacency_data = np.concatenate((full_train,test,test_exp), axis=0)\n",
    "# A = utils.get_adjacency_matrix(adjacency_data,entities,num_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "X_test = torch.tensor([entity_embeddings[ent2idx[h]] for h,_,_ in test])\n",
    "y_test = torch.tensor([(rel2idx[r]) for _,r,_ in test])\n",
    "test_ents = np.array([(ent2idx[h],ent2idx[t]) for h,_,t in test]).T\n",
    "test_ents_flipped = np.stack((test_ents[1,:], test_ents[0,:]))\n",
    "test_edge_index = torch.tensor(np.concatenate((test_ents,test_ents_flipped), axis=1), dtype=torch.long)\n",
    "test_data = Data(x_test=X_test,y_test=y_test,test_edge_index=test_edge_index,num_classes=num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row,col=A.nonzero()\n",
    "# points = []\n",
    "# i=10\n",
    "# for idx,l in enumerate(row):\n",
    "    \n",
    "#     if l == 10:\n",
    "#         points.append([row[idx],col[idx]])\n",
    "#         print(row[idx],col[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_dist = []\n",
    "\n",
    "# for i,_,j in test2idx:\n",
    "    \n",
    "#     row,col = A.nonzero()\n",
    "\n",
    "#     points = []\n",
    "\n",
    "#     for idx, l in enumerate(row):\n",
    "#         if l == i:\n",
    "#             if (i,j) != (row[idx],col[idx]):\n",
    "#                 points.append([row[idx],col[idx]])\n",
    "#                 #print(i,j,row[idx],col[idx])\n",
    "#     points_dist.append(len(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reflexive (head and tail are the same, flipping head and tail triple still true): related by blood, same nationality as\n",
    "#Irreflexive (head and tail can't be the same, flipping head and tail, triple is false): parent, uncle/aunt,spouse brother/sister\n",
    "#Symmetric (fliping head and tail, still true): sibling, spouse\n",
    "#Antisymmetric (fliping head and tail makes triple false): uncle/aunt?, brother/sister\n",
    "#Transitive: predecessor, ancestor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for subj, pred, obj in g:\n",
    "    \n",
    "    #print(subj,subj.split('/')[-1])\n",
    "#     a = subj.split('resource/')[-1]\n",
    "#     b = pred.split('ontology/')[-1]\n",
    "#     c = obj.split('resource/')[-1]\n",
    "    \n",
    "#     print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# with open('/Users/nhalliwe/Desktop/criage/CRIAGE/data/WN-18/e1rel_to_e2_ranking_test.json', 'r') as f:\n",
    "    \n",
    "#     for i in f:\n",
    "\n",
    "#         temp_dict = json.loads(f.readline())\n",
    "#         print(temp_dict)\n",
    "#         #print(temp_dict['e2_multi1'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #criage_exps = utils.read_json('/Users/nhalliwe/Desktop/criage/CRIAGE/data/WN-18/e1rel_to_e2_ranking_test.json')\n",
    "# import numpy as np\n",
    "\n",
    "# triples = [('Eve', 'type', 'Lecturer'),\n",
    "#            ('Eve', 'type', 'Person'), \n",
    "#            ('Lecturer', 'subClassOf', 'Person'), \n",
    "#            ('David', 'type', 'Person'),\n",
    "#            ('David', 'type', 'Researcher'),\n",
    "#            ('Researcher', 'subClassOf', 'Person'),\n",
    "#            ('Flora', 'hasSpouse', 'Gaston'),\n",
    "#            ('Gaston', 'type', 'Person'),\n",
    "#            ('Flora', 'type', 'Person')\n",
    "#           ]\n",
    "\n",
    "# train = np.array(triples)\n",
    "\n",
    "# entities = np.unique(np.concatenate((train[:,0], train[:,2]), axis=0)).tolist()\n",
    "# relations = np.unique(train[:,1]).tolist()\n",
    "\n",
    "# num_entities = len(entities)\n",
    "# num_relations = len(relations)\n",
    "\n",
    "# ent2idx = dict(zip(entities, range(num_entities)))\n",
    "# rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "# idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "# idx2rel = {idx:rel for rel,idx in rel2idx.items()}\n",
    "\n",
    "# train2idx = utils.train2idx(train,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in sorted(indices.keys()):\n",
    "#     for j in indices[i]:\n",
    "# #         if len(indices[i]) == 1:\n",
    "# #             print(i,j)\n",
    "# #         else:\n",
    "#             for k in indices[i]:\n",
    "#                 if j != k:\n",
    "#                     print(i,j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "data = np.load(os.path.join('.','data','royalty.npz'))\n",
    "\n",
    "train = data['X_train']\n",
    "test = data['X_test']\n",
    "\n",
    "train_exp = data['train_exp']\n",
    "test_exp = data['test_exp']\n",
    "\n",
    "full_exp = np.concatenate((train_exp,test_exp), axis=0)\n",
    "\n",
    "full_train = np.concatenate((train,train_exp), axis=0)\n",
    "\n",
    "entities = data['entities'].tolist()\n",
    "relations = data['relations'].tolist()\n",
    "\n",
    "num_entities = len(entities)\n",
    "num_relations = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(num_entities)))\n",
    "rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "idx2ent = {v:k for k,v in ent2idx.items()}\n",
    "idx2rel = {v:k for k,v in rel2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2idx = utils.array2idx(train,ent2idx,rel2idx)\n",
    "exp2idx = utils.array2idx(train_exp,ent2idx,rel2idx)\n",
    "\n",
    "test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
