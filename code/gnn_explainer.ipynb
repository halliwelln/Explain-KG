{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, RGCNConv#GNNExplainer\n",
    "from torch.nn import Sequential, Linear\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import random as rn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108afbe70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, RGCNConv,GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'Cora'\n",
    "# #path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "# dataset = Planetoid('/Users/nhalliwe/Desktop/pytorch_geometric-master/data/Planetoid',\n",
    "#                     dataset, transform=T.NormalizeFeatures())\n",
    "# data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = RGCNConv(in_channels=2,out_channels=50,num_relations=2)\n",
    "#         self.conv2 = RGCNConv(16, dataset.num_classes, dataset.num_relations,\n",
    "#                               num_bases=30)\n",
    "        self.DistMult = DistMult(embedding_dim=50,num_relations=2)\n",
    "\n",
    "    def forward(self,triple, edge_index, rel_idx):\n",
    "        x = F.relu(self.conv1(triple, edge_index, rel_idx))\n",
    "        x = F.sigmoid(self.DistMult(x,rel_idx))\n",
    "        #x = self.conv2(x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "class DistMult(torch.nn.Module):\n",
    "    def __init__(self,embedding_dim,num_relations):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_relations = num_relations\n",
    "        self.weights = torch.nn.Parameter(torch.Tensor(self.num_relations,self.embedding_dim))\n",
    "        \n",
    "        torch.nn.init.normal_(self.weights, mean=0.0, std=1)\n",
    "        \n",
    "    def forward(self,x,rel_idx):\n",
    "        \n",
    "        head_e,tail_e = x\n",
    "\n",
    "        return torch.sum(head_e * self.weights[rel_idx][0] * tail_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from collections import Counter\n",
    "\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import (InMemoryDataset, Data, download_url,\n",
    "                                  extract_tar)\n",
    "\n",
    "\n",
    "class Entities(InMemoryDataset):\n",
    "    r\"\"\"The relational entities networks \"AIFB\", \"MUTAG\", \"BGS\" and \"AM\" from\n",
    "    the `\"Modeling Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper.\n",
    "    Training and test splits are given by node indices.\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        name (string): The name of the dataset (:obj:`\"AIFB\"`,\n",
    "            :obj:`\"MUTAG\"`, :obj:`\"BGS\"`, :obj:`\"AM\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://data.dgl.ai/dataset/{}.tgz'\n",
    "\n",
    "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
    "        assert name in ['AIFB', 'AM', 'MUTAG', 'BGS']\n",
    "        self.name = name.lower()\n",
    "        super(Entities, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self):\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self):\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def num_relations(self):\n",
    "        return self.data.edge_type.max().item() + 1\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self.data.train_y.max().item() + 1\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\n",
    "            '{}_stripped.nt.gz'.format(self.name),\n",
    "            'completeDataset.tsv',\n",
    "            'trainingSet.tsv',\n",
    "            'testSet.tsv',\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url.format(self.name), self.root)\n",
    "        extract_tar(path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "\n",
    "    def triples(self, graph, relation=None):\n",
    "        for s, p, o in graph.triples((None, relation, None)):\n",
    "            yield s, p, o\n",
    "\n",
    "    def process(self):\n",
    "        import rdflib as rdf\n",
    "\n",
    "        graph_file, task_file, train_file, test_file = self.raw_paths\n",
    "\n",
    "        g = rdf.Graph()\n",
    "        with gzip.open(graph_file, 'rb') as f:\n",
    "            g.parse(file=f, format='nt')\n",
    "\n",
    "        freq_ = Counter(g.predicates())\n",
    "\n",
    "        def freq(rel):\n",
    "            return freq_[rel] if rel in freq_ else 0\n",
    "\n",
    "        relations = sorted(set(g.predicates()), key=lambda rel: -freq(rel))\n",
    "        subjects = set(g.subjects())\n",
    "        objects = set(g.objects())\n",
    "        nodes = list(subjects.union(objects))\n",
    "\n",
    "        relations_dict = {rel: i for i, rel in enumerate(list(relations))}\n",
    "        nodes_dict = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "        edge_list = []\n",
    "        for s, p, o in g.triples((None, None, None)):\n",
    "            src, dst, rel = nodes_dict[s], nodes_dict[o], relations_dict[p]\n",
    "            edge_list.append([src, dst, 2 * rel])\n",
    "            edge_list.append([dst, src, 2 * rel + 1])\n",
    "\n",
    "        edge_list = sorted(edge_list, key=lambda x: (x[0], x[1], x[2]))\n",
    "        edge = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        edge_index, edge_type = edge[:2], edge[2]\n",
    "\n",
    "        if self.name == 'am':\n",
    "            label_header = 'label_cateogory'\n",
    "            nodes_header = 'proxy'\n",
    "        elif self.name == 'aifb':\n",
    "            label_header = 'label_affiliation'\n",
    "            nodes_header = 'person'\n",
    "        elif self.name == 'mutag':\n",
    "            label_header = 'label_mutagenic'\n",
    "            nodes_header = 'bond'\n",
    "        elif self.name == 'bgs':\n",
    "            label_header = 'label_lithogenesis'\n",
    "            nodes_header = 'rock'\n",
    "\n",
    "        labels_df = pd.read_csv(task_file, sep='\\t')\n",
    "        labels_set = set(labels_df[label_header].values.tolist())\n",
    "        labels_dict = {lab: i for i, lab in enumerate(list(labels_set))}\n",
    "        nodes_dict = {np.unicode(key): val for key, val in nodes_dict.items()}\n",
    "\n",
    "        train_labels_df = pd.read_csv(train_file, sep='\\t')\n",
    "        train_indices, train_labels = [], []\n",
    "        for nod, lab in zip(train_labels_df[nodes_header].values,\n",
    "                            train_labels_df[label_header].values):\n",
    "            train_indices.append(nodes_dict[nod])\n",
    "            train_labels.append(labels_dict[lab])\n",
    "\n",
    "        train_idx = torch.tensor(train_indices, dtype=torch.long)\n",
    "        train_y = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "        test_labels_df = pd.read_csv(test_file, sep='\\t')\n",
    "        test_indices, test_labels = [], []\n",
    "        for nod, lab in zip(test_labels_df[nodes_header].values,\n",
    "                            test_labels_df[label_header].values):\n",
    "            test_indices.append(nodes_dict[nod])\n",
    "            test_labels.append(labels_dict[lab])\n",
    "\n",
    "        test_idx = torch.tensor(test_indices, dtype=torch.long)\n",
    "        test_y = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "        data = Data(edge_index=edge_index)\n",
    "        data.edge_type = edge_type\n",
    "        data.train_idx = train_idx\n",
    "        data.train_y = train_y\n",
    "        data.test_idx = test_idx\n",
    "        data.test_y = test_y\n",
    "        data.num_nodes = edge_index.max().item() + 1\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}{}()'.format(self.name.upper(), self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data.dgl.ai/dataset/mutag.tgz\n",
      "Extracting /Users/nhalliwe/Downlaods/MUTAG.tar.gz/mutag.tgz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Entities('/Users/nhalliwe/Downlaods/MUTAG.tar.gz', 'MUTAG')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import k_hop_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_idx = torch.cat([data.train_idx, data.test_idx], dim=0)\n",
    "node_idx, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "    node_idx, 2, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "data.num_nodes = node_idx.size(0)\n",
    "data.edge_index = edge_index\n",
    "data.edge_type = data.edge_type[edge_mask]\n",
    "data.train_idx = mapping[:data.train_idx.size(0)]\n",
    "data.test_idx = mapping[data.train_idx.size(0):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# node_idx, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "#     2, num_hops=2, edge_index=edge_index, relabel_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGCNConv(in_channels=3,out_channels=50,num_relations=1)(edge_index=edge_index,edge_type=edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('/Users/nhalliwe/Desktop/Explain-KG','data','royalty.npz'))\n",
    "RULE = 'aunt'\n",
    "\n",
    "triples, traces = data[RULE + '_triples'], data[RULE + '_traces']\n",
    "entities = data[RULE + '_entities'].tolist()\n",
    "relations = data[RULE + '_relations'].tolist()  \n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "EMBEDDING_DIM = 50\n",
    "OUTPUT_DIM = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 2000\n",
    "\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "triples2idx = utils.array2idx(triples,ent2idx,rel2idx)\n",
    "traces2idx = utils.array2idx(traces,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(triple.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(rel_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from math import sqrt\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "class GNNExplainer(torch.nn.Module):\n",
    "    r\"\"\"The GNN-Explainer model from the `\"GNNExplainer: Generating\n",
    "    Explanations for Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1903.03894>`_ paper for identifying compact subgraph\n",
    "    structures and small subsets node features that play a crucial role in a\n",
    "    GNN’s node-predictions.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using GNN-Explainer, see `examples/gnn_explainer.py\n",
    "        <https://github.com/rusty1s/pytorch_geometric/blob/master/examples/\n",
    "        gnn_explainer.py>`_.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GNN module to explain.\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        num_hops (int, optional): The number of hops the :obj:`model` is\n",
    "            aggregating information from.\n",
    "            If set to :obj:`None`, will automatically try to detect this\n",
    "            information based on the number of\n",
    "            :class:`~torch_geometric.nn.conv.message_passing.MessagePassing`\n",
    "            layers inside :obj:`model`. (default: :obj:`None`)\n",
    "        log (bool, optional): If set to :obj:`False`, will not log any learning\n",
    "            progress. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "    }\n",
    "\n",
    "    def __init__(self, model, epochs: int = 100, lr: float = 0.01,\n",
    "                 num_hops: Optional[int] = None, log: bool = True):\n",
    "        super(GNNExplainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.__num_hops__ = num_hops\n",
    "        self.log = log\n",
    "\n",
    "    def __set_masks__(self, x, edge_index, init=\"normal\"):\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        std = 0.1\n",
    "        self.node_feat_mask = torch.nn.Parameter(torch.randn(F) * 0.1)\n",
    "\n",
    "        std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "        self.edge_mask = torch.nn.Parameter(torch.randn(E) * std)\n",
    "\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = True\n",
    "                module.__edge_mask__ = self.edge_mask\n",
    "\n",
    "    def __clear_masks__(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = False\n",
    "                module.__edge_mask__ = None\n",
    "        self.node_feat_masks = None\n",
    "        self.edge_mask = None\n",
    "\n",
    "    @property\n",
    "    def num_hops(self):\n",
    "        if self.__num_hops__ is not None:\n",
    "            return self.__num_hops__\n",
    "\n",
    "        k = 0\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                k += 1\n",
    "        return k\n",
    "\n",
    "    def __flow__(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def __subgraph__(self, node_idx, x, edge_index, **kwargs):\n",
    "        num_nodes, num_edges = x.size(0), edge_index.size(1)\n",
    "\n",
    "        subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "            num_nodes=num_nodes, flow=self.__flow__())\n",
    "\n",
    "        x = x[subset]\n",
    "        for key, item in kwargs.items():\n",
    "            if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "                item = item[subset]\n",
    "            elif torch.is_tensor(item) and item.size(0) == num_edges:\n",
    "                item = item[edge_mask]\n",
    "            kwargs[key] = item\n",
    "\n",
    "        return x, edge_index, mapping, edge_mask, kwargs\n",
    "\n",
    "    def __loss__(self, node_idx, log_logits, pred_label):\n",
    "        loss = -log_logits[node_idx, pred_label[node_idx]]\n",
    "        print('pred_label',pred_label)\n",
    "        print('pred_label[idx]',pred_label[node_idx])\n",
    "        print(';;',log_logits)\n",
    "\n",
    "        m = self.edge_mask.sigmoid()\n",
    "        edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "        loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        m = self.node_feat_mask.sigmoid()\n",
    "        node_feat_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "        loss = loss + self.coeffs['node_feat_size'] * node_feat_reduce(m)\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def explain_node(self, node_idx, x, edge_index, **kwargs):\n",
    "        r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n",
    "        crucial role to explain the prediction made by the GNN for node\n",
    "        :attr:`node_idx`.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The node to explain.\n",
    "            x (Tensor): The node feature matrix.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "\n",
    "        :rtype: (:class:`Tensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        self.__clear_masks__()\n",
    "\n",
    "        num_edges = edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        x, edge_index, mapping, hard_edge_mask, kwargs = self.__subgraph__(\n",
    "            node_idx, x, edge_index, **kwargs)\n",
    "\n",
    "        # Get the initial prediction.\n",
    "        with torch.no_grad():\n",
    "            log_logits = self.model(x=x, edge_index=edge_index, **kwargs)\n",
    "            pred_label = log_logits.argmax(dim=-1)\n",
    "\n",
    "        self.__set_masks__(x, edge_index)\n",
    "        self.to(x.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask],\n",
    "                                     lr=self.lr)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar = tqdm(total=self.epochs)\n",
    "            pbar.set_description(f'Explain node {node_idx}')\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            h = x * self.node_feat_mask.view(1, -1).sigmoid()\n",
    "            log_logits = self.model(x=h, edge_index=edge_index, **kwargs)\n",
    "            loss = self.__loss__(mapping, log_logits, pred_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if self.log:  # pragma: no cover\n",
    "                pbar.update(1)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar.close()\n",
    "\n",
    "        node_feat_mask = self.node_feat_mask.detach().sigmoid()\n",
    "        edge_mask = self.edge_mask.new_zeros(num_edges)\n",
    "        edge_mask[hard_edge_mask] = self.edge_mask.detach().sigmoid()\n",
    "\n",
    "        self.__clear_masks__()\n",
    "\n",
    "        return node_feat_mask, edge_mask\n",
    "\n",
    "\n",
    "    def visualize_subgraph(self, node_idx, edge_index, edge_mask, y=None,\n",
    "                           threshold=None, **kwargs):\n",
    "        r\"\"\"Visualizes the subgraph around :attr:`node_idx` given an edge mask\n",
    "        :attr:`edge_mask`.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The node id to explain.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            edge_mask (Tensor): The edge mask.\n",
    "            y (Tensor, optional): The ground-truth node-prediction labels used\n",
    "                as node colorings. (default: :obj:`None`)\n",
    "            threshold (float, optional): Sets a threshold for visualizing\n",
    "                important edges. If set to :obj:`None`, will visualize all\n",
    "                edges with transparancy indicating the importance of edges.\n",
    "                (default: :obj:`None`)\n",
    "            **kwargs (optional): Additional arguments passed to\n",
    "                :func:`nx.draw`.\n",
    "\n",
    "        :rtype: :class:`matplotlib.axes.Axes`, :class:`networkx.DiGraph`\n",
    "        \"\"\"\n",
    "\n",
    "        assert edge_mask.size(0) == edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        subset, edge_index, _, hard_edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "            num_nodes=None, flow=self.__flow__())\n",
    "\n",
    "        edge_mask = edge_mask[hard_edge_mask]\n",
    "\n",
    "        if threshold is not None:\n",
    "            edge_mask = (edge_mask >= threshold).to(torch.float)\n",
    "\n",
    "        if y is None:\n",
    "            y = torch.zeros(edge_index.max().item() + 1,\n",
    "                            device=edge_index.device)\n",
    "        else:\n",
    "            y = y[subset].to(torch.float) / y.max().item()\n",
    "\n",
    "        data = Data(edge_index=edge_index, att=edge_mask, y=y,\n",
    "                    num_nodes=y.size(0)).to('cpu')\n",
    "        G = to_networkx(data, node_attrs=['y'], edge_attrs=['att'])\n",
    "        mapping = {k: i for k, i in enumerate(subset.tolist())}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        node_kwargs = copy(kwargs)\n",
    "        node_kwargs['node_size'] = kwargs.get('node_size') or 800\n",
    "        node_kwargs['cmap'] = kwargs.get('cmap') or 'cool'\n",
    "\n",
    "        label_kwargs = copy(kwargs)\n",
    "        label_kwargs['font_size'] = kwargs.get('font_size') or 10\n",
    "\n",
    "        pos = nx.spring_layout(G)\n",
    "        ax = plt.gca()\n",
    "        for source, target, data in G.edges(data=True):\n",
    "            ax.annotate(\n",
    "                '', xy=pos[target], xycoords='data', xytext=pos[source],\n",
    "                textcoords='data', arrowprops=dict(\n",
    "                    arrowstyle=\"->\",\n",
    "                    alpha=max(data['att'], 0.1),\n",
    "                    shrinkA=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                    shrinkB=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                    connectionstyle=\"arc3,rad=0.1\",\n",
    "                ))\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=y.tolist(), **node_kwargs)\n",
    "        nx.draw_networkx_labels(G, pos, **label_kwargs)\n",
    "\n",
    "        return ax, G\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = GNNExplainer(model, epochs=200)\n",
    "node_idx = 10\n",
    "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)\n",
    "ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGS and AM graphs are too big to process them in a full-batch fashion.\n",
    "# Since our model does only make use of a rather small receptive field, we\n",
    "# filter the graph to only contain the nodes that are at most 2-hop neighbors\n",
    "# away from any training/test node.\n",
    "\n",
    "# path = '/Users/nhalliwe/Desktop/pytorch_geometric-master/data/Entities'#osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Entities')\n",
    "# dataset = Entities(path, 'AM')\n",
    "# data = dataset[0]\n",
    "\n",
    "# node_idx = torch.cat([data.train_idx, data.test_idx], dim=0)\n",
    "# node_idx, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "#     node_idx, 2, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "# data.num_nodes = node_idx.size(0)\n",
    "# data.edge_index = edge_index\n",
    "# data.edge_type = data.edge_type[edge_mask]\n",
    "# data.train_idx = mapping[:data.train_idx.size(0)]\n",
    "# data.test_idx = mapping[data.train_idx.size(0):]\n",
    "\n",
    "# import pickle as pkl\n",
    "# def load_data_pkl(pkl_path):\n",
    "#     with open(pkl_path,\"rb\") as f:\n",
    "#         data=pkl.load(f)\n",
    "#     A=data[\"A\"]\n",
    "#     y=data[\"y\"]\n",
    "#     train_idx=data[\"train_idx\"]\n",
    "#     test_idx=data[\"test_idx\"]\n",
    "\n",
    "#     return A,y,train_idx,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_path=\"/Users/nhalliwe/Downloads/relation-gcn-pytorch-master/aifb/aifb.pickle\"\n",
    "# A,y,train_idx,test_idx=load_data_pkl(pkl_path)\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = RGCNConv(data.num_nodes, 16, dataset.num_relations,\n",
    "#                               num_bases=30)\n",
    "#         self.conv2 = RGCNConv(16, dataset.num_classes, dataset.num_relations,\n",
    "#                               num_bases=30)\n",
    "\n",
    "#     def forward(self, edge_index, edge_type):\n",
    "#         x = F.relu(self.conv1(None, edge_index, edge_type))\n",
    "#         x = self.conv2(x, edge_index, edge_type)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data.edge_index, data.edge_type)\n",
    "#     loss = F.nll_loss(out[data.train_idx], data.train_y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     pred = model(data.edge_index, data.edge_type).argmax(dim=-1)\n",
    "#     train_acc = pred[data.train_idx].eq(data.train_y).to(torch.float).mean()\n",
    "#     test_acc = pred[data.test_idx].eq(data.test_y).to(torch.float).mean()\n",
    "#     return train_acc.item(), test_acc.item()\n",
    "\n",
    "\n",
    "# for epoch in range(1, 51):\n",
    "#     loss = train()\n",
    "#     train_acc, test_acc = test()\n",
    "#     print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f} '\n",
    "#           f'Test: {test_acc:.4f}')\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = Net().to(device)\n",
    "# cora_data = cora_data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# x, edge_index = cora_data.x, cora_data.edge_index\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     log_logits = model(x, edge_index)\n",
    "#     loss = F.nll_loss(log_logits[cora_data.train_mask], cora_data.y[cora_data.train_mask])\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# explainer = GNNExplainer(model, epochs=200)\n",
    "# node_idx = 10\n",
    "# node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)\n",
    "# ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=cora_data.y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "\n",
    "# npdata = np.load(os.path.join('.','data','royalty_spouse.npz'))\n",
    "\n",
    "# train = npdata['X_train']\n",
    "# test = npdata['X_test']\n",
    "\n",
    "# train_exp = npdata['train_exp']\n",
    "# test_exp = npdata['test_exp']\n",
    "\n",
    "# full_train = np.concatenate((train,train_exp.reshape(-1,3)), axis=0)\n",
    "\n",
    "# entities = npdata['entities'].tolist()\n",
    "# relations = npdata['relations'].tolist()\n",
    "\n",
    "# NUM_ENTITIES = len(entities)\n",
    "# NUM_RELATIONS = len(relations)\n",
    "# NUM_FEATURES = 50\n",
    "\n",
    "# ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "# rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "# train2idx = utils.array2idx(full_train,ent2idx,rel2idx)\n",
    "# test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "\n",
    "# testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)\n",
    "\n",
    "# # #entity_embeddings = np.load(os.path.join('.','data','transE_embeddings.npz'))['entity_embeddings']\n",
    "# X = np.random.randn(NUM_ENTITIES,NUM_FEATURES)\n",
    "\n",
    "# train2idx_ = np.concatenate([train2idx[:,0].reshape(-1,1),train2idx[:,2].reshape(-1,1)], axis=1)\n",
    "# test2idx_ = np.concatenate([test2idx[:,0].reshape(-1,1),test2idx[:,2].reshape(-1,1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate([train2idx[,test2idx], axis=0).shape\n",
    "\n",
    "all_data = np.concatenate([train2idx,test2idx], axis=0)\n",
    "all_labels = np.concatenate([train2idx[:,1],test2idx[:,1]], axis=0)\n",
    "\n",
    "train_mask = np.concatenate([np.ones(train2idx.shape[0], dtype=bool), np.zeros(test2idx.shape[0],dtype=bool)])\n",
    "test_mask = np.concatenate([np.zeros(train2idx.shape[0], dtype=bool), np.ones(test2idx.shape[0],dtype=bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = []\n",
    "# for i,_,j in all_data:\n",
    "    \n",
    "#     X.append([entity_embeddings[i],entity_embeddings[j]])\n",
    "\n",
    "# X = np.array(X)\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(all_labels)\n",
    "\n",
    "train_horizontal = np.stack([train2idx_[:,0],train2idx_[:,1]],axis=0)\n",
    "test_horizontal = np.stack([test2idx_[:,0],test2idx_[:,1]],axis=0)\n",
    "\n",
    "edge_index = torch.tensor(np.concatenate([train_horizontal, test_horizontal],axis=1), dtype=torch.long)\n",
    "data = Data(x=X, y=y, edge_index=edge_index,num_classes=NUM_RELATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = torch.tensor([entity_embeddings[ent2idx[h]] for h,_,_ in test])\n",
    "# y_test = torch.tensor([(rel2idx[r]) for _,r,_ in test])\n",
    "# test_ents = np.array([(ent2idx[h],ent2idx[t]) for h,_,t in test]).T\n",
    "# test_ents_flipped = np.stack((test_ents[1,:], test_ents[0,:]))\n",
    "# test_edge_index = torch.tensor(np.concatenate((test_ents,test_ents_flipped), axis=1), dtype=torch.long)\n",
    "# test_data = Data(x_test=X_test,y_test=y_test,test_edge_index=test_edge_index,num_classes=num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.lin = Sequential(Linear(10,10))\n",
    "#         self.conv1 = GCNConv(data.num_features, 16)\n",
    "#         self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = Net().to(device)\n",
    "# data = data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# x, edge_index = data.x, data.edge_index\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     log_logits = model(x, edge_index)\n",
    "#     loss = F.nll_loss(log_logits, data.y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# explainer = GNNExplainer(model, epochs=200)\n",
    "\n",
    "# x_test,test_y, test_edge_index = test_data.x_test,test_data.y_test,test_data.test_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_explanations(i,x,y,edge_index, explainer):\n",
    "\n",
    "#     node_feat_mask, edge_mask = explainer.explain_node(i, x, edge_index)\n",
    "    \n",
    "#     _, G = explainer.visualize_subgraph(i, edge_index, edge_mask, y=y)\n",
    "\n",
    "#     temp = []\n",
    "#     exp = list(G.edges)\n",
    "#     for tup in exp:\n",
    "#         sorted_tup = tuple(sorted(tup))\n",
    "#         temp.append(sorted_tup)\n",
    "\n",
    "#     return list(set(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_feat_mask, edge_mask = explainer.explain_node(0, x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explainer.visualize_subgraph(2278, edge_index, edge_mask, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = np.zeros(shape=(num_entities,num_entities))\n",
    "\n",
    "# for h,r,t in d:\n",
    "    \n",
    "#     h_idx = entities.index(h)\n",
    "#     #r_idx = relations.index(r)\n",
    "#     t_idx = entities.index(t)\n",
    "    \n",
    "#     A[h_idx, t_idx] = 1\n",
    "    \n",
    "# A = torch.tensor(A, dtype=torch.float)\n",
    "#A = torch.randn(5,10)\n",
    "# X = torch.randn(2708,1433)\n",
    "# y = torch.randint(3, (2708,))\n",
    "\n",
    "# X = torch.randn(10,1433)\n",
    "# y = torch.randint(3, (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.lin = Sequential(Linear(10,10))\n",
    "#         self.conv1 = GCNConv(data.num_features, 16)\n",
    "#         self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = Net().to(device)\n",
    "# data = data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# x, edge_index = data.x, data.edge_index\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     log_logits = model(x, edge_index)\n",
    "#     loss = F.nll_loss(log_logits, data.y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# explainer = GNNExplainer(model, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_idx = 3\n",
    "# node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)\n",
    "# ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanations = []\n",
    "\n",
    "# for i in range(2):\n",
    "    \n",
    "#     node_feat_mask, edge_mask = explainer.explain_node(i, x, edge_index)\n",
    "#     _, G = explainer.visualize_subgraph(i, edge_index, edge_mask, y=data.y)\n",
    "    \n",
    "#     explanations.append(list(G.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_explanations = []\n",
    "# for i in explanations:\n",
    "#     temp = []\n",
    "#     for tup in i:\n",
    "#         sorted_tup = tuple(sorted(tup))\n",
    "#         temp.append(sorted_tup)\n",
    "#     unique_explanations.append(list(set(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
