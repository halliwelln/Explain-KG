{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, RGCNConv#GNNExplainer\n",
    "from torch.nn import Sequential, Linear\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import random as rn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x108f43e70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, RGCNConv,GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'Cora'\n",
    "# #path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
    "# dataset = Planetoid('/Users/nhalliwe/Desktop/pytorch_geometric-master/data/Planetoid',\n",
    "#                     dataset, transform=T.NormalizeFeatures())\n",
    "# data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('/Users/nhalliwe/Desktop/Explain-KG','data','royalty.npz'))\n",
    "RULE = 'aunt'\n",
    "\n",
    "triples, traces = data[RULE + '_triples'], data[RULE + '_traces']\n",
    "entities = data[RULE + '_entities'].tolist()\n",
    "relations = data[RULE + '_relations'].tolist()  \n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "EMBEDDING_DIM = 50\n",
    "OUTPUT_DIM = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 2000\n",
    "\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "triples2idx = utils.array2idx(triples,ent2idx,rel2idx)\n",
    "traces2idx = utils.array2idx(traces,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triple = torch.tensor([0,1])\n",
    "# edge_index = torch.tensor([[0,1],[1,0]])\n",
    "# rel_idx = torch.tensor([1,1])\n",
    "\n",
    "# print(triple.shape)\n",
    "# print(edge_index.shape)\n",
    "# print(rel_idx.shape)\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseRGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases,\n",
    "                 num_hidden_layers=1, dropout=0,\n",
    "                 use_self_loop=False, use_cuda=False):\n",
    "        super(BaseRGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = None if num_bases < 0 else num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_self_loop = use_self_loop\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # i2h\n",
    "        i2h = self.build_input_layer()\n",
    "        if i2h is not None:\n",
    "            self.layers.append(i2h)\n",
    "        # h2h\n",
    "        for idx in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "        # h2o\n",
    "        h2o = self.build_output_layer()\n",
    "        if h2o is not None:\n",
    "            self.layers.append(h2o)\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return None\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return None\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h, r, norm)\n",
    "        return h\n",
    "\n",
    "class RelGraphEmbedLayer(nn.Module):\n",
    "    r\"\"\"Embedding layer for featureless heterograph.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dev_id : int\n",
    "        Device to run the layer.\n",
    "    num_nodes : int\n",
    "        Number of nodes.\n",
    "    node_tides : tensor\n",
    "        Storing the node type id for each node starting from 0\n",
    "    num_of_ntype : int\n",
    "        Number of node types\n",
    "    input_size : list of int\n",
    "        A list of input feature size for each node type. If None, we then\n",
    "        treat certain input feature as an one-hot encoding feature.\n",
    "    embed_size : int\n",
    "        Output embed size\n",
    "    embed_name : str, optional\n",
    "        Embed name\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dev_id,\n",
    "                 num_nodes,\n",
    "                 node_tids,\n",
    "                 num_of_ntype,\n",
    "                 input_size,\n",
    "                 embed_size,\n",
    "                 sparse_emb=False,\n",
    "                 embed_name='embed'):\n",
    "        super(RelGraphEmbedLayer, self).__init__()\n",
    "        self.dev_id = dev_id\n",
    "        self.embed_size = embed_size\n",
    "        self.embed_name = embed_name\n",
    "        self.num_nodes = num_nodes\n",
    "        self.sparse_emb = sparse_emb\n",
    "\n",
    "        # create weight embeddings for each node for each relation\n",
    "        self.embeds = nn.ParameterDict()\n",
    "        self.num_of_ntype = num_of_ntype\n",
    "        self.idmap = th.empty(num_nodes).long()\n",
    "\n",
    "        for ntype in range(num_of_ntype):\n",
    "            if input_size[ntype] is not None:\n",
    "                input_emb_size = input_size[ntype].shape[1]\n",
    "                embed = nn.Parameter(th.Tensor(input_emb_size, self.embed_size))\n",
    "                nn.init.xavier_uniform_(embed)\n",
    "                self.embeds[str(ntype)] = embed\n",
    "\n",
    "        self.node_embeds = th.nn.Embedding(node_tids.shape[0], self.embed_size, sparse=self.sparse_emb)\n",
    "        nn.init.uniform_(self.node_embeds.weight, -1.0, 1.0)\n",
    "\n",
    "    def forward(self, node_ids, node_tids, type_ids, features):\n",
    "        \"\"\"Forward computation\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_ids : tensor\n",
    "            node ids to generate embedding for.\n",
    "        node_ids : tensor\n",
    "            node type ids\n",
    "        features : list of features\n",
    "            list of initial features for nodes belong to different node type.\n",
    "            If None, the corresponding features is an one-hot encoding feature,\n",
    "            else use the features directly as input feature and matmul a\n",
    "            projection matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            embeddings as the input of the next layer\n",
    "        \"\"\"\n",
    "        tsd_ids = node_ids.to(self.node_embeds.weight.device)\n",
    "        embeds = th.empty(node_ids.shape[0], self.embed_size, device=self.dev_id)\n",
    "        for ntype in range(self.num_of_ntype):\n",
    "            if features[ntype] is not None:\n",
    "                loc = node_tids == ntype\n",
    "                embeds[loc] = features[ntype][type_ids[loc]].to(self.dev_id) @ self.embeds[str(ntype)].to(self.dev_id)\n",
    "            else:\n",
    "                loc = node_tids == ntype\n",
    "                embeds[loc] = self.node_embeds(tsd_ids[loc]).to(self.dev_id)\n",
    "\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.embedding(h.squeeze())\n",
    "\n",
    "class RGCN(BaseRGCN):\n",
    "    def build_input_layer(self):\n",
    "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
    "\n",
    "    def build_hidden_layer(self, idx):\n",
    "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
    "        return RelGraphConv(self.h_dim, self.h_dim, self.num_rels, \"bdd\",\n",
    "                self.num_bases, activation=act, self_loop=True,\n",
    "                dropout=self.dropout)\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
    "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
    "        super(LinkPredict, self).__init__()\n",
    "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
    "                         num_hidden_layers, dropout, use_cuda)\n",
    "        self.reg_param = reg_param\n",
    "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
    "        nn.init.xavier_uniform_(self.w_relation,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def calc_score(self, embedding, triplets):\n",
    "        # DistMult\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.w_relation[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        return score\n",
    "\n",
    "    def forward(self, g, h, r, norm):\n",
    "        return self.rgcn.forward(g, h, r, norm)\n",
    "\n",
    "    def regularization_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
    "\n",
    "    def get_loss(self, g, embed, triplets, labels):\n",
    "        # triplets is a list of data samples (positive and negative)\n",
    "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
    "        score = self.calc_score(embed, triplets)\n",
    "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
    "        reg_loss = self.regularization_loss(embed)\n",
    "        return predict_loss + self.reg_param * reg_loss\n",
    "\n",
    "def node_norm_to_edge_norm(g, node_norm):\n",
    "    g = g.local_var()\n",
    "    # convert to edge norm\n",
    "    g.ndata['norm'] = node_norm\n",
    "    g.apply_edges(lambda edges : {'norm' : edges.dst['norm']})\n",
    "    return g.edata['norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "#DistMult(embedding_dim=50,num_relations=2)(Net()(triple,edge_index,rel_idx),rel_idx)\n",
    "#Net()(triple,edge_index,rel_idx)\n",
    "from dgl.nn.pytorch import RelGraphConv\n",
    "from dgl.data.knowledge_graph import load_data\n",
    "\n",
    "\n",
    "# triple = torch.tensor([0,1])\n",
    "# edge_index = torch.tensor([[0,1],[1,0]])\n",
    "# rel_idx = torch.tensor([1,1])\n",
    "\n",
    "model = LinkPredict(in_dim=2,\n",
    "                    h_dim=2,\n",
    "                    num_rels=1,\n",
    "                    num_bases=-1,\n",
    "                    num_hidden_layers=1,\n",
    "                    dropout=0,\n",
    "                    use_cuda=False,\n",
    "                    reg_param=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "def get_adj_and_degrees(num_nodes, triplets):\n",
    "    \"\"\" Get adjacency list and degrees of the graph\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        adj_list[triplet[0]].append([i, triplet[2]])\n",
    "        adj_list[triplet[2]].append([i, triplet[0]])\n",
    "\n",
    "    degrees = np.array([len(a) for a in adj_list])\n",
    "    adj_list = [np.array(a) for a in adj_list]\n",
    "    return adj_list, degrees\n",
    "\n",
    "def get_adj_and_degrees(num_nodes, triplets):\n",
    "    \"\"\" Get adjacency list and degrees of the graph\n",
    "    \"\"\"\n",
    "    adj_list = [[] for _ in range(num_nodes)]\n",
    "    for i,triplet in enumerate(triplets):\n",
    "        adj_list[triplet[0]].append([i, triplet[2]])\n",
    "        adj_list[triplet[2]].append([i, triplet[0]])\n",
    "\n",
    "    degrees = np.array([len(a) for a in adj_list])\n",
    "    adj_list = [np.array(a) for a in adj_list]\n",
    "    return adj_list, degrees\n",
    "\n",
    "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges by neighborhool expansion.\n",
    "    This guarantees that the sampled edges form a connected graph, which\n",
    "    may help deeper GNNs that require information from more than one hop.\n",
    "    \"\"\"\n",
    "    edges = np.zeros((sample_size), dtype=np.int32)\n",
    "\n",
    "    #initialize\n",
    "    sample_counts = np.array([d for d in degrees])\n",
    "    picked = np.array([False for _ in range(n_triplets)])\n",
    "    seen = np.array([False for _ in degrees])\n",
    "\n",
    "    for i in range(0, sample_size):\n",
    "        weights = sample_counts * seen\n",
    "\n",
    "        if np.sum(weights) == 0:\n",
    "            weights = np.ones_like(weights)\n",
    "            weights[np.where(sample_counts == 0)] = 0\n",
    "\n",
    "        probabilities = (weights) / np.sum(weights)\n",
    "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
    "                                         p=probabilities)\n",
    "        chosen_adj_list = adj_list[chosen_vertex]\n",
    "        seen[chosen_vertex] = True\n",
    "\n",
    "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "        chosen_edge = chosen_adj_list[chosen_edge]\n",
    "        edge_number = chosen_edge[0]\n",
    "\n",
    "        while picked[edge_number]:\n",
    "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
    "            chosen_edge = chosen_adj_list[chosen_edge]\n",
    "            edge_number = chosen_edge[0]\n",
    "\n",
    "        edges[i] = edge_number\n",
    "        other_vertex = chosen_edge[1]\n",
    "        picked[edge_number] = True\n",
    "        sample_counts[chosen_vertex] -= 1\n",
    "        sample_counts[other_vertex] -= 1\n",
    "        seen[other_vertex] = True\n",
    "\n",
    "    return edges\n",
    "\n",
    "def sample_edge_uniform(adj_list, degrees, n_triplets, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triplets)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    "\n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
    "                                      num_rels, adj_list, degrees,\n",
    "                                      negative_rate, sampler=\"uniform\"):\n",
    "    \"\"\"Get training graph and signals\n",
    "    First perform edge neighborhood sampling on graph, then perform negative\n",
    "    sampling to generate negative samples\n",
    "    \"\"\"\n",
    "    # perform edge neighbor sampling\n",
    "    if sampler == \"uniform\":\n",
    "        edges = sample_edge_uniform(adj_list, degrees, len(triplets), sample_size)\n",
    "    elif sampler == \"neighbor\":\n",
    "        edges = sample_edge_neighborhood(adj_list, degrees, len(triplets), sample_size)\n",
    "    else:\n",
    "        raise ValueError(\"Sampler type must be either 'uniform' or 'neighbor'.\")\n",
    "\n",
    "    # relabel nodes to have consecutive node ids\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "    # negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
    "                                        negative_rate)\n",
    "\n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "    src = src[graph_split_ids]\n",
    "    dst = dst[graph_split_ids]\n",
    "    rel = rel[graph_split_ids]\n",
    "\n",
    "    # build DGL graph\n",
    "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
    "    print(\"# sampled edges: {}\".format(len(src) * 2))\n",
    "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
    "                                             (src, rel, dst))\n",
    "    return g, uniq_v, rel, norm, samples, labels\n",
    "\n",
    "def comp_deg_norm(g):\n",
    "    g = g.local_var()\n",
    "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
    "    norm = 1.0 / in_deg\n",
    "    norm[np.isinf(norm)] = 0\n",
    "    return norm\n",
    "\n",
    "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
    "    \"\"\" Create a DGL graph. The graph is bidirectional because RGCN authors\n",
    "        use reversed relations.\n",
    "        This function also generates edge type and normalization factor\n",
    "        (reciprocal of node incoming degree)\n",
    "    \"\"\"\n",
    "    g = dgl.graph(([], []))\n",
    "    g.add_nodes(num_nodes)\n",
    "    src, rel, dst = triplets\n",
    "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
    "    rel = np.concatenate((rel, rel + num_rels))\n",
    "    edges = sorted(zip(dst, src, rel))\n",
    "    dst, src, rel = np.array(edges).transpose()\n",
    "    g.add_edges(src, dst)\n",
    "    norm = comp_deg_norm(g)\n",
    "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
    "    return g, rel.astype('int64'), norm.astype('int64')\n",
    "\n",
    "def build_test_graph(num_nodes, num_rels, edges):\n",
    "    src, rel, dst = edges.transpose()\n",
    "    print(\"Test graph:\")\n",
    "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
    "\n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.randint(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    "\n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (raw)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    return indices\n",
    "\n",
    "def perturb_and_get_raw_rank(embedding, w, a, r, b, test_size, batch_size=100):\n",
    "    \"\"\" Perturb one element in the triplets\n",
    "    \"\"\"\n",
    "    n_batch = (test_size + batch_size - 1) // batch_size\n",
    "    ranks = []\n",
    "    for idx in range(n_batch):\n",
    "        print(\"batch {} / {}\".format(idx, n_batch))\n",
    "        batch_start = idx * batch_size\n",
    "        batch_end = min(test_size, (idx + 1) * batch_size)\n",
    "        batch_a = a[batch_start: batch_end]\n",
    "        batch_r = r[batch_start: batch_end]\n",
    "        emb_ar = embedding[batch_a] * w[batch_r]\n",
    "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
    "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
    "        # out-prod and reduce sum\n",
    "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
    "        score = torch.sum(out_prod, dim=0) # size E x V\n",
    "        score = torch.sigmoid(score)\n",
    "        target = b[batch_start: batch_end]\n",
    "        ranks.append(sort_and_rank(score, target))\n",
    "    return torch.cat(ranks)\n",
    "\n",
    "# return MRR (raw), and Hits @ (1, 3, 10)\n",
    "def calc_raw_mrr(embedding, w, test_triplets, hits=[], eval_bz=100):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        # perturb subject\n",
    "        ranks_s = perturb_and_get_raw_rank(embedding, w, o, r, s, test_size, eval_bz)\n",
    "        # perturb object\n",
    "        ranks_o = perturb_and_get_raw_rank(embedding, w, s, r, o, test_size, eval_bz)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Utility functions for evaluations (filtered)\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_o = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider an object if it is part of a triplet to filter\n",
    "    for o in range(num_entities):\n",
    "        if (target_s, target_r, o) not in triplets_to_filter:\n",
    "            filtered_o.append(o)\n",
    "    return torch.LongTensor(filtered_o)\n",
    "\n",
    "def filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities):\n",
    "    target_s, target_r, target_o = int(target_s), int(target_r), int(target_o)\n",
    "    filtered_s = []\n",
    "    # Do not filter out the test triplet, since we want to predict on it\n",
    "    if (target_s, target_r, target_o) in triplets_to_filter:\n",
    "        triplets_to_filter.remove((target_s, target_r, target_o))\n",
    "    # Do not consider a subject if it is part of a triplet to filter\n",
    "    for s in range(num_entities):\n",
    "        if (s, target_r, target_o) not in triplets_to_filter:\n",
    "            filtered_s.append(s)\n",
    "    return torch.LongTensor(filtered_s)\n",
    "\n",
    "def perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb object in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_o = filter_o(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_o_idx = int((filtered_o == target_o).nonzero())\n",
    "        emb_s = embedding[target_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[filtered_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_o_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter):\n",
    "    \"\"\" Perturb subject in the triplets\n",
    "    \"\"\"\n",
    "    num_entities = embedding.shape[0]\n",
    "    ranks = []\n",
    "    for idx in range(test_size):\n",
    "        if idx % 100 == 0:\n",
    "            print(\"test triplet {} / {}\".format(idx, test_size))\n",
    "        target_s = s[idx]\n",
    "        target_r = r[idx]\n",
    "        target_o = o[idx]\n",
    "        filtered_s = filter_s(triplets_to_filter, target_s, target_r, target_o, num_entities)\n",
    "        target_s_idx = int((filtered_s == target_s).nonzero())\n",
    "        emb_s = embedding[filtered_s]\n",
    "        emb_r = w[target_r]\n",
    "        emb_o = embedding[target_o]\n",
    "        emb_triplet = emb_s * emb_r * emb_o\n",
    "        scores = torch.sigmoid(torch.sum(emb_triplet, dim=1))\n",
    "        _, indices = torch.sort(scores, descending=True)\n",
    "        rank = int((indices == target_s_idx).nonzero())\n",
    "        ranks.append(rank)\n",
    "    return torch.LongTensor(ranks)\n",
    "\n",
    "def calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        s = test_triplets[:, 0]\n",
    "        r = test_triplets[:, 1]\n",
    "        o = test_triplets[:, 2]\n",
    "        test_size = test_triplets.shape[0]\n",
    "\n",
    "        triplets_to_filter = torch.cat([train_triplets, valid_triplets, test_triplets]).tolist()\n",
    "        triplets_to_filter = {tuple(triplet) for triplet in triplets_to_filter}\n",
    "        print('Perturbing subject...')\n",
    "        ranks_s = perturb_s_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "        print('Perturbing object...')\n",
    "        ranks_o = perturb_o_and_get_filtered_rank(embedding, w, s, r, o, test_size, triplets_to_filter)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "    return mrr.item()\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Main evaluation function\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "def calc_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits=[], eval_bz=100, eval_p=\"filtered\"):\n",
    "    if eval_p == \"filtered\":\n",
    "        mrr = calc_filtered_mrr(embedding, w, train_triplets, valid_triplets, test_triplets, hits)\n",
    "    else:\n",
    "        mrr = calc_raw_mrr(embedding, w, test_triplets, hits, eval_bz)\n",
    "    return mrr\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#\n",
    "# Multithread wrapper\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "# According to https://github.com/pytorch/pytorch/issues/17199, this decorator\n",
    "# is necessary to make fork() and openmp work together.\n",
    "def thread_wrapped_func(func):\n",
    "    \"\"\"\n",
    "    Wraps a process entry point to make it work with OpenMP.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        queue = Queue()\n",
    "        def _queue_result():\n",
    "            exception, trace, res = None, None, None\n",
    "            try:\n",
    "                res = func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                exception = e\n",
    "                trace = traceback.format_exc()\n",
    "            queue.put((res, exception, trace))\n",
    "\n",
    "        start_new_thread(_queue_result, ())\n",
    "        result, exception, trace = queue.get()\n",
    "        if exception is None:\n",
    "            return result\n",
    "        else:\n",
    "            assert isinstance(exception, Exception)\n",
    "            raise exception.__class__(trace)\n",
    "    return decorated_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list, degrees = get_adj_and_degrees(NUM_ENTITIES, triples2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sampled nodes: 2\n",
      "# sampled edges: 2\n",
      "# nodes: 2, # edges: 2\n"
     ]
    }
   ],
   "source": [
    "g, node_id, edge_type, node_norm, data, labels = \\\n",
    "            generate_sampled_graph_and_labels(\n",
    "                triples2idx, 1, 1,\n",
    "                NUM_RELATIONS, adj_list, degrees, negative_rate=1,\n",
    "                sampler='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id = torch.from_numpy(node_id).view(-1, 1).long()\n",
    "edge_type = torch.from_numpy(edge_type)\n",
    "edge_norm = node_norm_to_edge_norm(g, torch.from_numpy(node_norm).view(-1, 1))\n",
    "data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
    "deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fc78ebf815fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ce55831bee9c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, h, r, norm)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregularization_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-877166fda380>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, h, r, norm)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ce55831bee9c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, h, r, norm)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseRGCN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "embed = model(g, node_id, edge_type, edge_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from math import sqrt\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "class GNNExplainer(torch.nn.Module):\n",
    "    r\"\"\"The GNN-Explainer model from the `\"GNNExplainer: Generating\n",
    "    Explanations for Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1903.03894>`_ paper for identifying compact subgraph\n",
    "    structures and small subsets node features that play a crucial role in a\n",
    "    GNNâ€™s node-predictions.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using GNN-Explainer, see `examples/gnn_explainer.py\n",
    "        <https://github.com/rusty1s/pytorch_geometric/blob/master/examples/\n",
    "        gnn_explainer.py>`_.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The GNN module to explain.\n",
    "        epochs (int, optional): The number of epochs to train.\n",
    "            (default: :obj:`100`)\n",
    "        lr (float, optional): The learning rate to apply.\n",
    "            (default: :obj:`0.01`)\n",
    "        num_hops (int, optional): The number of hops the :obj:`model` is\n",
    "            aggregating information from.\n",
    "            If set to :obj:`None`, will automatically try to detect this\n",
    "            information based on the number of\n",
    "            :class:`~torch_geometric.nn.conv.message_passing.MessagePassing`\n",
    "            layers inside :obj:`model`. (default: :obj:`None`)\n",
    "        log (bool, optional): If set to :obj:`False`, will not log any learning\n",
    "            progress. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = {\n",
    "        'edge_size': 0.005,\n",
    "        'edge_reduction': 'sum',\n",
    "        'node_feat_size': 1.0,\n",
    "        'node_feat_reduction': 'mean',\n",
    "        'edge_ent': 1.0,\n",
    "        'node_feat_ent': 0.1,\n",
    "    }\n",
    "\n",
    "    def __init__(self, model, epochs: int = 100, lr: float = 0.01,\n",
    "                 num_hops: Optional[int] = None, log: bool = True):\n",
    "        super(GNNExplainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.__num_hops__ = num_hops\n",
    "        self.log = log\n",
    "\n",
    "    def __set_masks__(self, x, edge_index, init=\"normal\"):\n",
    "        (N, F), E = x.size(), edge_index.size(1)\n",
    "\n",
    "        std = 0.1\n",
    "        self.node_feat_mask = torch.nn.Parameter(torch.randn(F) * 0.1)\n",
    "\n",
    "        std = torch.nn.init.calculate_gain('relu') * sqrt(2.0 / (2 * N))\n",
    "        self.edge_mask = torch.nn.Parameter(torch.randn(E) * std)\n",
    "\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = True\n",
    "                module.__edge_mask__ = self.edge_mask\n",
    "\n",
    "    def __clear_masks__(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                module.__explain__ = False\n",
    "                module.__edge_mask__ = None\n",
    "        self.node_feat_masks = None\n",
    "        self.edge_mask = None\n",
    "\n",
    "    @property\n",
    "    def num_hops(self):\n",
    "        if self.__num_hops__ is not None:\n",
    "            return self.__num_hops__\n",
    "\n",
    "        k = 0\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                k += 1\n",
    "        return k\n",
    "\n",
    "    def __flow__(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                return module.flow\n",
    "        return 'source_to_target'\n",
    "\n",
    "    def __subgraph__(self, node_idx, x, edge_index, **kwargs):\n",
    "        num_nodes, num_edges = x.size(0), edge_index.size(1)\n",
    "\n",
    "        subset, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "            num_nodes=num_nodes, flow=self.__flow__())\n",
    "\n",
    "        x = x[subset]\n",
    "        for key, item in kwargs.items():\n",
    "            if torch.is_tensor(item) and item.size(0) == num_nodes:\n",
    "                item = item[subset]\n",
    "            elif torch.is_tensor(item) and item.size(0) == num_edges:\n",
    "                item = item[edge_mask]\n",
    "            kwargs[key] = item\n",
    "\n",
    "        return x, edge_index, mapping, edge_mask, kwargs\n",
    "\n",
    "    def __loss__(self, node_idx, log_logits, pred_label):\n",
    "        loss = -log_logits[node_idx, pred_label[node_idx]]\n",
    "        print('pred_label',pred_label)\n",
    "        print('pred_label[idx]',pred_label[node_idx])\n",
    "        print(';;',log_logits)\n",
    "\n",
    "        m = self.edge_mask.sigmoid()\n",
    "        edge_reduce = getattr(torch, self.coeffs['edge_reduction'])\n",
    "        loss = loss + self.coeffs['edge_size'] * edge_reduce(m)\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coeffs['edge_ent'] * ent.mean()\n",
    "\n",
    "        m = self.node_feat_mask.sigmoid()\n",
    "        node_feat_reduce = getattr(torch, self.coeffs['node_feat_reduction'])\n",
    "        loss = loss + self.coeffs['node_feat_size'] * node_feat_reduce(m)\n",
    "        ent = -m * torch.log(m + EPS) - (1 - m) * torch.log(1 - m + EPS)\n",
    "        loss = loss + self.coeffs['node_feat_ent'] * ent.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def explain_node(self, node_idx, x, edge_index, **kwargs):\n",
    "        r\"\"\"Learns and returns a node feature mask and an edge mask that play a\n",
    "        crucial role to explain the prediction made by the GNN for node\n",
    "        :attr:`node_idx`.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The node to explain.\n",
    "            x (Tensor): The node feature matrix.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            **kwargs (optional): Additional arguments passed to the GNN module.\n",
    "\n",
    "        :rtype: (:class:`Tensor`, :class:`Tensor`)\n",
    "        \"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "        self.__clear_masks__()\n",
    "\n",
    "        num_edges = edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        x, edge_index, mapping, hard_edge_mask, kwargs = self.__subgraph__(\n",
    "            node_idx, x, edge_index, **kwargs)\n",
    "\n",
    "        # Get the initial prediction.\n",
    "        with torch.no_grad():\n",
    "            log_logits = self.model(x=x, edge_index=edge_index, **kwargs)\n",
    "            pred_label = log_logits.argmax(dim=-1)\n",
    "\n",
    "        self.__set_masks__(x, edge_index)\n",
    "        self.to(x.device)\n",
    "\n",
    "        optimizer = torch.optim.Adam([self.node_feat_mask, self.edge_mask],\n",
    "                                     lr=self.lr)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar = tqdm(total=self.epochs)\n",
    "            pbar.set_description(f'Explain node {node_idx}')\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            optimizer.zero_grad()\n",
    "            h = x * self.node_feat_mask.view(1, -1).sigmoid()\n",
    "            log_logits = self.model(x=h, edge_index=edge_index, **kwargs)\n",
    "            loss = self.__loss__(mapping, log_logits, pred_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if self.log:  # pragma: no cover\n",
    "                pbar.update(1)\n",
    "\n",
    "        if self.log:  # pragma: no cover\n",
    "            pbar.close()\n",
    "\n",
    "        node_feat_mask = self.node_feat_mask.detach().sigmoid()\n",
    "        edge_mask = self.edge_mask.new_zeros(num_edges)\n",
    "        edge_mask[hard_edge_mask] = self.edge_mask.detach().sigmoid()\n",
    "\n",
    "        self.__clear_masks__()\n",
    "\n",
    "        return node_feat_mask, edge_mask\n",
    "\n",
    "\n",
    "    def visualize_subgraph(self, node_idx, edge_index, edge_mask, y=None,\n",
    "                           threshold=None, **kwargs):\n",
    "        r\"\"\"Visualizes the subgraph around :attr:`node_idx` given an edge mask\n",
    "        :attr:`edge_mask`.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The node id to explain.\n",
    "            edge_index (LongTensor): The edge indices.\n",
    "            edge_mask (Tensor): The edge mask.\n",
    "            y (Tensor, optional): The ground-truth node-prediction labels used\n",
    "                as node colorings. (default: :obj:`None`)\n",
    "            threshold (float, optional): Sets a threshold for visualizing\n",
    "                important edges. If set to :obj:`None`, will visualize all\n",
    "                edges with transparancy indicating the importance of edges.\n",
    "                (default: :obj:`None`)\n",
    "            **kwargs (optional): Additional arguments passed to\n",
    "                :func:`nx.draw`.\n",
    "\n",
    "        :rtype: :class:`matplotlib.axes.Axes`, :class:`networkx.DiGraph`\n",
    "        \"\"\"\n",
    "\n",
    "        assert edge_mask.size(0) == edge_index.size(1)\n",
    "\n",
    "        # Only operate on a k-hop subgraph around `node_idx`.\n",
    "        subset, edge_index, _, hard_edge_mask = k_hop_subgraph(\n",
    "            node_idx, self.num_hops, edge_index, relabel_nodes=True,\n",
    "            num_nodes=None, flow=self.__flow__())\n",
    "\n",
    "        edge_mask = edge_mask[hard_edge_mask]\n",
    "\n",
    "        if threshold is not None:\n",
    "            edge_mask = (edge_mask >= threshold).to(torch.float)\n",
    "\n",
    "        if y is None:\n",
    "            y = torch.zeros(edge_index.max().item() + 1,\n",
    "                            device=edge_index.device)\n",
    "        else:\n",
    "            y = y[subset].to(torch.float) / y.max().item()\n",
    "\n",
    "        data = Data(edge_index=edge_index, att=edge_mask, y=y,\n",
    "                    num_nodes=y.size(0)).to('cpu')\n",
    "        G = to_networkx(data, node_attrs=['y'], edge_attrs=['att'])\n",
    "        mapping = {k: i for k, i in enumerate(subset.tolist())}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "        node_kwargs = copy(kwargs)\n",
    "        node_kwargs['node_size'] = kwargs.get('node_size') or 800\n",
    "        node_kwargs['cmap'] = kwargs.get('cmap') or 'cool'\n",
    "\n",
    "        label_kwargs = copy(kwargs)\n",
    "        label_kwargs['font_size'] = kwargs.get('font_size') or 10\n",
    "\n",
    "        pos = nx.spring_layout(G)\n",
    "        ax = plt.gca()\n",
    "        for source, target, data in G.edges(data=True):\n",
    "            ax.annotate(\n",
    "                '', xy=pos[target], xycoords='data', xytext=pos[source],\n",
    "                textcoords='data', arrowprops=dict(\n",
    "                    arrowstyle=\"->\",\n",
    "                    alpha=max(data['att'], 0.1),\n",
    "                    shrinkA=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                    shrinkB=sqrt(node_kwargs['node_size']) / 2.0,\n",
    "                    connectionstyle=\"arc3,rad=0.1\",\n",
    "                ))\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=y.tolist(), **node_kwargs)\n",
    "        nx.draw_networkx_labels(G, pos, **label_kwargs)\n",
    "\n",
    "        return ax, G\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = GNNExplainer(model, epochs=200)\n",
    "node_idx = 10\n",
    "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)\n",
    "ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGS and AM graphs are too big to process them in a full-batch fashion.\n",
    "# Since our model does only make use of a rather small receptive field, we\n",
    "# filter the graph to only contain the nodes that are at most 2-hop neighbors\n",
    "# away from any training/test node.\n",
    "\n",
    "# path = '/Users/nhalliwe/Desktop/pytorch_geometric-master/data/Entities'#osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Entities')\n",
    "# dataset = Entities(path, 'AM')\n",
    "# data = dataset[0]\n",
    "\n",
    "# node_idx = torch.cat([data.train_idx, data.test_idx], dim=0)\n",
    "# node_idx, edge_index, mapping, edge_mask = k_hop_subgraph(\n",
    "#     node_idx, 2, data.edge_index, relabel_nodes=True)\n",
    "\n",
    "# data.num_nodes = node_idx.size(0)\n",
    "# data.edge_index = edge_index\n",
    "# data.edge_type = data.edge_type[edge_mask]\n",
    "# data.train_idx = mapping[:data.train_idx.size(0)]\n",
    "# data.test_idx = mapping[data.train_idx.size(0):]\n",
    "\n",
    "# import pickle as pkl\n",
    "# def load_data_pkl(pkl_path):\n",
    "#     with open(pkl_path,\"rb\") as f:\n",
    "#         data=pkl.load(f)\n",
    "#     A=data[\"A\"]\n",
    "#     y=data[\"y\"]\n",
    "#     train_idx=data[\"train_idx\"]\n",
    "#     test_idx=data[\"test_idx\"]\n",
    "\n",
    "#     return A,y,train_idx,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_path=\"/Users/nhalliwe/Downloads/relation-gcn-pytorch-master/aifb/aifb.pickle\"\n",
    "# A,y,train_idx,test_idx=load_data_pkl(pkl_path)\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = RGCNConv(data.num_nodes, 16, dataset.num_relations,\n",
    "#                               num_bases=30)\n",
    "#         self.conv2 = RGCNConv(16, dataset.num_classes, dataset.num_relations,\n",
    "#                               num_bases=30)\n",
    "\n",
    "#     def forward(self, edge_index, edge_type):\n",
    "#         x = F.relu(self.conv1(None, edge_index, edge_type))\n",
    "#         x = self.conv2(x, edge_index, edge_type)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     out = model(data.edge_index, data.edge_type)\n",
    "#     loss = F.nll_loss(out[data.train_idx], data.train_y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     return loss.item()\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test():\n",
    "#     model.eval()\n",
    "#     pred = model(data.edge_index, data.edge_type).argmax(dim=-1)\n",
    "#     train_acc = pred[data.train_idx].eq(data.train_y).to(torch.float).mean()\n",
    "#     test_acc = pred[data.test_idx].eq(data.test_y).to(torch.float).mean()\n",
    "#     return train_acc.item(), test_acc.item()\n",
    "\n",
    "\n",
    "# for epoch in range(1, 51):\n",
    "#     loss = train()\n",
    "#     train_acc, test_acc = test()\n",
    "#     print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f} '\n",
    "#           f'Test: {test_acc:.4f}')\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = Net().to(device)\n",
    "# cora_data = cora_data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# x, edge_index = cora_data.x, cora_data.edge_index\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     log_logits = model(x, edge_index)\n",
    "#     loss = F.nll_loss(log_logits[cora_data.train_mask], cora_data.y[cora_data.train_mask])\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# explainer = GNNExplainer(model, epochs=200)\n",
    "# node_idx = 10\n",
    "# node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)\n",
    "# ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=cora_data.y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "\n",
    "# npdata = np.load(os.path.join('.','data','royalty_spouse.npz'))\n",
    "\n",
    "# train = npdata['X_train']\n",
    "# test = npdata['X_test']\n",
    "\n",
    "# train_exp = npdata['train_exp']\n",
    "# test_exp = npdata['test_exp']\n",
    "\n",
    "# full_train = np.concatenate((train,train_exp.reshape(-1,3)), axis=0)\n",
    "\n",
    "# entities = npdata['entities'].tolist()\n",
    "# relations = npdata['relations'].tolist()\n",
    "\n",
    "# NUM_ENTITIES = len(entities)\n",
    "# NUM_RELATIONS = len(relations)\n",
    "# NUM_FEATURES = 50\n",
    "\n",
    "# ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "# rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "# train2idx = utils.array2idx(full_train,ent2idx,rel2idx)\n",
    "# test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "\n",
    "# testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)\n",
    "\n",
    "# # #entity_embeddings = np.load(os.path.join('.','data','transE_embeddings.npz'))['entity_embeddings']\n",
    "# X = np.random.randn(NUM_ENTITIES,NUM_FEATURES)\n",
    "\n",
    "# train2idx_ = np.concatenate([train2idx[:,0].reshape(-1,1),train2idx[:,2].reshape(-1,1)], axis=1)\n",
    "# test2idx_ = np.concatenate([test2idx[:,0].reshape(-1,1),test2idx[:,2].reshape(-1,1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate([train2idx[,test2idx], axis=0).shape\n",
    "\n",
    "all_data = np.concatenate([train2idx,test2idx], axis=0)\n",
    "all_labels = np.concatenate([train2idx[:,1],test2idx[:,1]], axis=0)\n",
    "\n",
    "train_mask = np.concatenate([np.ones(train2idx.shape[0], dtype=bool), np.zeros(test2idx.shape[0],dtype=bool)])\n",
    "test_mask = np.concatenate([np.zeros(train2idx.shape[0], dtype=bool), np.ones(test2idx.shape[0],dtype=bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = []\n",
    "# for i,_,j in all_data:\n",
    "    \n",
    "#     X.append([entity_embeddings[i],entity_embeddings[j]])\n",
    "\n",
    "# X = np.array(X)\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(all_labels)\n",
    "\n",
    "train_horizontal = np.stack([train2idx_[:,0],train2idx_[:,1]],axis=0)\n",
    "test_horizontal = np.stack([test2idx_[:,0],test2idx_[:,1]],axis=0)\n",
    "\n",
    "edge_index = torch.tensor(np.concatenate([train_horizontal, test_horizontal],axis=1), dtype=torch.long)\n",
    "data = Data(x=X, y=y, edge_index=edge_index,num_classes=NUM_RELATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = torch.tensor([entity_embeddings[ent2idx[h]] for h,_,_ in test])\n",
    "# y_test = torch.tensor([(rel2idx[r]) for _,r,_ in test])\n",
    "# test_ents = np.array([(ent2idx[h],ent2idx[t]) for h,_,t in test]).T\n",
    "# test_ents_flipped = np.stack((test_ents[1,:], test_ents[0,:]))\n",
    "# test_edge_index = torch.tensor(np.concatenate((test_ents,test_ents_flipped), axis=1), dtype=torch.long)\n",
    "# test_data = Data(x_test=X_test,y_test=y_test,test_edge_index=test_edge_index,num_classes=num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.lin = Sequential(Linear(10,10))\n",
    "#         self.conv1 = GCNConv(data.num_features, 16)\n",
    "#         self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = Net().to(device)\n",
    "# data = data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# x, edge_index = data.x, data.edge_index\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     log_logits = model(x, edge_index)\n",
    "#     loss = F.nll_loss(log_logits, data.y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# explainer = GNNExplainer(model, epochs=200)\n",
    "\n",
    "# x_test,test_y, test_edge_index = test_data.x_test,test_data.y_test,test_data.test_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_explanations(i,x,y,edge_index, explainer):\n",
    "\n",
    "#     node_feat_mask, edge_mask = explainer.explain_node(i, x, edge_index)\n",
    "    \n",
    "#     _, G = explainer.visualize_subgraph(i, edge_index, edge_mask, y=y)\n",
    "\n",
    "#     temp = []\n",
    "#     exp = list(G.edges)\n",
    "#     for tup in exp:\n",
    "#         sorted_tup = tuple(sorted(tup))\n",
    "#         temp.append(sorted_tup)\n",
    "\n",
    "#     return list(set(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node_feat_mask, edge_mask = explainer.explain_node(0, x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explainer.visualize_subgraph(2278, edge_index, edge_mask, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = np.zeros(shape=(num_entities,num_entities))\n",
    "\n",
    "# for h,r,t in d:\n",
    "    \n",
    "#     h_idx = entities.index(h)\n",
    "#     #r_idx = relations.index(r)\n",
    "#     t_idx = entities.index(t)\n",
    "    \n",
    "#     A[h_idx, t_idx] = 1\n",
    "    \n",
    "# A = torch.tensor(A, dtype=torch.float)\n",
    "#A = torch.randn(5,10)\n",
    "# X = torch.randn(2708,1433)\n",
    "# y = torch.randint(3, (2708,))\n",
    "\n",
    "# X = torch.randn(10,1433)\n",
    "# y = torch.randint(3, (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.lin = Sequential(Linear(10,10))\n",
    "#         self.conv1 = GCNConv(data.num_features, 16)\n",
    "#         self.conv2 = GCNConv(16, data.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = Net().to(device)\n",
    "# data = data.to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# x, edge_index = data.x, data.edge_index\n",
    "\n",
    "# for epoch in range(1, 201):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     log_logits = model(x, edge_index)\n",
    "#     loss = F.nll_loss(log_logits, data.y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# explainer = GNNExplainer(model, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_idx = 3\n",
    "# node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)\n",
    "# ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanations = []\n",
    "\n",
    "# for i in range(2):\n",
    "    \n",
    "#     node_feat_mask, edge_mask = explainer.explain_node(i, x, edge_index)\n",
    "#     _, G = explainer.visualize_subgraph(i, edge_index, edge_mask, y=data.y)\n",
    "    \n",
    "#     explanations.append(list(G.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_explanations = []\n",
    "# for i in explanations:\n",
    "#     temp = []\n",
    "#     for tup in i:\n",
    "#         sorted_tup = tuple(sorted(tup))\n",
    "#         temp.append(sorted_tup)\n",
    "#     unique_explanations.append(list(set(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
