{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import random as rn\n",
    "import RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('..','data','royalty.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RULE = 'spouse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triples,traces,nopred = utils.concat_triples(data, data['rules'])\n",
    "# entities = data['all_entities'].tolist()\n",
    "# relations = data['all_relations'].tolist()\n",
    "\n",
    "triples,traces,nopred = utils.concat_triples(data, [RULE,'brother','sister'])\n",
    "sister_relations = data['sister_relations'].tolist()\n",
    "sister_entities = data['sister_entities'].tolist()\n",
    "\n",
    "brother_relations = data['brother_relations'].tolist()\n",
    "brother_entities = data['brother_entities'].tolist()\n",
    "\n",
    "entities = np.unique(data[RULE + '_entities'].tolist()+brother_entities+sister_entities).tolist()\n",
    "relations = np.unique(data[RULE + '_relations'].tolist()+brother_relations+sister_relations).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "EMBEDDING_DIM = 50\n",
    "OUTPUT_DIM = 50\n",
    "LEARNING_RATE = .01\n",
    "NUM_EPOCHS = 2\n",
    "THRESHOLD = .01\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method RGCN_Layer.call of <RGCN.RGCN_Layer object at 0x1959795410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method RGCN_Layer.call of <RGCN.RGCN_Layer object at 0x1959795410>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DistMult.call of <RGCN.DistMult object at 0x1959795310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DistMult.call of <RGCN.DistMult object at 0x1959795310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "all_indices = tf.reshape(tf.range(0,NUM_ENTITIES,1,dtype=tf.int64), (1,-1))\n",
    "\n",
    "model = RGCN.get_RGCN_Model(\n",
    "    num_entities=NUM_ENTITIES,\n",
    "    num_relations=NUM_RELATIONS,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    seed=SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join('..','data','weights',RULE+'.h5'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "relation_embeddings = model.get_layer('output').get_weights()[0]\n",
    "\n",
    "relation_kernel, self_kernel = model.get_layer('rgcn__layer').get_weights()\n",
    "\n",
    "entity_embeddings = model.get_layer('entity_embeddings').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_scores = []\n",
    "preds = []\n",
    "\n",
    "train2idx = utils.array2idx(triples,ent2idx,rel2idx)\n",
    "trainexp2idx = utils.array2idx(traces,ent2idx,rel2idx)\n",
    "nopred2idx = utils.array2idx(nopred,ent2idx,rel2idx)\n",
    "\n",
    "adjacency_data = tf.concat([train2idx,trainexp2idx.reshape(-1,3),nopred2idx],axis=0)\n",
    "\n",
    "test2idx = utils.array2idx(triples,ent2idx,rel2idx)\n",
    "testexp2idx = utils.array2idx(traces,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(data_subset,node_idx):\n",
    "    \n",
    "    neighbors = tf.concat([data_subset[data_subset[:,0] == node_idx],\n",
    "                           data_subset[data_subset[:,2] == node_idx]],axis=0)\n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "def get_computation_graph(head,rel,tail,k,data,num_relations):\n",
    "\n",
    "    '''Get k hop neighbors (may include duplicates)'''\n",
    "         \n",
    "    # subset = data[data[:,1] == rel]\n",
    "\n",
    "    # neighbors_head = get_neighbors(subset,head)\n",
    "    # neighbors_tail = get_neighbors(subset,tail)\n",
    "\n",
    "    neighbors_head = get_neighbors(data,head)\n",
    "    neighbors_tail = get_neighbors(data,tail)\n",
    "\n",
    "    all_neighbors = tf.concat([neighbors_head,neighbors_tail],axis=0)\n",
    "\n",
    "    if k > 1:\n",
    "        num_indices = all_neighbors.shape[0]\n",
    "\n",
    "        seen_nodes = []\n",
    "        \n",
    "        for _ in range(k-1):#-1 since we already computed 1st degree neighbors above\n",
    "\n",
    "            for idx in range(num_indices):\n",
    "\n",
    "                head_neighbor_idx = all_neighbors[idx,0]\n",
    "                tail_neighbor_idx = all_neighbors[idx,2]\n",
    "\n",
    "                if head_neighbor_idx not in seen_nodes:\n",
    "                    \n",
    "                    seen_nodes.append(head_neighbor_idx)\n",
    "\n",
    "                    more_head_neighbors = get_neighbors(data,head_neighbor_idx)\n",
    "\n",
    "                    all_neighbors = tf.concat([all_neighbors,more_head_neighbors],axis=0)\n",
    "\n",
    "                if tail_neighbor_idx not in seen_nodes:\n",
    "\n",
    "                    seen_nodes.append(tail_neighbor_idx)\n",
    "\n",
    "                    more_tail_neighbors = get_neighbors(data,tail_neighbor_idx)\n",
    "\n",
    "                    all_neighbors = tf.concat([all_neighbors,more_tail_neighbors],axis=0)\n",
    "\n",
    "    return all_neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "head = test2idx[i,0]\n",
    "rel = test2idx[i,1]\n",
    "tail = test2idx[i,2]\n",
    "\n",
    "comp_graph = get_computation_graph(head,rel,tail,K,adjacency_data,NUM_RELATIONS)\n",
    "\n",
    "adj_mats = utils.get_adj_mats(comp_graph, NUM_ENTITIES, NUM_RELATIONS)\n",
    "\n",
    "masks = [tf.Variable(\n",
    "        initial_value=tf.random.normal(\n",
    "            (1,NUM_ENTITIES,NUM_ENTITIES), \n",
    "            mean=0, \n",
    "            stddev=1, \n",
    "            dtype=tf.float32, \n",
    "            seed=SEED),\n",
    "        name='mask_'+str(i),\n",
    "        trainable=True) for i in range(NUM_RELATIONS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.04588475078344345 @ epoch 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    #with tf.GradientTape() as tape:\n",
    "\n",
    "        tape.watch(masks)\n",
    "        \n",
    "        masked_adjs = [adj_mats[i] * tf.sigmoid(masks[i]) for i in range(NUM_RELATIONS)]\n",
    "        \n",
    "        pred = model([all_indices,\n",
    "           tf.reshape(head,(1,-1)),\n",
    "           tf.reshape(rel,(1,-1)),\n",
    "           tf.reshape(tail,(1,-1)),\n",
    "           masked_adjs])\n",
    "\n",
    "        loss = -1 * tf.math.log(pred+0.00001)# + tf.reduce_mean(masks)\n",
    "\n",
    "    print(f\"Loss {tf.squeeze(loss).numpy()} @ epoch {epoch}\")\n",
    "\n",
    "    grads = tape.gradient(loss,masks)\n",
    "    optimizer.apply_gradients(zip(grads,masks))                                    \n",
    "#         head_output = tf.matmul(tf.reshape(entity_embeddings[head],(1,-1)),self_kernel)\n",
    "#         tail_output = tf.matmul( tf.reshape(entity_embeddings[tail],(1,-1)),self_kernel)\n",
    "\n",
    "#         for i in range(NUM_RELATIONS):\n",
    "            \n",
    "#             adj_i = tf.sparse.reshape(adj_mats[i] * tf.sigmoid(masks[i]), \n",
    "#                                       (NUM_ENTITIES,NUM_ENTITIES))\n",
    "            \n",
    "#             adj_indices = adj_i.indices\n",
    "            \n",
    "#             unique_vals, _ = tf.unique(tf.reshape(adj_indices,(-1)))\n",
    "            \n",
    "#             embeddings = tf.nn.embedding_lookup(entity_embeddings,unique_vals)\n",
    "            \n",
    "#             print(embeddings)\n",
    "            \n",
    "#             break\n",
    "\n",
    "#             sum_embeddings = tf.matmul(adj_i,entity_embeddings)\n",
    "\n",
    "#             head_update = tf.reshape(sum_embeddings[head],(1,-1))\n",
    "#             tail_update = tf.reshape(sum_embeddings[tail],(1,-1))\n",
    "\n",
    "#             head_output += tf.matmul(head_update,relation_kernel[i])\n",
    "#             tail_output += tf.matmul(tail_update,relation_kernel[i])\n",
    "\n",
    "    #             #adj_i = tf.sparse.to_dense(adj_mats[i])[0] * tf.sigmoid(masks[i][0])\n",
    "#             adj_i = tf.sparse.reshape(adj_mats[i] * tf.sigmoid(masks[i]), (NUM_ENTITIES,NUM_ENTITIES))\n",
    "            \n",
    "# #             idx = adj_i.indices\n",
    "            \n",
    "            \n",
    "            \n",
    "# #             head_update = tf.nn.embedding_lookup(entity_embeddings,)\n",
    "            \n",
    "            \n",
    "# # #             tf.nn.embedding_lookup(entity_embeddings,\n",
    "# # #                        tf.concat([tf.gather(adj_mats[5].indices,[1],axis=1),\n",
    "# # #                                   tf.gather(adj_mats[5].indices, [2],axis=1)],\n",
    "# # #                                  axis=0)[:,0])\n",
    "\n",
    "#             head_update = tf.sparse.sparse_dense_matmul(adj_i, tf.reshape(entity_embeddings[head], (1,-1)))\n",
    "#             tail_update = tf.sparse.sparse_dense_matmul(adj_i, tf.reshape(entity_embeddings[tail], (1,-1)))\n",
    "\n",
    "#             sum_embeddings = tf.sparse.sparse_dense_matmul(adj_i,entity_embeddings)\n",
    "\n",
    "#             head_update = tf.reshape(sum_embeddings[head],(1,-1))\n",
    "#             tail_update = tf.reshape(sum_embeddings[tail],(1,-1))\n",
    "\n",
    "            #head_output += tf.matmul(head_update,relation_kernel[i])\n",
    "            #tail_output += tf.matmul(tail_update,relation_kernel[i])            \n",
    "                \n",
    "    \n",
    "\n",
    "#         head_output = tf.sigmoid(head_output)\n",
    "#         tail_output = tf.sigmoid(tail_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sparse.to_dense(tf.sparse.reshape(adj_i, (NUM_ENTITIES,NUM_ENTITIES)))[head]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.sparse.to_dense(tf.sparse.slice(adj_i, [head,head+1],size=(1,NUM_ENTITIES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.unique(tf.concat([tf.gather(adj_mats[5].indices,[1],axis=1),tf.gather(adj_mats[5].indices, [2],axis=1)],axis=0))\n",
    "\n",
    "#tf.unique(tf.reshape(tf.concat([tf.gather(adj_mats[5].indices,[1],axis=1),tf.gather(adj_mats[5].indices, [2],axis=1)],axis=0),(-1)))\n",
    "\n",
    "adj_mats[5].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_subgraphs = utils.get_adj_mats(testexp2idx[i],NUM_ENTITIES,NUM_RELATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
       "array([[   0,  922, 8507],\n",
       "       [   0, 8507,  922]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_subgraphs[5].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10358x10358 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.convert_to_tensor([[9892,9892]],dtype=tf.int64)\n",
    "\n",
    "csr_matrix((tf.ones(1),(a[:,0],a[:,1])),shape=(NUM_ENTITIES,NUM_ENTITIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(os.path.join('..','data','preds','gnn_explainer_'+RULE+'_preds.npz'),\n",
    "#     preds=pred_graphs,embedding_dim=EMBEDDING_DIM,k=K,\n",
    "#     threshold=THRESHOLD,learning_rate=LEARNING_RATE,num_epochs=NUM_EPOCHS\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "in_file=np.load(os.path.join('..','data','preds','gnn_explainer_'+'spouse'+'_preds.npz'),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# head_output = tf.matmul(tf.reshape(entity_embeddings[head],(1,-1)),self_kernel)\n",
    "# tail_output = tf.matmul(tf.reshape(entity_embeddings[tail],(1,-1)),self_kernel)\n",
    "\n",
    "# for i in range(NUM_RELATIONS):\n",
    "\n",
    "#     adj_i = tf.sparse.to_dense(adj_mats[i])[0] * tf.sigmoid(masks[i][0])\n",
    "\n",
    "#     sum_embeddings = tf.matmul(adj_i,entity_embeddings)\n",
    "\n",
    "#     head_update = tf.reshape(sum_embeddings[head],(1,-1))\n",
    "#     tail_update = tf.reshape(sum_embeddings[tail],(1,-1))\n",
    "\n",
    "#     head_output += tf.matmul(head_update,relation_kernel[i])\n",
    "#     tail_output += tf.matmul(tail_update,relation_kernel[i])\n",
    "\n",
    "# # for i in range(NUM_RELATIONS):\n",
    "\n",
    "# #     adj_i = tf.sparse.reshape(adj_mats[i] * tf.sigmoid(masks[i]), (NUM_ENTITIES,NUM_ENTITIES))\n",
    "\n",
    "# head_output = tf.sigmoid(head_output)\n",
    "# tail_output = tf.sigmoid(tail_output)\n",
    "\n",
    "# pred = tf.sigmoid(tf.reduce_sum(head_output*relation_kernel[rel]*tail_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
