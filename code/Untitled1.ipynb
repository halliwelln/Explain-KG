{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import random as rn\n",
    "import RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "\n",
    "data = np.load(os.path.join('..','data','royalty.npz'))\n",
    "\n",
    "entities = data['entities'].tolist()\n",
    "relations = data['relations'].tolist()\n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "EMBEDDING_DIM = 30\n",
    "OUTPUT_DIM = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "triples, traces = data['grandmother_triples'], data['grandmother_traces']\n",
    "\n",
    "train2idx = utils.array2idx(triples, ent2idx,rel2idx)\n",
    "\n",
    "NUM_TRIPLES = train2idx.shape[0]\n",
    "\n",
    "adj_mats = utils.get_adjacency_matrix_list(\n",
    "    num_relations=NUM_RELATIONS,\n",
    "    num_entities=NUM_ENTITIES,\n",
    "    data=train2idx\n",
    ")\n",
    "\n",
    "train2idx = np.expand_dims(train2idx,axis=0)\n",
    "\n",
    "all_indices = np.arange(NUM_ENTITIES).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RGCN_Layer(tf.keras.layers.Layer):\n",
    "#     def __init__(self,num_relations,output_dim,**kwargs):\n",
    "#         super(RGCN_Layer,self).__init__(**kwargs)\n",
    "#         self.num_relations = num_relations\n",
    "#         self.output_dim = output_dim\n",
    "        \n",
    "#     def build(self,input_shape):\n",
    "\n",
    "#         input_dim = int(input_shape[-2][-1])\n",
    "        \n",
    "#         self.relation_kernel = self.add_weight(\n",
    "#             shape=(self.num_relations,input_dim, self.output_dim),\n",
    "#             name=\"relation_kernels\",\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#         self.self_kernel = self.add_weight(\n",
    "#             shape=(input_dim, self.output_dim),\n",
    "#             name=\"self_kernel\",\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             )\n",
    "#         )\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "        \n",
    "#         embeddings,head_idx,head_e,tail_idx,tail_e,adj_mats = inputs\n",
    "            \n",
    "#         head_output = tf.matmul(head_e,self.self_kernel)\n",
    "#         tail_output = tf.matmul(tail_e,self.self_kernel)\n",
    "                \n",
    "#         for i in range(self.num_relations):\n",
    "            \n",
    "#             adj_i = adj_mats[i]\n",
    "            \n",
    "#             head_adj = tf.nn.embedding_lookup(adj_i,head_idx)\n",
    "#             tail_adj = tf.nn.embedding_lookup(adj_i,tail_idx)\n",
    "            \n",
    "#             head_update = tf.matmul(head_adj,embeddings)\n",
    "#             tail_update = tf.matmul(tail_adj,embeddings)\n",
    "\n",
    "#             head_output += tf.matmul(head_update,self.relation_kernel[i])\n",
    "#             tail_output += tf.matmul(tail_update,self.relation_kernel[i])\n",
    "       \n",
    "#         return head_output, tail_output\n",
    "\n",
    "# class DistMult(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_relations,**kwargs):\n",
    "#         super(DistMult,self).__init__(**kwargs)\n",
    "#         self.num_relations = num_relations\n",
    "        \n",
    "#     def build(self,input_shape):\n",
    "        \n",
    "#         embedding_dim = input_shape[0][-1]\n",
    "        \n",
    "#         self.kernel = self.add_weight(\n",
    "#             shape=(self.num_relations,embedding_dim),\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             ),\n",
    "#             name='rel_embedding'\n",
    "#         )\n",
    "        \n",
    "#     def call(self,inputs):\n",
    "        \n",
    "#         head_e,rel_idx,tail_e = inputs\n",
    "        \n",
    "#         rel_e = tf.nn.embedding_lookup(self.kernel,rel_idx)\n",
    "        \n",
    "#         score = tf.sigmoid(tf.reduce_sum(head_e*rel_e*tail_e,axis=-1))\n",
    "\n",
    "#         return tf.expand_dims(score,axis=0)\n",
    "\n",
    "# class RGCN_Model(tf.keras.Model):\n",
    "\n",
    "#     def __init__(self,num_entities,*args,**kwargs):\n",
    "#         super(RGCN_Model,self).__init__(*args, **kwargs)\n",
    "#         self.num_entities = num_entities\n",
    "\n",
    "#     def train_step(self,data):\n",
    "\n",
    "#         all_indices,pos_head,rel,pos_tail,adj_mats = data[0]\n",
    "#         y = data[1]\n",
    "\n",
    "#         neg_head, neg_tail = utils.get_negative_triples(\n",
    "#                 head=pos_head, \n",
    "#                 rel=rel, \n",
    "#                 tail=pos_tail,\n",
    "#                 num_entities=self.num_entities\n",
    "#             )\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "\n",
    "#             y_pos_pred = self([\n",
    "#                     all_indices,\n",
    "#                     pos_head,\n",
    "#                     rel,\n",
    "#                     pos_tail,\n",
    "#                     adj_mats\n",
    "#                     ],\n",
    "#                     training=True\n",
    "#                 )\n",
    "            \n",
    "#             y_neg_pred = self([\n",
    "#                     all_indices,\n",
    "#                     neg_head,\n",
    "#                     rel,\n",
    "#                     neg_tail,\n",
    "#                     adj_mats\n",
    "#                     ],\n",
    "#                     training=True\n",
    "#                 )\n",
    "\n",
    "#             y_pred = tf.concat([y_pos_pred,y_neg_pred],axis=1)\n",
    "#             y_true = tf.concat([y,tf.zeros_like(y)],axis=1)\n",
    "            \n",
    "#             loss = self.compiled_loss(y_true,y_pred)\n",
    "\n",
    "#         grads = tape.gradient(loss, self.trainable_weights)\n",
    "#         self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "#         self.compiled_metrics.update_state(y_true, y_pred)\n",
    "\n",
    "#         return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method RGCN_Layer.call of <RGCN.RGCN_Layer object at 0x1948ab0d90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method RGCN_Layer.call of <RGCN.RGCN_Layer object at 0x1948ab0d90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DistMult.call of <RGCN.DistMult object at 0x194bc62e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DistMult.call of <RGCN.DistMult object at 0x194bc62e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "model = RGCN.get_RGCN_Model(\n",
    "    num_triples=NUM_TRIPLES,\n",
    "    num_entities=NUM_ENTITIES,\n",
    "    num_relations=NUM_RELATIONS,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x194bc6ab00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x194bc6ab00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196e001410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=[\n",
    "        all_indices,\n",
    "        train2idx[:,:,0],\n",
    "        train2idx[:,:,1],\n",
    "        train2idx[:,:,2],\n",
    "        adj_mats\n",
    "        ],\n",
    "    y=np.ones(NUM_TRIPLES).reshape(1,-1),\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices((\n",
    "#         train2idx[0,:,0],\n",
    "#         train2idx[0,:,1],\n",
    "#         train2idx[0,:,2], \n",
    "#         np.ones(train2idx.shape[1])\n",
    "#     )\n",
    "# ).batch(BATCH_SIZE)\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "#     for pos_head,rel,pos_tail,y in data:\n",
    "\n",
    "#         neg_head, neg_tail = utils.get_negative_triples(\n",
    "#             head=pos_head, \n",
    "#             rel=rel, \n",
    "#             tail=pos_tail,\n",
    "#             num_entities=NUM_ENTITIES\n",
    "#         )\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "            \n",
    "# #             print(all_indices.shape)\n",
    "# #             print(pos_head.shape)\n",
    "# #             print(adj_mats.shape)\n",
    "\n",
    "#             y_pos_pred = model([\n",
    "#                 all_indices,\n",
    "#                 pos_head,\n",
    "#                 rel,\n",
    "#                 pos_tail,\n",
    "#                 adj_mats\n",
    "#                 ],\n",
    "#                 training=True\n",
    "#             )\n",
    "\n",
    "#             y_neg_pred = model([\n",
    "#                 all_indices,\n",
    "#                 neg_head,\n",
    "#                 rel,\n",
    "#                 neg_tail,\n",
    "#                 adj_mats\n",
    "#                 ],\n",
    "#                 training=True\n",
    "#             )\n",
    "\n",
    "#             y_pred = tf.concat([y_pos_pred,y_neg_pred],axis=0)\n",
    "#             y_true = tf.concat([y,tf.zeros_like(y)],axis=0)\n",
    "\n",
    "#             loss = bce(y_true,y_pred)\n",
    "\n",
    "#         grads = tape.gradient(loss, model.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "#     print(f'loss {loss} after epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5983)\n",
      "(1, 4024)\n",
      "(1, 4024)\n",
      "(1, 4024)\n",
      "(1, 4, 5983, 5983)\n"
     ]
    }
   ],
   "source": [
    "print(all_indices.shape)\n",
    "print(train2idx[:,:,0].shape)\n",
    "print(train2idx[:,:,1].shape)\n",
    "print(train2idx[:,:,2].shape)\n",
    "print(adj_mats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x196e2f6f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x196e2f6f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(\n",
    "    x=[\n",
    "        all_indices,\n",
    "        train2idx[:,:,0],\n",
    "        train2idx[:,:,1],\n",
    "        train2idx[:,:,2],\n",
    "        adj_mats\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = tf.Variable(\n",
    "    initial_value=tf.random.normal(\n",
    "        (adj_mats.shape), \n",
    "        mean=0, \n",
    "        stddev=1, \n",
    "        dtype=tf.dtypes.float32, \n",
    "        seed=SEED),\n",
    "    name='mask'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model([all_indices,train2idx[:,0:1,0],train2idx[:,0:1,1],train2idx[:,0:1,2],masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.get_layer('entity_embeddings').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,_,t = train2idx[0,0,:]\n",
    "\n",
    "k_hop_adj_mats = []\n",
    "\n",
    "for r in range(NUM_RELATIONS):\n",
    "\n",
    "    k_hop_adj_mat = np.zeros((NUM_ENTITIES,NUM_ENTITIES))\n",
    "\n",
    "    head_neighbors = tf.where(adj_mats[0,r,:][train2idx[0,0:1,0]]==1.)[:,-1]\n",
    "    tail_neighbors = tf.where(adj_mats[0,r,:][train2idx[0,0:1,2]]==1.)[:,-1]\n",
    "\n",
    "    for h_i in head_neighbors:\n",
    "        k_hop_adj_mat[h,h_i] = 1\n",
    "        k_hop_adj_mat[h_i,h] = 1\n",
    "        \n",
    "    for t_i in tail_neighbors:\n",
    "        k_hop_adj_mat[t,t_i] = 1\n",
    "        k_hop_adj_mat[t_i,t] = 1\n",
    "\n",
    "    k_hop_adj_mats.append(k_hop_adj_mat)\n",
    "\n",
    "k_hop_adj_mats = np.expand_dims(k_hop_adj_mats,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mask = tf.cast((k_hop_adj_mats*tf.nn.sigmoid(masks)) > .1,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "bce([1.],[1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class New_RGCN_Layer(tf.keras.layers.Layer):\n",
    "#     def __init__(self,num_relations,output_dim,**kwargs):\n",
    "#         super(New_RGCN_Layer,self).__init__(**kwargs)\n",
    "#         self.num_relations = num_relations\n",
    "#         self.output_dim = output_dim\n",
    "        \n",
    "#     def build(self,input_shape):\n",
    "\n",
    "#         input_dim = int(input_shape[-2][-1])\n",
    "        \n",
    "#         self.relation_kernel = self.add_weight(\n",
    "#             shape=(self.num_relations,input_dim, self.output_dim),\n",
    "#             name=\"relation_kernels\",\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "\n",
    "#         self.self_kernel = self.add_weight(\n",
    "#             shape=(input_dim, self.output_dim),\n",
    "#             name=\"self_kernel\",\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             )\n",
    "#         )\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "        \n",
    "#         embeddings,head_idx,head_e,tail_idx,tail_e,adj_mats = inputs\n",
    "        \n",
    "# #         print('embeddings',embeddings.shape)\n",
    "# #         print('head_idx',head_idx.shape)\n",
    "# #         print('head_e',head_e.shape)\n",
    "# #         print('adj_mats',adj_mats.shape)\n",
    "            \n",
    "#         head_output = tf.matmul(head_e,self.self_kernel)\n",
    "#         tail_output = tf.matmul(tail_e,self.self_kernel)\n",
    "        \n",
    "#         #print('head_output',head_output.shape)\n",
    "        \n",
    "#         for i in range(self.num_relations):\n",
    "            \n",
    "#             adj_i = adj_mats[i]\n",
    "\n",
    "#             #print('adj_i',adj_i.shape)\n",
    "            \n",
    "#             head_adj = tf.nn.embedding_lookup(adj_i,head_idx)\n",
    "#             tail_adj = tf.nn.embedding_lookup(adj_i,tail_idx)\n",
    "            \n",
    "#             #print('head_adj',head_adj.shape)\n",
    "            \n",
    "#             #print('head_adj',head_adj.shape)\n",
    "#             #print('embeddings',embeddings.shape)\n",
    "            \n",
    "#             head_update = tf.matmul(head_adj,embeddings)\n",
    "#             tail_update = tf.matmul(tail_adj,embeddings)\n",
    "\n",
    "#             head_output += tf.matmul(head_update,self.relation_kernel[i])\n",
    "#             tail_output += tf.matmul(tail_update,self.relation_kernel[i])\n",
    "       \n",
    "#         return head_output, tail_output\n",
    "    \n",
    "# class DistMult(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_relations,**kwargs):\n",
    "#         super(DistMult,self).__init__(**kwargs)\n",
    "#         self.num_relations = num_relations\n",
    "        \n",
    "#     def build(self,input_shape):\n",
    "        \n",
    "#         embedding_dim = input_shape[0][-1]\n",
    "        \n",
    "#         self.kernel = self.add_weight(\n",
    "#             shape=(self.num_relations,embedding_dim),\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             ),\n",
    "#             name='rel_embedding'\n",
    "#         )\n",
    "        \n",
    "#     def call(self,inputs):\n",
    "        \n",
    "#         head_e,rel_idx,tail_e = inputs\n",
    "        \n",
    "#         rel_e = tf.nn.embedding_lookup(self.kernel,rel_idx)\n",
    "        \n",
    "#         score = tf.sigmoid(tf.reduce_sum(head_e*rel_e*tail_e,axis=-1))\n",
    "        \n",
    "#         return tf.expand_dims(score,axis=0)\n",
    "#         embeddings,head_idx,tail_idx,head_e,tail_e,adj_mats = inputs\n",
    "\n",
    "#         adj_mats = tf.squeeze(adj_mats,axis=0)\n",
    "#         embeddings = tf.squeeze(embeddings,axis=0)\n",
    "\n",
    "#         head_output = tf.matmul(head_e,self.self_kernel)\n",
    "#         tail_output = tf.matmul(tail_e,self.self_kernel)\n",
    "        \n",
    "#         for i in range(self.num_relations):\n",
    "            \n",
    "#             adj_i = adj_mats[i]\n",
    "\n",
    "#             head_adj = tf.nn.embedding_lookup(adj_i,head_idx)\n",
    "#             tail_adj = tf.nn.embedding_lookup(adj_i,tail_idx)\n",
    "            \n",
    "#             h_head = tf.matmul(head_adj,embeddings)\n",
    "#             h_tail = tf.matmul(head_adj,embeddings)\n",
    "            \n",
    "#             head_output += tf.matmul(h_head,self.relation_kernel[i])\n",
    "#             tail_output += tf.matmul(h_tail,self.relation_kernel[i])\n",
    "\n",
    "#         return head_output,tail_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = tf.data.Dataset.from_tensor_slices((\n",
    "#         train2idx[:,:,0],\n",
    "#         train2idx[:,:,1],\n",
    "#         train2idx[:,:,2], \n",
    "#         np.ones(train2idx.shape[1]).reshape(1,-1)\n",
    "#     )\n",
    "# ).batch(1)\n",
    "\n",
    "# for h,r,t,y in data:\n",
    "\n",
    "#     neg_head, neg_tail = utils.get_negative_triples(head=h,rel=r,tail=t,num_entities=NUM_ENTITIES)\n",
    "#     print(h)\n",
    "#     print(neg_head)\n",
    "#     print(t) \n",
    "#     print(neg_tail)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_entities = tf.keras.Input(shape=(NUM_ENTITIES,), name='all_entities',dtype=tf.int64)\n",
    "# head_input = tf.keras.Input(shape=(None,), name='head_input',batch_size=1,dtype=tf.int64)\n",
    "# rel_input = tf.keras.Input(shape=(None,), name='rel_input',batch_size=1,dtype=tf.int64)\n",
    "# tail_input = tf.keras.Input(shape=(None,), name='tail_input',batch_size=1,dtype=tf.int64)\n",
    "\n",
    "# adj_inputs = tf.keras.Input(\n",
    "#         shape=(\n",
    "#             NUM_RELATIONS,\n",
    "#             NUM_ENTITIES,\n",
    "#             NUM_ENTITIES\n",
    "#         ),\n",
    "#         dtype=tf.float32,\n",
    "#         name='adj_inputs'\n",
    "#     )\n",
    "\n",
    "# entity_embeddings = Embedding(\n",
    "#         input_dim=NUM_ENTITIES,\n",
    "#         output_dim=EMBEDDING_DIM,\n",
    "#         name='entity_embeddings',\n",
    "#         embeddings_initializer=tf.keras.initializers.RandomUniform(\n",
    "#             minval=-1,\n",
    "#             maxval=1,\n",
    "#             seed=SEED\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# all_e = entity_embeddings(all_entities)\n",
    "# head_e = entity_embeddings(head_input)\n",
    "# tail_e = entity_embeddings(tail_input)\n",
    "\n",
    "# all_e = tf.keras.layers.Lambda(lambda x:x[0,:,:])(all_e)\n",
    "# head_e = tf.keras.layers.Lambda(lambda x:x[0,:,:])(head_e)\n",
    "# tail_e = tf.keras.layers.Lambda(lambda x:x[0,:,:])(tail_e)\n",
    "\n",
    "# head_index = tf.keras.layers.Lambda(lambda x:x[0,:])(head_input)\n",
    "# rel_index = tf.keras.layers.Lambda(lambda x:x[0,:])(rel_input)\n",
    "# tail_index = tf.keras.layers.Lambda(lambda x:x[0,:])(tail_input)\n",
    "\n",
    "# adj_mats_layer = tf.keras.layers.Lambda(lambda x:x[0,:,:])(adj_inputs)\n",
    "# #embeddings,head_idx,head_e,tail_idx,tail_e,adj_mats\n",
    "\n",
    "# new_head,new_tail = New_RGCN_Layer(NUM_RELATIONS,OUTPUT_DIM)([all_e,head_index,head_e,tail_index,tail_e,adj_mats_layer])\n",
    "# #new_head = New_RGCN_Layer(NUM_RELATIONS,OUTPUT_DIM)([all_entities,])\n",
    "\n",
    "# output = DistMult(num_relations=NUM_RELATIONS,name='output')([new_head,rel_index,new_tail])\n",
    "\n",
    "# output = tf.keras.layers.Dense(train2idx.shape[1],activation='sigmoid')(output)\n",
    "\n",
    "# #output = tf.keras.layers.Dense(1)(output)\n",
    "# #output = tf.keras.layers.Reshape((1,))(output)\n",
    "# # m = tf.keras.Model([head_input],[out])\n",
    "# # m([train2idx[:,0:32,0]])\n",
    "# m = tf.keras.Model([all_entities,head_input,rel_input,tail_input,adj_inputs],[output])\n",
    "\n",
    "# m([np.arange(NUM_ENTITIES).reshape(1,-1),train2idx[:,:,0],train2idx[:,:,1],train2idx[:,:,2],adj_mats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.compile(\n",
    "#     loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "#     optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# )\n",
    "\n",
    "# #m.summary()\n",
    "# m.fit(x=[\n",
    "#     all_indices,\n",
    "#     train2idx[:,:,0],\n",
    "#     train2idx[:,:,1],\n",
    "#     train2idx[:,:,2],\n",
    "#     adj_mats\n",
    "# ],\n",
    "#     y=np.ones(train2idx.shape[1]).reshape(1,-1),\n",
    "#     epochs=NUM_EPOCHS,\n",
    "#     batch_size=1,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = m.predict(\n",
    "#     x=[\n",
    "#         all_indices,\n",
    "#         train2idx[:,:,0],\n",
    "#         train2idx[:,:,1],\n",
    "#         train2idx[:,:,2],\n",
    "#         adj_mats\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m([all_indices,train2idx[:,0:1,0],train2idx[:,0:1,1],train2idx[:,0:1,2],masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_model = tf.keras.Model(inputs=m.input,outputs=m.get_layer('entity_embeddings').output)\n",
    "\n",
    "#embeddings = m.get_layer('entity_embeddings').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_embeddings,tail_embeddings = embeddings_model([\n",
    "#     all_indices,\n",
    "#     train2idx[:,0:1,0],\n",
    "#     train2idx[:,0:1,1],\n",
    "#     train2idx[:,0:1,2],\n",
    "#     adj_mats\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train2idx[0,:,:][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.where(adj_mats[0][2][100]==1.).numpy()\n",
    "#np.argwhere(train2idx[0,:,2]== 100)\n",
    "\n",
    "# for h,_,t in train2idx[:,822,:]:\n",
    "    \n",
    "#     head_neighbors = tf.where(adj_mats[0][2][h]==1.).numpy()\n",
    "#     tail_neighbors = tf.where(adj_mats[0][2][t]==1.).numpy()\n",
    "#tf.where(adj_mats[0][2][100]==1.).numpy()\n",
    "#tf.where(adj_mats[0,2,[100,100,100]]==1.)[:,-1]\n",
    "#train2idx[:,822,[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.nn.embedding_lookup(embeddings,train2idx[0,0:1,0])\n",
    "#k_hop_adj_mats*tf.nn.sigmoid(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for triple in test set:\n",
    "    #get k hop subgraph of each triple (all neighbors of head/tail)\n",
    "    #compute adjacency matrix of subgraph (1,NUM_RELATIONS,NUM_ENTITIES,NUM_ENTITIES)\n",
    "    #compute pred of triple\n",
    "    #for i in iter:\n",
    "        #learn mask\n",
    "        #mask * adj matrix \n",
    "    #mask * adj matrix-> reduce output to ints\n",
    "#edge_mask = tf.cast((k_hop_adj_mats*tf.nn.sigmoid(masks)) > .1,dtype=tf.int32)\n",
    "#tf.nn.sigmoid(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_indices = tf.where(edge_mask[0,2,h,:] == 1)\n",
    "tail_indices = tf.where(edge_mask[0,2,t,:] == 1)\n",
    "# swap = tf.concat([tf.reshape(indices[:,1],(-1,1)),tf.reshape(indices[:,0],(-1,1))],axis=1)\n",
    "\n",
    "# unique_indices = tf.concat([indices,swap],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
