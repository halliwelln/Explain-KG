{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import random as rn\n",
    "import RGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "\n",
    "RULE = 'aunt'\n",
    "\n",
    "data = np.load(os.path.join('..','data','royalty.npz'))\n",
    "\n",
    "triples, traces = data[RULE + '_triples'], data[RULE + '_traces']\n",
    "\n",
    "entities = data[RULE + '_entities'].tolist()\n",
    "relations = data[RULE + '_relations'].tolist()\n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "EMBEDDING_DIM = 50\n",
    "OUTPUT_DIM = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "triples2idx = utils.array2idx(triples, ent2idx,rel2idx)\n",
    "traces2idx = utils.array2idx(traces, ent2idx,rel2idx)\n",
    "\n",
    "# adj_mats = utils.get_adjacency_matrix_list(\n",
    "#     num_relations=NUM_RELATIONS,\n",
    "#     num_entities=NUM_ENTITIES,\n",
    "#     data=train2idx\n",
    "# )\n",
    "\n",
    "#train2idx = np.expand_dims(train2idx,axis=0)\n",
    "all_indices = np.arange(NUM_ENTITIES).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = tf.concat([triples2idx,tf.reshape(traces2idx,(-1,3))],axis=0)\n",
    "NUM_TRIPLES = full_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method RGCN_Layer.call of <RGCN.RGCN_Layer object at 0x1945c61610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method RGCN_Layer.call of <RGCN.RGCN_Layer object at 0x1945c61610>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DistMult.call of <RGCN.DistMult object at 0x1945c6ac50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DistMult.call of <RGCN.DistMult object at 0x1945c6ac50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "model = RGCN.get_RGCN_Model(\n",
    "        num_entities=NUM_ENTITIES,\n",
    "        num_relations=NUM_RELATIONS,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        seed=SEED\n",
    "    )\n",
    "model.load_weights(os.path.join('..','data','weights',RULE+'.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mats = utils.get_adj_mats(full_data,NUM_ENTITIES,NUM_RELATIONS,reshape=True)\n",
    "all_indices = np.arange(NUM_ENTITIES).reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_computation_graph(head,rel,tail,data,num_relations):\n",
    "    \n",
    "    '''Get 1st degree neighbors of head and tail'''\n",
    "     \n",
    "    subset = data[data[:,:,1] == rel]\n",
    "\n",
    "    neighbors_head = tf.concat([subset[subset[:,0] == head],\n",
    "                                subset[subset[:,2] == head]],axis=0)\n",
    "    neighbors_tail = tf.concat([subset[(subset[:,0] == tail) & (subset[:,0] != head)],\n",
    "                                subset[(subset[:,2] == tail) & (subset[:,0] != head)]],axis=0)\n",
    "\n",
    "    all_neighbors = tf.concat([neighbors_head,neighbors_tail],axis=0)\n",
    "\n",
    "    return all_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = tf.expand_dims(full_data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "head = triples2idx[i,0]\n",
    "rel = triples2idx[i,1]\n",
    "tail = triples2idx[i,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_subgraphs = utils.get_adj_mats(traces2idx[i],NUM_ENTITIES,NUM_RELATIONS,reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_graph = get_computation_graph(head,rel,tail,full_data,NUM_RELATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mats = utils.get_adj_mats(comp_graph, NUM_ENTITIES, NUM_RELATIONS,reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [tf.Variable(\n",
    "        initial_value=tf.random.normal(\n",
    "            (1,NUM_ENTITIES,NUM_ENTITIES), \n",
    "            mean=0, \n",
    "            stddev=1, \n",
    "            dtype=tf.float32, \n",
    "            seed=SEED),\n",
    "        name='mask_'+str(i),\n",
    "        trainable=True) for i in range(NUM_RELATIONS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[11.512925]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['mask_0:0', 'mask_1:0', 'mask_2:0', 'mask_3:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-414f14309a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    511\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \"\"\"\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m   1269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0;32m-> 1271\u001b[0;31m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[1;32m   1272\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m     logging.warning(\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ['mask_0:0', 'mask_1:0', 'mask_2:0', 'mask_3:0']."
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        tape.watch(masks)\n",
    "        \n",
    "        masked_adjs = []\n",
    "        \n",
    "        for i in range(NUM_RELATIONS):\n",
    "            \n",
    "            mask_adj = \u0010adj_mats[i] * tf.nn.sigmoid(masks[i])\n",
    "\n",
    "            thresholded_indices = mask_adj.indices[mask_adj.values > .1]\n",
    "    \n",
    "            masked_adj = tf.sparse.SparseTensor(\n",
    "                    indices=thresholded_indices,\n",
    "                    values=tf.ones(thresholded_indices.shape[0]),\n",
    "                    dense_shape=(1,NUM_ENTITIES,NUM_ENTITIES)\n",
    "                    )\n",
    "    \n",
    "            masked_adjs.append(masked_adj)\n",
    "        \n",
    "        y_pred = model(\n",
    "                    [\n",
    "                    all_indices,\n",
    "                    tf.reshape(head,shape=(-1,1)),\n",
    "                    tf.reshape(rel,shape=(-1,1)),\n",
    "                    tf.reshape(tail,shape=(-1,1)),\n",
    "                    masked_adjs\n",
    "                    ]\n",
    "                )\n",
    "        #sig_masks = tf.nn.sigmoid(masks)\n",
    "\n",
    "        penalty = tf.reduce_sum(masks)\n",
    "\n",
    "        loss = -1 * tf.math.log(y_pred +.00001) #+ (0.00001*penalty)\n",
    "    print(loss)\n",
    "    grads = tape.gradient(loss,masks)\n",
    "    optimizer.apply_gradients(zip(grads,masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_subgraphs(adj_mats,masks,true_subgraphs,num_entities,num_relations,thresh=.5):\n",
    "        \n",
    "    for i in range(num_relations):\n",
    "        \n",
    "        true_graph = tf.sparse.to_dense(true_subgraphs[i])\n",
    "\n",
    "        mask_i = adj_mats[i] * tf.nn.sigmoid(masks[i])\n",
    "                \n",
    "        non_masked_indices = mask_i.indices[mask_i.values > .5]\n",
    "        \n",
    "        pred_graph = tf.sparse.SparseTensor(\n",
    "            indices=non_masked_indices,\n",
    "            values=tf.ones(non_masked_indices.shape[0]),\n",
    "            dense_shape=(1,num_entities,num_entities)\n",
    "            )\n",
    "\n",
    "        pred_graph = tf.sparse.to_dense(pred_graph)\n",
    "        \n",
    "        #score = utils.tf_binary_jaccard(true_graph,pred_graph)\n",
    " \n",
    "        m11 = tf.reduce_sum(tf.cast(tf.math.logical_and(true_graph==1,\n",
    "                                                    pred_graph==1),dtype=tf.int32))\n",
    "    \n",
    "        print(m11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_pred_subgraphs(adj_mats,masks,true_subgraphs,NUM_ENTITIES,NUM_RELATIONS,thresh=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.sparse.SparseTensor(\n",
    "        indices=thresholded_indices,\n",
    "        values=tf.ones(thresholded_indices.shape[0]),\n",
    "        dense_shape=(1,NUM_ENTITIES,NUM_ENTITIES)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = []\n",
    "# values = []\n",
    "# for row in range(masks[0].shape[1]):\n",
    "#     for col in range(masks[0].shape[1]):\n",
    "#         indices.append([row,col])\n",
    "#         values.append(masks[0][0][row,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(\n",
    "            [\n",
    "            all_indices,\n",
    "            tf.reshape(head,shape=(-1,1)),\n",
    "            tf.reshape(rel,shape=(-1,1)),\n",
    "            tf.reshape(tail,shape=(-1,1)),\n",
    "            masked_adjs\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_embeddings = model.get_layer('output').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_kernel, self_kernel = model.get_layer('rgcn__layer').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = model.get_layer('entity_embeddings').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_output = tf.matmul(tf.reshape(entity_embeddings[head],(1,-1)),self_kernel)\n",
    "tail_output = tf.matmul(tf.reshape(entity_embeddings[tail],(1,-1)),self_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_RELATIONS):\n",
    "    adj_i = masks[i][0]\n",
    "    sum_embeddings = tf.matmul(adj_i,entity_embeddings)\n",
    "    head_update = tf.reshape(sum_embeddings[head],(1,-1))\n",
    "    tail_update = tf.reshape(sum_embeddings[tail],(1,-1))\n",
    "    \n",
    "    head_output += tf.matmul(head_update,relation_kernel[i])\n",
    "    tail_output += tf.matmul(tail_update,relation_kernel[i])\n",
    "head_output = tf.sigmoid(head_output)\n",
    "tail_output = tf.sigmoid(tail_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-69.73706]], dtype=float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(tf.matmul(head_output,relation_kernel[rel]),tf.transpose(tail_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-63.82659>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(head_output * relation_kernel[rel] * tail_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.cast(tf.sigmoid(masks) > .5,dtype=tf.float32)\n",
    "#np.argwhere(tf.sparse.to_dense(adj_mats[1])[0].numpy()==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_subgraphs = utils.get_adj_mats(traces2idx[0],NUM_ENTITIES,NUM_RELATIONS,reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in true_subgraphs:\n",
    "    print(tf.reduce_sum(tf.sparse.to_dense(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces2idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head,rel,tail = train2idx[0]\n",
    "print(head)\n",
    "print(rel)\n",
    "print(tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distinct(a):\n",
    "    _a = np.unique(a,axis=0)\n",
    "    return _a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_graph = get_computation_graph(head,rel,tail,train2idx,NUM_RELATIONS)\n",
    "symmetric_comp_graph = tf.concat([tf.gather(comp_graph,[0,2],axis=1),\n",
    "                                  tf.gather(comp_graph,[2,0],axis=1)],axis=0)\n",
    "\n",
    "distinct_comp_graph = tf.py_function(distinct,[symmetric_comp_graph],symmetric_comp_graph.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mats = utils.get_adj_mats(distinct_comp_graph, NUM_ENTITIES, NUM_RELATIONS,reshape=True)\n",
    "\n",
    "tf.sparse.to_dense(adj_mats[0]).numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.concatenate([train2idx[0][:,[0,2]],train2idx[0][:,[2,0]]],axis=0)\n",
    "\n",
    "# a = tf.sparse.SparseTensor(indices=indices,values=np.ones((indices.shape[0])),dense_shape=(NUM_ENTITIES,NUM_ENTITIES))\n",
    "# a = tf.sparse.reorder(\n",
    "#     a\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_adj_mats(data,num_entities,num_relations):\n",
    "\n",
    "#         adj_mats = []\n",
    "\n",
    "#         for i in range(num_relations):\n",
    "\n",
    "#             data_i = data[data[:,1] == i]\n",
    "\n",
    "#             indices = np.concatenate([data_i[:,[0,2]],data_i[:,[2,0]]],axis=0)\n",
    "\n",
    "#             sparse_mat = tf.sparse.SparseTensor(\n",
    "#                 indices=indices,\n",
    "#                 values=np.ones((indices.shape[0])),\n",
    "#                 dense_shape=(num_entities,num_entities)\n",
    "#                 )\n",
    "\n",
    "#             sparse_mat = tf.sparse.reorder(sparse_mat)\n",
    "\n",
    "#             sparse_mat = tf.sparse.reshape(sparse_mat, shape=(1,num_entities,num_entities))\n",
    "\n",
    "#             adj_mats.append(sparse_mat)\n",
    "\n",
    "#         return adj_mats\n",
    "\n",
    "# adj_mats = get_adj_mats(train2idx[0],NUM_ENTITIES,NUM_RELATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train2idx = tf.convert_to_tensor(train2idx[0])\n",
    "\n",
    "def computation_graph_mats(head,tail,data,num_relations,num_entities):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbors_head = np.concatenate([train2idx[0][(train2idx[0][:,0] == 7874)],train2idx[0][(train2idx[0][:,2] == 7874)]])\n",
    "# neighbors_tail = np.concatenate([train2idx[0][(train2idx[0][:,0] == 8589)],train2idx[0][(train2idx[0][:,2] == 8589)]])\n",
    "\n",
    "# all_neighbors = np.concatenate([neighbors_head,neighbors_tail], axis=0)\n",
    "\n",
    "# indices = np.unique(all_neighbors[:,[1,0,2]], axis=0)\n",
    "\n",
    "# a = tf.sparse.SparseTensor(indices=indices,values=np.ones((indices.shape[0])),dense_shape=(1,NUM_ENTITIES,NUM_ENTITIES))\n",
    "# a = tf.sparse.reorder(\n",
    "#     a\n",
    "#)   \n",
    "#k_hop_adj_mats = tf.sparse.expand_dims(a, axis=0)\n",
    "# for i in neighbors_1_head[:,2]:\n",
    "#     print(i,train2idx[0][train2idx[0][:,0] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,r,t = train2idx[0,3,:]\n",
    "\n",
    "#filter train2idx to get neighbors -> feed into utils.get_adj_mats\n",
    "\n",
    "head = np.array([h])\n",
    "rel = np.array([r])\n",
    "tail = np.array([t])\n",
    "\n",
    "tf_k_hop_adj_mats = []\n",
    "\n",
    "for i in range(NUM_RELATIONS):\n",
    "    \n",
    "    data = tf_train2idx[tf_train2idx[:,1] == i]\n",
    "\n",
    "    tf_neighbors_head = tf.concat([data[data[:,0] == head],data[data[:,2] == head]],axis=0)\n",
    "    tf_neighbors_tail = tf.concat([data[(data[:,0] == tail) & (data[:,0] != head)],data[(data[:,2] == tail) & (data[:,0] != head)]],axis=0)\n",
    "\n",
    "    tf_all_neighbors = tf.concat([tf_neighbors_head,tf_neighbors_tail],axis=0)\n",
    "    \n",
    "    tf_indices = tf.transpose(tf.stack([tf_all_neighbors[:,0],tf_all_neighbors[:,2]]))\n",
    "\n",
    "    tf_k_hop_adj_mat = tf.sparse.SparseTensor(\n",
    "        indices=tf_indices,\n",
    "        values=tf.ones((tf_indices.shape[0])),\n",
    "        dense_shape=(NUM_ENTITIES,NUM_ENTITIES)\n",
    "    )\n",
    "    \n",
    "    tf_k_hop_adj_mat = tf.sparse.reorder(tf_k_hop_adj_mat)\n",
    "    \n",
    "    tf_k_hop_adj_mats.append(tf_k_hop_adj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_computation_graph(head,rel,tail,data,num_relations):\n",
    "    \n",
    "    '''Get 1st degree neighbors of head and tail'''\n",
    "     \n",
    "    subset = data[data[:,1] == rel]\n",
    "\n",
    "    neighbors_head = tf.concat([data[data[:,0] == head],\n",
    "                                data[data[:,2] == head]],axis=0)\n",
    "    neighbors_tail = tf.concat([data[(data[:,0] == tail) & (data[:,0] != head)],\n",
    "                                data[(data[:,2] == tail) & (data[:,0] != head)]],axis=0)\n",
    "\n",
    "    all_neighbors = tf.concat([neighbors_head,neighbors_tail],axis=0)\n",
    "\n",
    "    return all_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_mats(data,num_entities,num_relations,reshape=True):\n",
    "\n",
    "    adj_mats = []\n",
    "\n",
    "    for i in range(num_relations):\n",
    "\n",
    "        data_i = data[data[:,1] == i]\n",
    "   \n",
    "        indices = tf.concat([tf.gather(data_i,[0,2],axis=1),tf.gather(data_i,[2,0],axis=1)],axis=0)\n",
    "        \n",
    "        sparse_mat = tf.sparse.SparseTensor(\n",
    "            indices=indices,\n",
    "            values=tf.ones((indices.shape[0])),\n",
    "            dense_shape=(num_entities,num_entities)\n",
    "            )\n",
    "\n",
    "        sparse_mat = tf.sparse.reorder(sparse_mat)\n",
    "        \n",
    "        if reshape:\n",
    "            \n",
    "            sparse_mat = tf.sparse.reshape(sparse_mat, shape=(1,num_entities,num_entities))\n",
    "\n",
    "        adj_mats.append(sparse_mat)\n",
    "\n",
    "    return adj_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.sparse.SparseTensor(indices=indices,values=np.ones((indices.shape[0])),dense_shape=(NUM_RELATIONS,NUM_ENTITIES,NUM_ENTITIES))\n",
    "#tf_indices = tf.transpose(tf.stack([tf_all_neighbors[:,1],tf_all_neighbors[:,0],tf_all_neighbors[:,2]]))\n",
    "\n",
    "c_graph = get_computation_graph(head,rel,tail,tf_train2idx,NUM_RELATIONS)\n",
    "#get_adj_mats(c_graph, NUM_ENTITIES, NUM_RELATIONS,reshape=False)[1].indices\n",
    "c_graph\n",
    "#c_graph[c_graph[:,1] == 0]\n",
    "#tf.concat([tf_train2idx[tf_train2idx[:,0] == head],tf_train2idx[tf_train2idx[:,2] == head]],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_k_hop_adj_mats = tf.sparse.SparseTensor(\n",
    "#     indices=tf_indices,\n",
    "#     values=tf.ones((tf_indices.shape[0])),\n",
    "#     dense_shape=(NUM_RELATIONS,NUM_ENTITIES,NUM_ENTITIES))\n",
    "# tf_k_hop_adj_mats = tf.sparse.reorder(tf_k_hop_adj_mats)   \n",
    "# tf_k_hop_adj_mats = tf.sparse.expand_dims(tf_k_hop_adj_mats, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RGCN.get_RGCN_Model(\n",
    "        num_triples=NUM_TRIPLES,\n",
    "        num_entities=NUM_ENTITIES,\n",
    "        num_relations=NUM_RELATIONS,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        output_dim=OUTPUT_DIM,\n",
    "        seed=SEED\n",
    "    )\n",
    "#model.load_weights(os.path.join('..','data','weights','rgcn.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_data = tf.expand_dims(tf_train2idx,axis=0)\n",
    "\n",
    "# model([all_indices, tf_data[:,:,0],tf_data[:,:,1],tf_data[:,:,2],tf_k_hop_adj_mats]) < .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [tf.Variable(\n",
    "            initial_value=tf.random.normal(\n",
    "                (NUM_ENTITIES,NUM_ENTITIES), \n",
    "                mean=0, \n",
    "                stddev=1, \n",
    "                dtype=tf.dtypes.float32, \n",
    "                seed=SEED),\n",
    "            name='mask_'+str(i),\n",
    "            trainable=True) for i in range(NUM_RELATIONS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bce = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_init = model(\n",
    "#             [\n",
    "#             all_indices,\n",
    "#             head.reshape(1,1),\n",
    "#             rel.reshape(1,1),\n",
    "#             tail.reshape(1,1),\n",
    "#             tf_k_hop_adj_mats\n",
    "#             ]\n",
    "#         )\n",
    "# y_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model([\n",
    "#             all_indices,\n",
    "#             train2idx[:,:,0],\n",
    "#             train2idx[:,:,1],\n",
    "#             train2idx[:,:,2],\n",
    "#             tf_k_hop_adj_mats\n",
    "#             ]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        tape.watch(masks)\n",
    "        \n",
    "        masked_adj = []\n",
    "        \n",
    "        for i in range(NUM_RELATIONS):\n",
    "            \n",
    "            masked_adj.append(tf_k_hop_adj_mats[i] * tf.nn.sigmoid(masks[i]))\n",
    "            \n",
    "        y_pred = model(\n",
    "            [\n",
    "            all_indices,\n",
    "            head.reshape(1,1),\n",
    "            rel.reshape(1,1),\n",
    "            tail.reshape(1,1),\n",
    "            masked_adj\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(y_pred)\n",
    "        loss = -1*tf.math.log(y_pred+.00001) + tf.reduce_mean(tf.nn.sigmoid(masks))\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "    grads = tape.gradient(loss,masks)\n",
    "    optimizer.apply_gradients(zip(grads,masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_subgraph = tf.cast((tf_k_hop_adj_mats*tf.nn.sigmoid(masks)) > .5,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_subgraph = [tf.cast((tf_k_hop_adj_mats[i]*tf.nn.sigmoid(masks[i])) > .5,dtype=tf.int32)\n",
    "#                  for i in range(NUM_RELATIONS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_subgraphs = get_adj_mats(tf_trainexp2idx[0],NUM_ENTITIES,NUM_RELATIONS,reshape=False)\n",
    "\n",
    "for i in range(NUM_RELATIONS):\n",
    "    \n",
    "    mask_i = tf_k_hop_adj_mats[i]*masks[i]\n",
    "    \n",
    "    if mask_i.indices.shape[0]:\n",
    "\n",
    "        non_masked_indices = mask_i.indices[mask_i.values > .5]\n",
    "\n",
    "        pred_graph = tf.sparse.SparseTensor(\n",
    "            indices=non_masked_indices,\n",
    "            values=tf.ones(non_masked_indices.shape[0]),\n",
    "            dense_shape=(NUM_ENTITIES,NUM_ENTITIES)\n",
    "        )\n",
    "        \n",
    "        pred_graph = tf.sparse.to_dense(pred_graph)\n",
    "        \n",
    "        true_graph = true_subgraphs[i]\n",
    "        \n",
    "        print(tf_binary_jaccard(true_graph,pred_graph))\n",
    "#    print(tf.sparse.to_dense(computation_graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODAY:\n",
    "#jaccard gnn explainer\n",
    "#name weight file -> retrain RGCN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.math.logical_and(tf_k_hop_adj_mats[1]==1.,tf_k_hop_adj_mats[1]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(tf_k_hop_adj_mats[1]* masks[1]).values > .5\n",
    "#a = tf.sparse.to_dense(tf_k_hop_adj_mats[1])\n",
    "#tf.reduce_sum(tf.cast(tf.math.logical_and(a==1,a==1), tf.float32))\n",
    "#tf_k_hop_adj_mats[1].indices[(tf_k_hop_adj_mats[1]* masks[1]).values > .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train2idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 1], [2, 3],[1,1],[2,3],[3,4]])\n",
    "uni,index=np.unique(a,axis=0,return_index=True)\n",
    "print(index)\n",
    "print(a[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triples,traces = utils.parse_ttl(\n",
    "#     file_name=os.path.join('..','data','traces','spouse'+'.ttl'),\n",
    "#     max_padding=3\n",
    "# )\n",
    "#entities = data['spouse_entities']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RULE = 'spouse'\n",
    "\n",
    "data = np.load(os.path.join('..','data','royalty.npz'))\n",
    "\n",
    "entities = data[RULE + '_entities'].tolist()\n",
    "relations = data[RULE + '_relations'].tolist()  \n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "triples = data[RULE+'_triples']\n",
    "traces = data[RULE+'_traces']\n",
    "\n",
    "triples2idx = utils.array2idx(triples,ent2idx,rel2idx)\n",
    "traces2idx = utils.array2idx(traces,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    triples2idx,\n",
    "    traces2idx,\n",
    "    test_size=0.3,\n",
    "    random_state=SEED\n",
    ")\n",
    "X_train = np.concatenate([X_train,y_train.reshape(-1,3)],axis=0)\n",
    "X_train = np.unique(X_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_no_unseen(X, test_size=100, seed=0, allow_duplication=False, filtered_test_predicates=None):\n",
    "\n",
    "    if type(test_size) is float:\n",
    "        test_size = int(len(X) * test_size)\n",
    "\n",
    "    rnd = np.random.RandomState(seed)\n",
    "\n",
    "    subs, subs_cnt = np.unique(X[:, 0], return_counts=True)\n",
    "    objs, objs_cnt = np.unique(X[:, 2], return_counts=True)\n",
    "    rels, rels_cnt = np.unique(X[:, 1], return_counts=True)\n",
    "    dict_subs = dict(zip(subs, subs_cnt))\n",
    "    dict_objs = dict(zip(objs, objs_cnt))\n",
    "    dict_rels = dict(zip(rels, rels_cnt))\n",
    "\n",
    "    idx_test = np.array([], dtype=int)\n",
    "\n",
    "    loop_count = 0\n",
    "    tolerance = len(X) * 10\n",
    "    # Set the indices of test set triples. If filtered, reduce candidate triples to certain predicate types.\n",
    "    if filtered_test_predicates:\n",
    "        test_triples_idx = np.where(np.isin(X[:, 1], filtered_test_predicates))[0]\n",
    "    else:\n",
    "        test_triples_idx = np.arange(len(X))\n",
    "\n",
    "    while idx_test.shape[0] < test_size:\n",
    "        i = rnd.choice(test_triples_idx)\n",
    "        if dict_subs[X[i, 0]] > 1 and dict_objs[X[i, 2]] > 1 and dict_rels[X[i, 1]] > 1:\n",
    "            dict_subs[X[i, 0]] -= 1\n",
    "            dict_objs[X[i, 2]] -= 1\n",
    "            dict_rels[X[i, 1]] -= 1\n",
    "            if allow_duplication:\n",
    "                idx_test = np.append(idx_test, i)\n",
    "            else:\n",
    "                idx_test = np.unique(np.append(idx_test, i))\n",
    "\n",
    "        loop_count += 1\n",
    "\n",
    "        # in case can't find solution\n",
    "        if loop_count == tolerance:\n",
    "            if allow_duplication:\n",
    "                raise Exception(\"Cannot create a test split of the desired size. \"\n",
    "                                \"Some entities will not occur in both training and test set. \"\n",
    "                                \"Change seed values, remove filter on test predicates or set \"\n",
    "                                \"test_size to a smaller value.\")\n",
    "            else:\n",
    "                raise Exception(\"Cannot create a test split of the desired size. \"\n",
    "                                \"Some entities will not occur in both training and test set. \"\n",
    "                                \"Set allow_duplication=True,\"\n",
    "                                \"change seed values, remove filter on test predicates or \"\n",
    "                                \"set test_size to a smaller value.\")\n",
    "    idx = np.arange(len(X))\n",
    "    idx_train = np.setdiff1d(idx, idx_test)\n",
    "\n",
    "    return idx_train,idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train,idx_test = train_test_split_no_unseen(\n",
    "    triples2idx, \n",
    "    test_size=50,\n",
    "    seed=0, \n",
    "    allow_duplication=False, \n",
    "    filtered_test_predicates=None)\n",
    "\n",
    "idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(triples[triples[:,1] == 'spouse'].shape)\n",
    "# #print(triples[triples[:,1] == 'sister'].shape)\n",
    "# print(triples.shape)\n",
    "#print((count>1).sum())\n",
    "#print(unique[count>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(np.concatenate([data['spouse_triples'],data['spouse_traces'].reshape(-1,3)],axis=0),axis=0).shape\n",
    "#np.unique(np.concatenate([X_train,y_train.reshape(-1,3)],axis=0),axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_masks = tf.nn.sigmoid(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sig_masks = tf.cast(sig_masks,dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.boolean_mask(concat,concat==[5780,5780])\n",
    "# for idx,i in enumerate(concat==[5780]):\n",
    "#     if i.numpy().all():\n",
    "#         print(idx,i)\n",
    "# idx = [i for i in range(concat.shape[0]) if i != 4981]\n",
    "# tf.gather(concat,idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni,index = np.unique(triples,axis=0,return_index=True)\n",
    "# #print(uni==triples)\n",
    "# # print(triples.shape)\n",
    "# # print(uni.shape)\n",
    "# # print(index.shape)\n",
    "# # print(counts.shape)\n",
    "# # print(triples[index][0])\n",
    "# # print(traces[index][0])\n",
    "\n",
    "# for i in range(triples.shape[0]):\n",
    "#     if i not in index:\n",
    "#         print(i)\n",
    "# # print(triples[924])\n",
    "# # print(traces[924])\n",
    "# np.argwhere(triples[:,0] == 'Uzana_I_of_Pinya')\n",
    "# print(traces[[922,923,924]])\n",
    "# print(triples[[922,923,924]])\n",
    "\n",
    "#triples = np.unique(triples,axis=0)\n",
    "#traces = np.unique(traces,axis=0)\n",
    "\n",
    "# _, unique_idx = np.unique(triples,axis=0,return_index=True)\n",
    "\n",
    "# triples = triples[unique_idx]\n",
    "# traces = traces[unique_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_binary_jaccard(true_graph,pred_graph):\n",
    "    \n",
    "    m11 = tf.reduce_sum(tf.cast(tf.math.logical_and(true_graph==1,\n",
    "                                                    pred_graph==1),dtype=tf.int32))\n",
    "    m01 = tf.reduce_sum(tf.cast(tf.math.logical_and(true_graph==0,\n",
    "                                                    pred_graph==1),dtype=tf.int32))\n",
    "    m10 = tf.reduce_sum(tf.cast(tf.math.logical_and(true_graph==1,\n",
    "                                                    pred_graph==0),dtype=tf.int32))\n",
    "    \n",
    "    return m11 / (m01 + m10 + m11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_trainexp2idx = tf.convert_to_tensor(trainexp2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(tf_train2idx[0,0],(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_trainexp2idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = tf_train2idx[0,0]\n",
    "rel = tf_train2idx[0,1]\n",
    "tail = tf_train2idx[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train2idx[tf_train2idx[:,1] == tf_train2idx[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = np.concatenate([train2idx[0][:,[1,0,2]],train2idx[0][:,[1,2,0]]],axis=0)\n",
    "\n",
    "#a = tf.sparse.expand_dims(a, axis=0)\n",
    "\n",
    "# a = tf.sparse.SparseTensor(indices=np.concatenate([np.zeros((train2idx.shape[1],1),dtype=np.int64),train2idx[0][:,[1,0,2]]], axis=1),\n",
    "#                           values=np.ones(train2idx.shape[1]),dense_shape=(1,NUM_RELATIONS,NUM_ENTITIES,NUM_ENTITIES))\n",
    "# a = tf.sparse.reorder(\n",
    "#     a\n",
    "# )\n",
    "\n",
    "#np.concatenate([np.zeros((train2idx.shape[1],1),dtype=np.int64),train2idx[0][:,[1,0,2]]],axis=1)\n",
    "#np.concatenate([np.zeros((train2idx.shape[1],1),dtype=np.int64),train2idx[0][:,[1,2,0]]],axis=1)\n",
    "\n",
    "\n",
    "#[0,4,823,4246]\n",
    "#0,4,1532,9141\n",
    "\n",
    "# np.argwhere(train2idx[0,:,0] == 1532)\n",
    "# triples[5313,:]\n",
    "\n",
    "#np.concatenate([train2idx[0,:,1].reshape(1,-1),train2idx[0,:,0].reshape(-1,1),train2idx[0,:,2].reshape(-1,1)],axis=1)\n",
    "#np.swapaxis(train2idx,)\n",
    "\n",
    "#tf.sparse.to_dense(a)\n",
    "\n",
    "#tf.nn.embedding_lookup_sparse(a, tf.convert_to_tensor([3,3]),sp_weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.nn.embedding_lookup_sparse(a, train2idx[:,0:5,0],sp_weights=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(a_dense[8,15:20,:],tf.ones((14154,5),tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.sparse.sparse_dense_matmul(a,tf.ones((14154,5),tf.float64))[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x=[\n",
    "        all_indices,\n",
    "        train2idx[:,:,0],\n",
    "        train2idx[:,:,1],\n",
    "        train2idx[:,:,2],\n",
    "        adj_mats\n",
    "        ],\n",
    "    y=np.ones(NUM_TRIPLES).reshape(1,-1),\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices((\n",
    "#         train2idx[0,:,0],\n",
    "#         train2idx[0,:,1],\n",
    "#         train2idx[0,:,2], \n",
    "#         np.ones(train2idx.shape[1])\n",
    "#     )\n",
    "# ).batch(BATCH_SIZE)\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "#     for pos_head,rel,pos_tail,y in data:\n",
    "\n",
    "#         neg_head, neg_tail = utils.get_negative_triples(\n",
    "#             head=pos_head, \n",
    "#             rel=rel, \n",
    "#             tail=pos_tail,\n",
    "#             num_entities=NUM_ENTITIES\n",
    "#         )\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "            \n",
    "# #             print(all_indices.shape)\n",
    "# #             print(pos_head.shape)\n",
    "# #             print(adj_mats.shape)\n",
    "\n",
    "#             y_pos_pred = model([\n",
    "#                 all_indices,\n",
    "#                 pos_head,\n",
    "#                 rel,\n",
    "#                 pos_tail,\n",
    "#                 adj_mats\n",
    "#                 ],\n",
    "#                 training=True\n",
    "#             )\n",
    "\n",
    "#             y_neg_pred = model([\n",
    "#                 all_indices,\n",
    "#                 neg_head,\n",
    "#                 rel,\n",
    "#                 neg_tail,\n",
    "#                 adj_mats\n",
    "#                 ],\n",
    "#                 training=True\n",
    "#             )\n",
    "\n",
    "#             y_pred = tf.concat([y_pos_pred,y_neg_pred],axis=0)\n",
    "#             y_true = tf.concat([y,tf.zeros_like(y)],axis=0)\n",
    "\n",
    "#             loss = bce(y_true,y_pred)\n",
    "\n",
    "#         grads = tape.gradient(loss, model.trainable_weights)\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "#     print(f'loss {loss} after epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_indices.shape)\n",
    "print(train2idx[:,:,0].shape)\n",
    "print(train2idx[:,:,1].shape)\n",
    "print(train2idx[:,:,2].shape)\n",
    "print(adj_mats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = model.predict(\n",
    "    x=[\n",
    "        all_indices,\n",
    "        train2idx[:,:,0],\n",
    "        train2idx[:,:,1],\n",
    "        train2idx[:,:,2],\n",
    "        adj_mats\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.where(adj_mats[0,2,:][train2idx[:,0:1,0]]==1.)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model([\n",
    "    all_indices,\n",
    "    train2idx[:,0:1,0],\n",
    "    train2idx[:,0:1,1],\n",
    "    train2idx[:,0:1,2],\n",
    "    k_hop_adj_mats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = model.get_layer('entity_embeddings').get_weights()[0]\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,_,t = train2idx[0,0,:]\n",
    "\n",
    "k_hop_adj_mats = []\n",
    "\n",
    "for r in range(NUM_RELATIONS):\n",
    "\n",
    "    k_hop_adj_mat = np.zeros((NUM_ENTITIES,NUM_ENTITIES))\n",
    "\n",
    "    head_neighbors = tf.where(adj_mats[0,r,:][h]==1.)[:,-1]\n",
    "    tail_neighbors = tf.where(adj_mats[0,r,:][t]==1.)[:,-1]\n",
    "\n",
    "    for h_i in head_neighbors:\n",
    "        k_hop_adj_mat[h,h_i] = 1.\n",
    "        k_hop_adj_mat[h_i,h] = 1.\n",
    "        \n",
    "    for t_i in tail_neighbors:\n",
    "        k_hop_adj_mat[t,t_i] = 1.\n",
    "        k_hop_adj_mat[t_i,t] = 1.\n",
    "\n",
    "    k_hop_adj_mats.append(k_hop_adj_mat)\n",
    "\n",
    "k_hop_adj_mats = np.expand_dims(k_hop_adj_mats,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "tf_train2idx = tf.convert_to_tensor(train2idx)\n",
    "tf_k_hop_adj_mats = tf.convert_to_tensor(k_hop_adj_mats,dtype=tf.float32)\n",
    "tf_all_indices = tf.convert_to_tensor(all_indices)\n",
    "\n",
    "y_true = model([\n",
    "    all_indices,\n",
    "    train2idx[:,0:1,0],\n",
    "    train2idx[:,0:1,1],\n",
    "    train2idx[:,0:1,2],\n",
    "    k_hop_adj_mats])\n",
    "\n",
    "masks = tf.Variable(\n",
    "        initial_value=tf.random.normal(\n",
    "            (adj_mats.shape), \n",
    "            mean=0, \n",
    "            stddev=1, \n",
    "            dtype=tf.dtypes.float32, \n",
    "            seed=SEED),\n",
    "        name='mask',\n",
    "        trainable=True\n",
    "    )\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    #tape.watch(tf_train2idx)\n",
    "    #tape.watch(tf_k_hop_adj_mats)\n",
    "    tape.watch(masks)\n",
    "\n",
    "    \n",
    "    masked_adj = tf_k_hop_adj_mats*tf.nn.sigmoid(masks)\n",
    "    \n",
    "    y_pred = model([\n",
    "        tf_all_indices,\n",
    "        tf_train2idx[:,0:1,0],\n",
    "        tf_train2idx[:,0:1,1],\n",
    "        tf_train2idx[:,0:1,2],\n",
    "        masked_adj])\n",
    "\n",
    "    loss = bce(y_true,y_pred)\n",
    "    \n",
    "grads = tape.gradient(loss,masks)\n",
    "optimizer.apply_gradients(zip(grads,masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subgraph = tf.cast((tf_k_hop_adj_mats*tf.nn.sigmoid(masks)) > .5,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reduce_sum(tape.gradient(loss,tf_k_hop_adj_mats)[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp to subgraph function\n",
    "#list for jaccard scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainexp2idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_subgraph = []\n",
    "\n",
    "for i in range(NUM_RELATIONS):\n",
    "    \n",
    "    mat = np.zeros((NUM_ENTITIES,NUM_ENTITIES))\n",
    "\n",
    "    exp_triples = trainexp2idx[0][trainexp2idx[0][:,1] == i]\n",
    "    \n",
    "    for exp_h,_,exp_t in exp_triples:\n",
    "        \n",
    "        mat[exp_h,exp_t] = 1\n",
    "        mat[exp_t,exp_h] = 1\n",
    "        \n",
    "    true_subgraph.append(mat)\n",
    "\n",
    "true_subgraph = np.expand_dims(true_subgraph,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_subgraph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_jaccard(truth,pred):\n",
    "    \n",
    "    m11 = np.logical_and(truth==1,pred==1).sum()\n",
    "    m01 = np.logical_and(truth==0,pred==1).sum()\n",
    "    m10 = np.logical_and(truth==1,pred==0).sum()\n",
    "    \n",
    "    return (m11 / (m01+m10+m11))\n",
    "\n",
    "binary_jaccard(truth,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truth = np.array([[1,1,1],[1,0,0],[1,0,0]])\n",
    "# pred = np.array([[1,1,1],[1,0,0],[1,0,0]])\n",
    "\n",
    "# a = truth.sum()\n",
    "# b = pred.sum()\n",
    "# intersect = np.logical_and(truth, pred)\n",
    "# print(truth)\n",
    "# print(pred)\n",
    "# print(intersect)\n",
    "# print(intersect.sum()/(a+b-intersect.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail_triples = []\n",
    "# for i in tail_indices:\n",
    "#     tail_triples.append((h,r,i.numpy()[0]))\n",
    "    \n",
    "# tail_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class New_RGCN_Layer(tf.keras.layers.Layer):\n",
    "#     def __init__(self,num_relations,output_dim,**kwargs):\n",
    "#         super(New_RGCN_Layer,self).__init__(**kwargs)\n",
    "#         self.num_relations = num_relations\n",
    "#         self.output_dim = output_dim\n",
    "        \n",
    "#     def build(self,input_shape):\n",
    "\n",
    "#         input_dim = int(input_shape[-2][-1])\n",
    "        \n",
    "#         self.relation_kernel = self.add_weight(\n",
    "#             shape=(self.num_relations,input_dim, self.output_dim),\n",
    "#             name=\"relation_kernels\",\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "\n",
    "#         self.self_kernel = self.add_weight(\n",
    "#             shape=(input_dim, self.output_dim),\n",
    "#             name=\"self_kernel\",\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             )\n",
    "#         )\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "        \n",
    "#         embeddings,head_idx,head_e,tail_idx,tail_e,adj_mats = inputs\n",
    "        \n",
    "# #         print('embeddings',embeddings.shape)\n",
    "# #         print('head_idx',head_idx.shape)\n",
    "# #         print('head_e',head_e.shape)\n",
    "# #         print('adj_mats',adj_mats.shape)\n",
    "            \n",
    "#         head_output = tf.matmul(head_e,self.self_kernel)\n",
    "#         tail_output = tf.matmul(tail_e,self.self_kernel)\n",
    "        \n",
    "#         #print('head_output',head_output.shape)\n",
    "        \n",
    "#         for i in range(self.num_relations):\n",
    "            \n",
    "#             adj_i = adj_mats[i]\n",
    "\n",
    "#             #print('adj_i',adj_i.shape)\n",
    "            \n",
    "#             head_adj = tf.nn.embedding_lookup(adj_i,head_idx)\n",
    "#             tail_adj = tf.nn.embedding_lookup(adj_i,tail_idx)\n",
    "            \n",
    "#             #print('head_adj',head_adj.shape)\n",
    "            \n",
    "#             #print('head_adj',head_adj.shape)\n",
    "#             #print('embeddings',embeddings.shape)\n",
    "            \n",
    "#             head_update = tf.matmul(head_adj,embeddings)\n",
    "#             tail_update = tf.matmul(tail_adj,embeddings)\n",
    "\n",
    "#             head_output += tf.matmul(head_update,self.relation_kernel[i])\n",
    "#             tail_output += tf.matmul(tail_update,self.relation_kernel[i])\n",
    "       \n",
    "#         return head_output, tail_output\n",
    "    \n",
    "# class DistMult(tf.keras.layers.Layer):\n",
    "#     def __init__(self, num_relations,**kwargs):\n",
    "#         super(DistMult,self).__init__(**kwargs)\n",
    "#         self.num_relations = num_relations\n",
    "        \n",
    "#     def build(self,input_shape):\n",
    "        \n",
    "#         embedding_dim = input_shape[0][-1]\n",
    "        \n",
    "#         self.kernel = self.add_weight(\n",
    "#             shape=(self.num_relations,embedding_dim),\n",
    "#             trainable=True,\n",
    "#             initializer=tf.keras.initializers.RandomNormal(\n",
    "#                 mean=0.0,\n",
    "#                 stddev=1,\n",
    "#                 seed=SEED\n",
    "#             ),\n",
    "#             name='rel_embedding'\n",
    "#         )\n",
    "        \n",
    "#     def call(self,inputs):\n",
    "        \n",
    "#         head_e,rel_idx,tail_e = inputs\n",
    "        \n",
    "#         rel_e = tf.nn.embedding_lookup(self.kernel,rel_idx)\n",
    "        \n",
    "#         score = tf.sigmoid(tf.reduce_sum(head_e*rel_e*tail_e,axis=-1))\n",
    "        \n",
    "#         return tf.expand_dims(score,axis=0)\n",
    "#         embeddings,head_idx,tail_idx,head_e,tail_e,adj_mats = inputs\n",
    "\n",
    "#         adj_mats = tf.squeeze(adj_mats,axis=0)\n",
    "#         embeddings = tf.squeeze(embeddings,axis=0)\n",
    "\n",
    "#         head_output = tf.matmul(head_e,self.self_kernel)\n",
    "#         tail_output = tf.matmul(tail_e,self.self_kernel)\n",
    "        \n",
    "#         for i in range(self.num_relations):\n",
    "            \n",
    "#             adj_i =x adj_mats[i]\n",
    "\n",
    "#             head_adj = tf.nn.embedding_lookup(adj_i,head_idx)\n",
    "#             tail_adj = tf.nn.embedding_lookup(adj_i,tail_idx)\n",
    "            \n",
    "#             h_head = tf.matmul(head_adj,embeddings)\n",
    "#             h_tail = tf.matmul(head_adj,embeddings)\n",
    "            \n",
    "#             head_output += tf.matmul(h_head,self.relation_kernel[i])\n",
    "#             tail_output += tf.matmul(h_tail,self.relation_kernel[i])\n",
    "\n",
    "#         return head_output,tail_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = tf.data.Dataset.from_tensor_slices((\n",
    "#         train2idx[:,:,0],\n",
    "#         train2idx[:,:,1],\n",
    "#         train2idx[:,:,2], \n",
    "#         np.ones(train2idx.shape[1]).reshape(1,-1)\n",
    "#     )\n",
    "# ).batch(1)\n",
    "\n",
    "# for h,r,t,y in data:\n",
    "\n",
    "#     neg_head, neg_tail = utils.get_negative_triples(head=h,rel=r,tail=t,num_entities=NUM_ENTITIES)\n",
    "#     print(h)\n",
    "#     print(neg_head)\n",
    "#     print(t) \n",
    "#     print(neg_tail)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_entities = tf.keras.Input(shape=(NUM_ENTITIES,), name='all_entities',dtype=tf.int64)\n",
    "# head_input = tf.keras.Input(shape=(None,), name='head_input',batch_size=1,dtype=tf.int64)\n",
    "# rel_input = tf.keras.Input(shape=(None,), name='rel_input',batch_size=1,dtype=tf.int64)\n",
    "# tail_input = tf.keras.Input(shape=(None,), name='tail_input',batch_size=1,dtype=tf.int64)\n",
    "\n",
    "# adj_inputs = tf.keras.Input(\n",
    "#         shape=(\n",
    "#             NUM_RELATIONS,\n",
    "#             NUM_ENTITIES,\n",
    "#             NUM_ENTITIES\n",
    "#         ),\n",
    "#         dtype=tf.float32,\n",
    "#         name='adj_inputs'\n",
    "#     )\n",
    "\n",
    "# entity_embeddings = Embedding(\n",
    "#         input_dim=NUM_ENTITIES,\n",
    "#         output_dim=EMBEDDING_DIM,\n",
    "#         name='entity_embeddings',\n",
    "#         embeddings_initializer=tf.keras.initializers.RandomUniform(\n",
    "#             minval=-1,\n",
    "#             maxval=1,\n",
    "#             seed=SEED\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# all_e = entity_embeddings(all_entities)\n",
    "# head_e = entity_embeddings(head_input)\n",
    "# tail_e = entity_embeddings(tail_input)\n",
    "\n",
    "# all_e = tf.keras.layers.Lambda(lambda x:x[0,:,:])(all_e)\n",
    "# head_e = tf.keras.layers.Lambda(lambda x:x[0,:,:])(head_e)\n",
    "# tail_e = tf.keras.layers.Lambda(lambda x:x[0,:,:])(tail_e)\n",
    "\n",
    "# head_index = tf.keras.layers.Lambda(lambda x:x[0,:])(head_input)\n",
    "# rel_index = tf.keras.layers.Lambda(lambda x:x[0,:])(rel_input)\n",
    "# tail_index = tf.keras.layers.Lambda(lambda x:x[0,:])(tail_input)\n",
    "\n",
    "# adj_mats_layer = tf.keras.layers.Lambda(lambda x:x[0,:,:])(adj_inputs)\n",
    "# #embeddings,head_idx,head_e,tail_idx,tail_e,adj_mats\n",
    "\n",
    "# new_head,new_tail = New_RGCN_Layer(NUM_RELATIONS,OUTPUT_DIM)([all_e,head_index,head_e,tail_index,tail_e,adj_mats_layer])\n",
    "# #new_head = New_RGCN_Layer(NUM_RELATIONS,OUTPUT_DIM)([all_entities,])\n",
    "\n",
    "# output = DistMult(num_relations=NUM_RELATIONS,name='output')([new_head,rel_index,new_tail])\n",
    "\n",
    "# output = tf.keras.layers.Dense(train2idx.shape[1],activation='sigmoid')(output)\n",
    "\n",
    "# #output = tf.keras.layers.Dense(1)(output)\n",
    "# #output = tf.keras.layers.Reshape((1,))(output)\n",
    "# # m = tf.keras.Model([head_input],[out])\n",
    "# # m([train2idx[:,0:32,0]])\n",
    "# m = tf.keras.Model([all_entities,head_input,rel_input,tail_input,adj_inputs],[output])\n",
    "\n",
    "# m([np.arange(NUM_ENTITIES).reshape(1,-1),train2idx[:,:,0],train2idx[:,:,1],train2idx[:,:,2],adj_mats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.compile(\n",
    "#     loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "#     optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# )\n",
    "\n",
    "# #m.summary()\n",
    "# m.fit(x=[\n",
    "#     all_indices,\n",
    "#     train2idx[:,:,0],\n",
    "#     train2idx[:,:,1],\n",
    "#     train2idx[:,:,2],\n",
    "#     adj_mats\n",
    "# ],\n",
    "#     y=np.ones(train2idx.shape[1]).reshape(1,-1),\n",
    "#     epochs=NUM_EPOCHS,\n",
    "#     batch_size=1,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = m.predict(\n",
    "#     x=[\n",
    "#         all_indices,\n",
    "#         train2idx[:,:,0],\n",
    "#         train2idx[:,:,1],\n",
    "#         train2idx[:,:,2],\n",
    "#         adj_mats\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m([all_indices,train2idx[:,0:1,0],train2idx[:,0:1,1],train2idx[:,0:1,2],masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_model = tf.keras.Model(inputs=m.input,outputs=m.get_layer('entity_embeddings').output)\n",
    "\n",
    "#embeddings = m.get_layer('entity_embeddings').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_embeddings,tail_embeddings = embeddings_model([\n",
    "#     all_indices,\n",
    "#     train2idx[:,0:1,0],\n",
    "#     train2idx[:,0:1,1],\n",
    "#     train2idx[:,0:1,2],\n",
    "#     adj_mats\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train2idx[0,:,:][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.where(adj_mats[0][2][100]==1.).numpy()\n",
    "#np.argwhere(train2idx[0,:,2]== 100)\n",
    "\n",
    "# for h,_,t in train2idx[:,822,:]:\n",
    "    \n",
    "#     head_neighbors = tf.where(adj_mats[0][2][h]==1.).numpy()\n",
    "#     tail_neighbors = tf.where(adj_mats[0][2][t]==1.).numpy()\n",
    "#tf.where(adj_mats[0][2][100]==1.).numpy()\n",
    "#tf.where(adj_mats[0,2,[100,100,100]]==1.)[:,-1]\n",
    "#train2idx[:,822,[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.nn.embedding_lookup(embeddings,train2idx[0,0:1,0])\n",
    "#k_hop_adj_mats*tf.nn.sigmoid(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for triple in test set:\n",
    "    #get k hop subgraph of each triple (all neighbors of head/tail)\n",
    "    #compute adjacency matrix of subgraph (1,NUM_RELATIONS,NUM_ENTITIES,NUM_ENTITIES)\n",
    "    #compute pred of triple\n",
    "    #for i in iter:\n",
    "        #learn mask\n",
    "        #mask * adj matrix \n",
    "    #mask * adj matrix-> reduce output to ints\n",
    "#edge_mask = tf.cast((k_hop_adj_mats*tf.nn.sigmoid(masks)) > .1,dtype=tf.int32)\n",
    "#tf.nn.sigmoid(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_indices = tf.where(edge_mask[0,2,h,:] == 1)\n",
    "# tail_indices = tf.where(edge_mask[0,2,t,:] == 1)\n",
    "# swap = tf.concat([tf.reshape(indices[:,1],(-1,1)),tf.reshape(indices[:,0],(-1,1))],axis=1)\n",
    "\n",
    "# unique_indices = tf.concat([indices,swap],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
