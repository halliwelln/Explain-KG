{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import os\n",
    "import utils\n",
    "import transE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb15k_237 = np.load('./data/fb15k_237.npz', allow_pickle=True)\n",
    "# fb_train = fb15k_237['train']\n",
    "# fb_valid = fb15k_237['valid']\n",
    "# fb_test = fb15k_237['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "data = np.load(os.path.join('.','data','royalty_spouse.npz'))\n",
    "\n",
    "train = data['X_train']\n",
    "test = data['X_test']\n",
    "\n",
    "train_exp = data['train_exp']\n",
    "test_exp = data['test_exp']\n",
    "\n",
    "full_exp = np.concatenate((train_exp,test_exp), axis=0)\n",
    "\n",
    "#full_train = np.concatenate((train,train_exp), axis=0)\n",
    "\n",
    "entities = data['entities'].tolist()\n",
    "relations = data['relations'].tolist()\n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(NUM_ENTITIES)))\n",
    "rel2idx = dict(zip(relations, range(NUM_RELATIONS)))\n",
    "\n",
    "idx2ent = {v:k for k,v in ent2idx.items()}\n",
    "idx2rel = {v:k for k,v in rel2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2idx = utils.array2idx(train,ent2idx,rel2idx)\n",
    "trainexp2idx = utils.array2idx(train_exp,ent2idx,rel2idx)\n",
    "\n",
    "test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 500\n",
    "MARGIN = 2\n",
    "LEARNING_RATE = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transE.ExTransE(NUM_ENTITIES,NUM_RELATIONS,EMBEDDING_SIZE,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE),\n",
    "        num_entities=NUM_ENTITIES,\n",
    "        margin=MARGIN,\n",
    "        pred_loss=transE.pred_loss,\n",
    "        exp_loss=transE.exp_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=[\n",
    "        train2idx[:,0],\n",
    "        train2idx[:,1],\n",
    "        train2idx[:,2],\n",
    "        trainexp2idx[:,:,0].flatten(),\n",
    "        trainexp2idx[:,:,1].flatten(),\n",
    "        trainexp2idx[:,:,2].flatten()\n",
    "    ],\n",
    "    epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_head_e, test_rel_e, test_tail_e, test_exp_head_e, test_exp_rel_e, test_exp_tail_e = model.predict(x=[\n",
    "        test2idx[:,0],\n",
    "        test2idx[:,1],\n",
    "        test2idx[:,2],\n",
    "        testexp2idx[:,:,0].flatten(),\n",
    "        testexp2idx[:,:,1].flatten(),\n",
    "        testexp2idx[:,:,2].flatten()\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 1\n",
    "pred_exp = []\n",
    "\n",
    "for i in range(len(test2idx)):\n",
    "    \n",
    "    triple_h_e = test_head_e[i]\n",
    "    triple_r_e = test_rel_e[i]\n",
    "    triple_t_e = test_tail_e[i]\n",
    "    \n",
    "    squared_diff = np.square(triple_h_e - test_exp_head_e) + np.square(triple_r_e-test_exp_rel_e) + np.square(triple_t_e-test_exp_tail_e)\n",
    "\n",
    "    l2_dist = np.sqrt(np.sum(squared_diff,axis=1))\n",
    "\n",
    "    closest_l2 = np.argsort(l2_dist)[:top_k]\n",
    "\n",
    "    k_closest = testexp2idx[closest_l2]\n",
    "\n",
    "    pred_exp.append(k_closest)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exp = np.array(pred_exp).reshape(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.jaccard_score(testexp2idx, pred_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0], train2idx[:,1], train2idx[:,2],\n",
    "# #                                                 exp2idx[:,0],exp2idx[:,1],exp2idx[:,2])).batch(batch_size)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0],train2idx[:,1],train2idx[:,2],\n",
    "                                                trainexp2idx[:,:,0].reshape(-1),trainexp2idx[:,:,1].reshape(-1),\n",
    "                                                 trainexp2idx[:,:,2].reshape(-1))).batch(BATCH_SIZE)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a,b,c,d,e,f in train_data:\n",
    "    print(a.shape, b.shape, c.shape, d.shape, e.shape,f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_loss = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     for pos_head, rel, pos_tail, pos_head_exp,rel_exp, pos_tail_exp in train_data:\n",
    "\n",
    "#         neg_head, neg_tail = utils.get_negative_triples(\n",
    "#             head=pos_head, \n",
    "#             rel=rel, \n",
    "#             tail=pos_tail,\n",
    "#             num_entities=num_entities,\n",
    "#             random_state=SEED\n",
    "#             )\n",
    "\n",
    "#         neg_head_exp, neg_tail_exp = utils.get_negative_triples(\n",
    "#             head=pos_head_exp, \n",
    "#             rel=rel_exp, \n",
    "#             tail=pos_tail_exp,\n",
    "#             num_entities=num_entities,\n",
    "#             random_state=SEED\n",
    "#             )\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "\n",
    "#             pos_head_e, pos_tail_e, neg_head_e, neg_tail_e, rel_e = model([\n",
    "#                 pos_head,\n",
    "#                 pos_tail, \n",
    "#                 neg_head, \n",
    "#                 neg_tail, \n",
    "#                 rel\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#             pos_head_exp_e, pos_tail_exp_e, _, _, rel_exp_e = model([\n",
    "#                 pos_head_exp,\n",
    "#                 pos_tail_exp, \n",
    "#                 tf.zeros_like(neg_head_exp),  \n",
    "#                 tf.zeros_like(neg_tail_exp), \n",
    "#                 rel_exp\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#             prediction_loss = transE.pred_loss(pos_head_e,pos_tail_e,neg_head_e,neg_tail_e,rel_e)\n",
    "#             #explain_loss = transE.exp_loss(pos_head_exp_e,pos_tail_exp_e,neg_head_exp_e, neg_tail_exp_e, rel_exp_e)\n",
    "#             explain_loss = transE.exp_loss(pos_head_e,pos_tail_exp_e,pos_head_exp_e,pos_tail_exp_e,rel_e,rel_exp_e)\n",
    "#             #print(f\"pred loss {prediction_loss}\")\n",
    "#             #print(f\"explain loss {explain_loss}\")\n",
    "#             total_loss = prediction_loss + explain_loss\n",
    "\n",
    "#         grads = tape.gradient(total_loss,model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "#     #if not epoch % 10:\n",
    "#     #print(f\"Loss at epoch {epoch}: {total_loss.numpy()} \")\n",
    "        \n",
    "#     epoch_loss.append(np.round(total_loss.numpy(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = utils.get_entity_embeddings(model)\n",
    "relation_embeddings = utils.get_relation_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = transE.ExTransE(\n",
    "#     num_entities=NUM_ENTITIES,\n",
    "#     num_relations=NUM_RELATIONS,\n",
    "#     embedding_size=EMBEDDING_SIZE,\n",
    "#     margin=MARGIN,\n",
    "#     random_state=SEED)\n",
    "# optimizer=tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0],train2idx[:,1],train2idx[:,2],\n",
    "#                                                 trainexp2idx[:,:,0].reshape(-1),trainexp2idx[:,:,1].reshape(-1),\n",
    "#                                                  trainexp2idx[:,:,2].reshape(-1))).batch(BATCH_SIZE)\n",
    "\n",
    "# epoch_loss = []\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "#     for pos_head, pos_rel, pos_tail, pos_head_exp,pos_rel_exp, pos_tail_exp in train_data:\n",
    "\n",
    "#         neg_head, neg_tail = utils.get_negative_triples(\n",
    "#             head=pos_head, \n",
    "#             rel=pos_rel, \n",
    "#             tail=pos_tail,\n",
    "#             num_entities=NUM_ENTITIES\n",
    "#             )\n",
    "\n",
    "#         neg_head_exp, neg_tail_exp = utils.get_negative_triples(\n",
    "#             head=pos_head_exp, \n",
    "#             rel=pos_rel_exp, \n",
    "#             tail=pos_tail_exp,\n",
    "#             num_entities=NUM_ENTITIES\n",
    "#             )\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "\n",
    "#             pos_head_e,pos_rel_e,pos_tail_e,pos_head_exp_e,pos_rel_exp_e,pos_tail_exp_e = model([\n",
    "#                 pos_head,\n",
    "#                 pos_rel,\n",
    "#                 pos_tail,\n",
    "#                 pos_head_exp,\n",
    "#                 pos_rel_exp,\n",
    "#                 pos_tail_exp\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#             neg_head_e,neg_rel_e,neg_tail_e,neg_head_exp_e,neg_rel_exp_e,neg_tail_exp_e = model([\n",
    "#                 neg_head,\n",
    "#                 pos_rel,#pos_rel is correct, \n",
    "#                 neg_tail,\n",
    "#                 neg_head_exp,\n",
    "#                 pos_rel_exp,\n",
    "#                 neg_tail_exp\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#             prediction_loss = transE.pred_loss(\n",
    "#                 pos_head_e,\n",
    "#                 pos_rel_e,\n",
    "#                 pos_tail_e,\n",
    "#                 neg_head_e,\n",
    "#                 neg_rel_e,\n",
    "#                 neg_tail_e,\n",
    "#                 margin=MARGIN\n",
    "#             )\n",
    "\n",
    "#             explain_loss = transE.exp_loss(\n",
    "#                 pos_head_exp_e,\n",
    "#                 pos_rel_exp_e,\n",
    "#                 pos_tail_exp_e,\n",
    "#                 neg_head_exp_e,\n",
    "#                 neg_rel_exp_e,\n",
    "#                 neg_tail_exp_e,\n",
    "#                 margin=MARGIN\n",
    "#             )\n",
    "\n",
    "#             total_loss = prediction_loss + explain_loss\n",
    "\n",
    "#         grads = tape.gradient(total_loss,model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "#     epoch_loss.append(np.round(total_loss.numpy(),5))\n",
    "\n",
    "# print(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k = transE.exp_score(test2idx[0],k=2,data=test2idx,entity_embeddings=entity_embeddings,\n",
    "#                          relation_embeddings=relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_k[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def exp_score(triple,k,data,entity_embeddings,relation_embeddings):\n",
    "    \n",
    "#     triple_h_e = entity_embeddings[triple[0]]\n",
    "#     triple_r_e = relation_embeddings[triple[1]]\n",
    "#     triple_t_e = entity_embeddings[triple[2]]\n",
    "\n",
    "#     h_e = entity_embeddings[data[:,0]]\n",
    "#     r_e = relation_embeddings[data[:,1]]\n",
    "#     t_e = entity_embeddings[data[:,2]]\n",
    "\n",
    "#     squared_diff = np.square(triple_h_e - h_e) + np.square(triple_r_e-r_e) + np.square(triple_t_e-t_e)\n",
    "\n",
    "#     l2_dist = np.sqrt(np.sum(squared_diff,axis=1))\n",
    "\n",
    "#     closest_l2 = np.argsort(l2_dist)[:k]\n",
    "    \n",
    "#     return data[closest_l2]\n",
    "\n",
    "#########account for padding\n",
    "pred_exp = []\n",
    "\n",
    "for i in range(len(test2idx)):\n",
    "    \n",
    "    top_k = transE.exp_score(test2idx[i],k=1,data=testexp2idx,\n",
    "                             entity_embeddings=entity_embeddings,\n",
    "                             relation_embeddings=relation_embeddings)\n",
    "\n",
    "    h=np.array([idx2ent[i] for i in top_k[:,0]]).reshape(-1,1)\n",
    "    r=np.array([idx2rel[i] for i in top_k[:,1]]).reshape(-1,1)\n",
    "    t=np.array([idx2ent[i] for i in top_k[:,2]]).reshape(-1,1)\n",
    "\n",
    "    pred = [list(i) for i in np.concatenate([h,r,t], axis=1)]\n",
    "    \n",
    "    pred_exp.append(pred)\n",
    "\n",
    "#utils.jaccard_score(test_exp,pred_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_exp = np.array([\n",
    "    np.array([['John_II_of_France', 'spouse', 'Bonne_of_Bohemia'],['Nanda_Bayin', 'spouse', 'Min_Htwe_of_Toungoo']]),\n",
    "    np.array([['Keelikōlani', 'spouse', 'Leleiohoku_I'],['Saliha_Sultan', 'spouse', 'Mustafa_II']]),\n",
    "    np.array([['Kalanipauahi', 'spouse', 'Kamehameha_II']])\n",
    "])\n",
    "\n",
    "#true_exp = test_exp[0:4]\n",
    "#pred_exp[0:5]\n",
    "true_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_exp[0:5]\n",
    "pred_exp[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# utils.jaccard_score(true_exp,pred_exp[0:3])\n",
    "def jaccard_score(true_exp,pred_exp):\n",
    "\n",
    "    assert len(true_exp) == len(pred_exp)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for i in range(len(true_exp)):\n",
    "\n",
    "        pred_i = pred_exp[i]\n",
    "        true_i = true_exp[i]\n",
    "\n",
    "        num_true_traces = min(true_i.ndim,true_i.shape[0])\n",
    "\n",
    "        if isinstance(pred_i,np.ndarray):\n",
    "            num_pred_traces = pred_i.ndim\n",
    "        \n",
    "        elif isinstance(pred_i,list):\n",
    "            num_pred_traces = len(pred_i)\n",
    "    \n",
    "        bool_array = (pred_i == true_i)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for row in bool_array:\n",
    "            if row.all():\n",
    "                count +=1\n",
    "\n",
    "        score = count / (num_true_traces+num_pred_traces-count)\n",
    "\n",
    "        scores.append(score)\n",
    "    print(scores)\n",
    "    return np.mean(scores)\n",
    "\n",
    "jaccard_score(true_exp,pred_exp[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what data to use for exp triple ranking\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# for i in range(len(pred_exp[0:3])):\n",
    "    \n",
    "#     pred_i = pred_exp[i]\n",
    "#     true_i = true_exp[i]\n",
    "    \n",
    "#     num_true_traces = min(true_i.ndim,true_i.shape[0])\n",
    "    \n",
    "#     if isinstance(pred_i,np.ndarray):\n",
    "#         num_pred_traces = pred_i.ndim\n",
    "        \n",
    "#     elif isinstance(pred_i,list):\n",
    "#         num_pred_traces = len(pred_i)\n",
    "    \n",
    "#     bool_array = (pred_i == true_i)\n",
    "\n",
    "#     count = 0\n",
    "#     print(pred_i,true_i)\n",
    "#     for row in bool_array:\n",
    "#         print(row)\n",
    "#         if row.all():\n",
    "#             count += 1\n",
    "#     score = count / (num_true_traces+num_pred_traces-count)\n",
    "#     scores.append(score)\n",
    "        \n",
    "#     if (num_pred_traces > num_true_traces):\n",
    "\n",
    "#         count = 0\n",
    "        \n",
    "#         for j in range(num_true_traces):\n",
    "\n",
    "#             for k in range(num_pred_traces):\n",
    "\n",
    "#                 if j != k:\n",
    "       \n",
    "#                     if (true_i[j]==pred_i[k]).all():\n",
    "\n",
    "#                         count +=1\n",
    "\n",
    "#         score = count/num_pred_traces\n",
    "#     elif (num_pred_traces < num_true_traces):\n",
    "#         print('still no')\n",
    "#         pass\n",
    "#     elif (num_pred_traces == num_true_traces):\n",
    "        \n",
    "#         if (num_true_traces==1) and (num_pred_traces==1):\n",
    "#             if (pred_i==true_i).all():\n",
    "#                 score = 1\n",
    "#             else:\n",
    "#                 score = 0\n",
    "#         else:\n",
    "#             count = 0\n",
    "            \n",
    "#             for j in range(num_true_traces):\n",
    "                \n",
    "#                 if (pred_i[j] == true_i[j]).all():\n",
    "                    \n",
    "#                     count += 1\n",
    "                    \n",
    "#             score = count/num_true_traces\n",
    "            \n",
    "#     scores.append(score)\n",
    "\n",
    "#     if (num_true_traces > 1) and (num_pred_traces > 1):\n",
    "        \n",
    "#         count = 0\n",
    "        \n",
    "#         if (num_true_traces > num_pred_traces):\n",
    "            \n",
    "#             for j in range(num_true_traces):\n",
    "\n",
    "#                 for k in range(num_true_traces):\n",
    "\n",
    "#                     if j != k:\n",
    "#                         print('all pred', pred_i)\n",
    "#                         print('pred',pred_i[j])\n",
    "#                         print('true',test_exp[i][k])\n",
    "#                         if (pred_i[j]==test_exp[i][k]).all():\n",
    "\n",
    "#                             count +=1\n",
    "\n",
    "#             score = count/num_traces\n",
    "\n",
    "#             scores.append(score)\n",
    "            \n",
    "#         elif (num_true_traces < num_pred_traces):\n",
    "        \n",
    "#         else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_exp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data = np.load('./data/human_data.npz')\n",
    "# train = data['X']\n",
    "\n",
    "# g=Graph()\n",
    "# g.parse(\"../CORESE-DATA/human-data.rdf\", format=\"xml\")\n",
    "\n",
    "# triples = []\n",
    "\n",
    "# for i,j,k in g:\n",
    "    \n",
    "#     head = str(i).split('#')\n",
    "#     rel = str(j).split('#')\n",
    "#     tail = str(k).split('#')\n",
    "    \n",
    "#     if head[0] == 'http://www.inria.fr/2015/humans-instances':\n",
    "        \n",
    "#         triples.append((head[-1], rel[-1], tail[-1]))\n",
    "\n",
    "# triples = [('Eve', 'type', 'Lecturer'),\n",
    "#            #('Eve', 'type', 'Person'), \n",
    "#            ('Lecturer', 'subClassOf', 'Person'), \n",
    "#            #('David', 'type', 'Person'),\n",
    "#            ('David', 'type', 'Researcher'),\n",
    "#            ('Researcher', 'subClassOf', 'Person'),\n",
    "#            ('Flora', 'hasSpouse', 'Gaston'),\n",
    "#            ('Gaston', 'type', 'Person'),\n",
    "#            #('Flora', 'type', 'Person'),\n",
    "#           ]\n",
    "# g=Graph()\n",
    "# g.parse(\"/Users/nhalliwe/Desktop/CORESE-DATA/human-data.rdf\", format=\"xml\")\n",
    "# triples = []\n",
    "\n",
    "# for i,j,k in g:\n",
    "    \n",
    "#     head = str(i).split('#')\n",
    "#     rel = str(j).split('#')\n",
    "#     tail = str(k).split('#')\n",
    "    \n",
    "#     if head[0] == 'http://www.inria.fr/2015/humans-instances':\n",
    "        \n",
    "#         triples.append((head[-1], rel[-1], tail[-1]))\n",
    "\n",
    "# train = np.array(triples)\n",
    "# traces = utils.parse_traces(file_name='../traces/entailment.ttl')\n",
    "# exp_triples = utils.get_exp_triples(train,traces)\n",
    "\n",
    "# entities = np.unique(np.concatenate((train[:,0], train[:,2], exp_triples[:,0], exp_triples[:,2]), axis=0)).tolist()\n",
    "# relations = np.unique(np.concatenate((train[:,1], exp_triples[:,1])), axis=0).tolist()\n",
    "\n",
    "# num_entities = len(entities)\n",
    "# num_relations = len(relations)\n",
    "\n",
    "# ent2idx = dict(zip(entities, range(num_entities)))\n",
    "# rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "# idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "# idx2rel = {idx:rel for rel,idx in rel2idx.items()}\n",
    "\n",
    "# train2idx = utils.train2idx(train,ent2idx,rel2idx)\n",
    "# idx2train = utils.idx2train(train2idx,idx2ent,idx2rel)\n",
    "\n",
    "# def get_neighbor_idx(A):\n",
    "\n",
    "#     A = sparse.coo_matrix(A)\n",
    "\n",
    "#     indices = {}\n",
    "\n",
    "#     for i,j in zip(A.row,A.col):\n",
    "\n",
    "#         if i in indices:\n",
    "#             indices[i].append(j)\n",
    "\n",
    "#         else:\n",
    "#             indices[i] = [j]\n",
    "\n",
    "#     return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train:\n",
    "#     if tuple(i) in traces:\n",
    "#         print(tuple(i),traces[tuple(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h,r,t in traces[('Eve', 'type', 'Person')]:\n",
    "#     print(ent2idx[h],rel2idx[r],ent2idx[t])\n",
    "\n",
    "\n",
    "        \n",
    "        #exp_triples.append(list(traces[tuple(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = tf.convert_to_tensor([6,4])\n",
    "#tf.concat([tf.gather_nd(train2idx,tf.where(train2idx[:,0] == idx)), tf.gather_nd(train2idx,tf.where(train2idx[:,2] == idx))], axis=0)\n",
    "\n",
    "#tf.where(train2idx[:,0]==idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = np.zeros(shape=(num_entities,num_entities,num_relations))\n",
    "\n",
    "# for h,r,t in train:\n",
    "    \n",
    "#     h_idx = entities.index(h)\n",
    "#     r_idx = relations.index(r)\n",
    "#     t_idx = entities.index(t)\n",
    "    \n",
    "#     A[h_idx,t_idx,r_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## valid2idx = []\n",
    "\n",
    "# for head, rel, tail in valid:\n",
    "    \n",
    "#     head_idx = ent2idx[head]\n",
    "#     tail_idx = ent2idx[tail]\n",
    "#     rel_idx = rel2idx[rel]\n",
    "\n",
    "#     valid2idx.append([head_idx, rel_idx, tail_idx])\n",
    "    \n",
    "# valid2idx = np.array(valid2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transE\n",
    "# EMBEDDING_SIZE = 30\n",
    "# BATCH_SIZE = 2\n",
    "# NUM_EPOCHS = 200\n",
    "# MARGIN = 2\n",
    "# SQRT_SIZE = 6 / np.sqrt(EMBEDDING_SIZE)\n",
    "\n",
    "# model = transE.build_model(\n",
    "#     embedding_size=EMBEDDING_SIZE,\n",
    "#     num_entities=num_entities,\n",
    "#     num_relations=num_relations,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     margin=MARGIN,\n",
    "#     sqrt_size=SQRT_SIZE,\n",
    "#     seed=SEED\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex\n",
    "# EMBEDDING_SIZE = 30\n",
    "# BATCH_SIZE = 3\n",
    "# NUM_EPOCHS = 200\n",
    "# MARGIN = 1\n",
    "# SQRT_SIZE = 6 / np.sqrt(EMBEDDING_SIZE)\n",
    "\n",
    "# real_head_input = tf.keras.layers.Input(shape=(1,), name='real_head_input')\n",
    "# img_head_input = tf.keras.layers.Input(shape=(1,), name='img_head_input')\n",
    "# real_tail_input = tf.keras.layers.Input(shape=(1,), name='real_tail_input')\n",
    "# img_tail_input = tf.keras.layers.Input(shape=(1,), name='img_tail_input')\n",
    "# real_rel_input = tf.keras.layers.Input(shape=(1,), name='real_rel_input')\n",
    "# img_rel_input = tf.keras.layers.Input(shape=(1,), name='img_rel_input')\n",
    "\n",
    "# real_entity_embeddings = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_entities,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='real_entity_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED))\n",
    "#     )\n",
    "\n",
    "# img_entity_embeddings = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_entities,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='img_entity_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED))\n",
    "#     )\n",
    "\n",
    "# real_relation_embedding = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_relations,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='real_relation_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED)),\n",
    "#     )\n",
    "\n",
    "# img_relation_embedding = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_relations,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='img_relation_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED)),\n",
    "#     )\n",
    "\n",
    "# real_head = real_entity_embeddings(real_head_input)\n",
    "# img_head = img_entity_embeddings(img_head_input)\n",
    "# real_tail = real_entity_embeddings(real_tail_input)\n",
    "# img_tail = img_entity_embeddings(img_tail_input)\n",
    "# real_rel = real_relation_embedding(real_rel_input)\n",
    "# img_rel = real_relation_embedding(img_rel_input)\n",
    "\n",
    "# model = tf.keras.models.Model(\n",
    "#     inputs=[\n",
    "#         real_head_input,\n",
    "#         img_head_input, \n",
    "#         real_tail_input, \n",
    "#         img_tail_input, \n",
    "#         real_rel_input,\n",
    "#         img_rel_input\n",
    "#         ], \n",
    "#     outputs=[\n",
    "#         real_head,\n",
    "#         img_head, \n",
    "#         real_tail, \n",
    "#         img_tail, \n",
    "#         real_rel,\n",
    "#         img_rel\n",
    "#         ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_negative_triples(head, rel, tail, num_entities, seed):\n",
    "    \n",
    "#     cond = tf.random.uniform(head.shape, 0, 2, dtype=tf.int64, seed=seed) #1 means keep entity\n",
    "#     rnd = tf.random.uniform(head.shape, 0, num_entities-1, dtype=tf.int64, seed=seed)\n",
    "    \n",
    "#     neg_head = tf.where(cond == 1, head, rnd)\n",
    "#     neg_tail = tf.where(cond == 1, rnd, tail)   \n",
    "    \n",
    "#     return neg_head, neg_tail\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0], train2idx[:,1], train2idx[:,2])).batch(BATCH_SIZE)\n",
    "# #train_data = train_data.shuffle(buffer_size=50000, seed=tf.random.set_seed(SEED)).batch(BATCH_SIZE)\n",
    "\n",
    "# #exp_decay = tf.keras.optimizers.schedules.ExponentialDecay(.01, 1000, .05)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score(h,r,t):\n",
    "#     return tf.reduce_sum(tf.square(h + r - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "#     for head, rel, tail in train_data:\n",
    "                \n",
    "#         neg_head, neg_tail = get_negative_triples(head, rel, tail,seed=tf.random.set_seed(SEED))\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "            \n",
    "#             real_head_e,img_head_e, real_tail_e, img_tail_e, real_rel_e,img_rel_e = model([head, \n",
    "#                                                                            neg_head, tail, neg_tail, rel, rel])\n",
    "\n",
    "            \n",
    "#             dot1 = tf.reduce_sum(tf.multiply(real_rel_e, tf.multiply(real_head_e, real_tail_e)),1)\n",
    "#             dot2 = tf.reduce_sum(tf.multiply(real_rel_e, tf.multiply(img_head_e, img_tail_e)),1)\n",
    "#             dot3 = tf.reduce_sum(tf.multiply(img_rel_e, tf.multiply(real_head_e, img_tail_e)),1)\n",
    "#             dot4 = tf.reduce_sum(tf.multiply(img_rel_e, tf.multiply(img_head_e, real_tail_e)),1)\n",
    "            \n",
    "#             embedding_loss = tf.reduce_sum(dot1+dot2+dot3-dot4)\n",
    "\n",
    "#         grads = tape.gradient(embedding_loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "#     if not epoch % 10:\n",
    "        \n",
    "#         print('Current loss' , embedding_loss.numpy(),'at epoch', epoch)\n",
    "    \n",
    "#     losses.append(embedding_loss.numpy())\n",
    "#train2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for head, rel, tail in train_data:\n",
    "#     print(head, rel, tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "#     for head, rel, tail in train_data:\n",
    "                \n",
    "#         neg_head, neg_tail = utils.get_negative_triples(head, rel, tail,num_entities=num_entities,seed=SEED)\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "                        \n",
    "# #             pos_head_e,neg_head_e, pos_tail_e, neg_tail_e, rel_e= model([head, \n",
    "# #                                                                            neg_head, tail, neg_tail, rel])\n",
    "            \n",
    "#             pos_head_0,neg_head_0, pos_tail_0, neg_tail_0, rel_0 = model([head[0], \n",
    "#                                                                           neg_head[0], tail[0], \n",
    "#                                                                           neg_tail[0], rel[0]])\n",
    "            \n",
    "#             pos_head_1,neg_head_1, pos_tail_1, neg_tail_1, rel_1 = model([head[1], \n",
    "#                                                                           neg_head[1], tail[1], neg_tail[1], rel[1]])\n",
    "            \n",
    "# #             pos_head_2,neg_head_2, pos_tail_2, neg_tail_2, rel_2 = model([head[2], \n",
    "# #                                                                           neg_head[2], tail[2], neg_tail[2], rel[2]])\n",
    "            \n",
    "#             #pos = score(pos_head_0,rel_0, pos_tail_0) + \\\n",
    "#             #score(pos_head_1,rel_1, pos_tail_1) #+score(pos_head_2,rel_2, pos_tail_2)\n",
    "            \n",
    "#             #neg = score(neg_head_0,rel_0, neg_tail_0) + \\\n",
    "#             #score(neg_head_1,rel_1, neg_tail_1) #+\\score(neg_head_2,rel_2, neg_tail_2)\n",
    "            \n",
    "            \n",
    "#             pos = tf.reduce_sum(tf.square(pos_head_e + rel_e - pos_tail_e), axis=1)\n",
    "#             neg = tf.reduce_sum(tf.square(neg_head_e + rel_e - neg_tail_e), axis=1)    \n",
    "            \n",
    "#             embedding_loss = tf.reduce_sum(tf.maximum(pos - neg + MARGIN, 0))\n",
    "#             #pred_loss = tf.reduce_sum(tf.maximum(pos - neg + MARGIN, 0))\n",
    "            \n",
    " \n",
    "#         grads = tape.gradient(embedding_loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "#     if not epoch % 10:\n",
    "        \n",
    "#         print('Current loss' , embedding_loss.numpy(),'at epoch', epoch)\n",
    "    \n",
    "#     losses.append(embedding_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_names = np.concatenate((entities,relations))\n",
    "\n",
    "# all_entities = model.get_layer('entity_embeddings').get_weights()[0]\n",
    "# all_relations = model.get_layer('relation_embeddings').get_weights()[0]\n",
    "\n",
    "# all_embeddings = np.concatenate((all_entities,all_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# np.load(os.path.join('.','data','transE_embeddings.npz'))['entity_embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_2d = PCA(n_components=2, random_state=SEED).fit_transform(all_embeddings)\n",
    "# fig, ax = plt.subplots(figsize=(12,12))\n",
    "# ax.scatter(embeddings_2d[:,0], embeddings_2d[:,1])\n",
    "# for i, txt in enumerate(all_names):\n",
    "#     ax.annotate(txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_names = list(all_names)\n",
    "# all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_head = 'Eve'\n",
    "# true_tail = 'Person'\n",
    "\n",
    "# head_idx = all_names.index(true_head)\n",
    "# tail_idx = all_names.index(true_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_score = -100000000\n",
    "# min_idx = -100000000\n",
    "\n",
    "# for rel in relations:\n",
    "    \n",
    "#     rel_idx = all_names.index(rel)\n",
    "    \n",
    "#     current_score = -score(all_embeddings[head_idx],\n",
    "#                            all_embeddings[rel_idx],\n",
    "#                            all_embeddings[tail_idx]).numpy()\n",
    "    \n",
    "#     if current_score > min_score:\n",
    "#         min_score = current_score\n",
    "#         min_idx = rel_idx\n",
    "        \n",
    "#     print(all_names[rel_idx], current_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(true_head, all_names[min_idx], true_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h,r,t in train:\n",
    "    \n",
    "#     h_idx = all_names.index(h)\n",
    "#     r_idx = all_names.index(r)\n",
    "#     t_idx = all_names.index(t)\n",
    "    \n",
    "#     print(h,r,t,-score(all_embeddings[h_idx], all_embeddings[r_idx], all_embeddings[t_idx]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def closest_l2(source_head, source_rel, source_tail,k, data, entity_rel_names):\n",
    "    \n",
    "#     for h,r,t in data:\n",
    "        \n",
    "#         h_idx = entity_rel_names.index(h)\n",
    "#         r_idx = entity_rel_names.index(r)\n",
    "#         t_idx = entity_rel_names.index(t)\n",
    "    \n",
    "#     l2 = np.sqrt(np.sum((source_head - all_embeddings[h_idx])**2 + (rel- all_embeddings[r_idx])**2\n",
    "#                 + (person-all_embeddings[t_idx])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find closest l2 triple\n",
    "# eve = all_embeddings[head_idx]\n",
    "# person = all_embeddings[tail_idx]\n",
    "# rel = all_embeddings[min_idx]\n",
    "# scores = []\n",
    "\n",
    "# for h,r,t in train:\n",
    "    \n",
    "#     h_idx = all_names.index(h)\n",
    "#     r_idx = all_names.index(r)\n",
    "#     t_idx = all_names.index(t)\n",
    "    \n",
    "#     l2 = np.sqrt(np.sum((eve - all_embeddings[h_idx])**2 + (rel- all_embeddings[r_idx])**2\n",
    "#                 + (person-all_embeddings[t_idx])**2))\n",
    "#     scores.append(((h,r,t), l2))\n",
    "#     print(h,r,t,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace = set([('Eve', 'type', 'Lecturer'), ('Lecturer', 'subClassOf', 'Person')])\n",
    "# gen = set([('Eve', 'type', 'Lecturer')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(scores, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eve = all_embeddings[all_names.index('Eve')]\n",
    "# person = all_embeddings[all_names.index('Person')]\n",
    "# rel = all_embeddings[all_names.index('type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head_idx, rel_idx, tail_idx = train2idx[0]\n",
    "\n",
    "# def get_grad(head_idx, tail_idx, rel_idx, A):\n",
    "    \n",
    "#     with tf.GradientTape(persistent=True) as g:\n",
    "\n",
    "# #         head_idx = tf.convert_to_tensor(head_idx)\n",
    "# #         tail_idx = tf.convert_to_tensor(tail_idx)\n",
    "# #         rel_idx = tf.convert_to_tensor(rel_idx)    \n",
    "#         A = tf.convert_to_tensor(A)\n",
    "\n",
    "#         head = tf.argmax(A[:,tail_idx, rel_idx])\n",
    "#         tail = tf.argmax(A[head_idx,:, rel_idx])\n",
    "#         rel = tf.argmax(A[head_idx,tail_idx,:])\n",
    " \n",
    "#         head_e,_,tail_e,_,rel_e= model([head,head,tail,tail,rel])\n",
    "\n",
    "#         get_score = score(head_e,rel_e,tail_e)\n",
    "        \n",
    "        \n",
    "#     nabla = g.gradient(get_score, head_e)\n",
    "#     return nabla\n",
    "#get_grad(head_idx, tail_idx, rel_idx, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('entity2wikidata.json','r') as f:\n",
    "    \n",
    "#     entities_dict = json.load(f)\n",
    "\n",
    "# for k, d in entities_dict.items():\n",
    "    \n",
    "#     if 'France' in d['label']:\n",
    "        \n",
    "#         print(k)\n",
    "\n",
    "# embeddings = model.get_layer('entity_embeddings').get_weights()[0]\n",
    "# relations = model.get_layer('relation_embeddings').get_weights()[0]\n",
    "\n",
    "# paris = '/m/05qtj'\n",
    "# france = '/m/0f8l9c'\n",
    "\n",
    "# paris_idx = ent2idx[paris]\n",
    "# france_idx = ent2idx[france]\n",
    "# capital = rel2idx['/location/country/capital']\n",
    "\n",
    "# head, tail = embeddings[[paris_idx, france_idx], :]\n",
    "# rel = relations[capital]\n",
    "\n",
    "# #-np.linalg.norm((head+rel - tail),ord=2)\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# for i in range(len(relations)):\n",
    "    \n",
    "#     temp_rel = relations[i]\n",
    "    \n",
    "#     score = -np.linalg.norm((head+temp_rel - tail),ord=2)\n",
    "    \n",
    "#     scores.append(score)\n",
    "\n",
    "# idx2rel[np.argmax(scores)]\n",
    "\n",
    "# for i in np.argsort(scores)[-10:]:\n",
    "    \n",
    "#     print(idx2rel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import os\n",
    "import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import transE\n",
    "\n",
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('.','data','royalty_spouse.npz'))\n",
    "\n",
    "train = data['X_train']\n",
    "test = data['X_test']\n",
    "\n",
    "train_exp = data['train_exp']\n",
    "test_exp = data['test_exp']\n",
    "\n",
    "full_train = np.concatenate((train,train_exp.reshape(-1,3)), axis=0)\n",
    "#full_test = np.concatenate((test,test_exp.reshape(-1,3)), axis=0)\n",
    "#full_data = np.concatenate((full_train,full_test), axis=0)\n",
    "\n",
    "entities = data['entities'].tolist()\n",
    "relations = data['relations'].tolist()\n",
    "\n",
    "num_entities = len(entities)\n",
    "num_relations = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(num_entities)))\n",
    "rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "#idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "#idx2rel = {idx:rel for rel,idx in rel2idx.items()}\n",
    "\n",
    "train2idx = utils.array2idx(train,ent2idx,rel2idx)\n",
    "test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "\n",
    "trainexp2idx = utils.array2idx(train_exp,ent2idx,rel2idx)\n",
    "testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "EMBEDDING_SIZE = 50\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "MARGIN = 2\n",
    "LEARNING_RATE = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model = transE.ExTransE(\n",
    "    num_entities=NUM_ENTITIES,\n",
    "    num_relations=2,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    margin=MARGIN,\n",
    "    random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ExTransE(\n",
    "#     num_entities=NUM_ENTITIES,\n",
    "#     num_relations=NUM_RELATIONS,\n",
    "#     embedding_size=EMBEDDING_SIZE,\n",
    "#     margin=MARGIN,\n",
    "#     random_state=SEED)\n",
    "#model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0],train2idx[:,1],train2idx[:,2],\n",
    "                                                trainexp2idx[:,:,0].reshape(-1),trainexp2idx[:,:,1].reshape(-1),\n",
    "                                                 trainexp2idx[:,:,2].reshape(-1))).batch(3)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_PADDING = 2\n",
    "# spouse_triples,spouse_traces = utils.parse_ttl(os.path.join('.','data','traces','spouse.ttl'),\n",
    "#                                               max_padding=MAX_PADDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, train_exp, test_exp = train_test_split(spouse_triples,\n",
    "#     spouse_traces,test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_entities = np.array([[spouse_traces[:,i,:][:,0],\n",
    "#     spouse_traces[:,i,:][:,2]] for i in range(MAX_PADDING)]).flatten()\n",
    "\n",
    "# exp_relations = np.array([spouse_traces[:,i,:][:,1] for i in range(MAX_PADDING)]).flatten()\n",
    "\n",
    "# entities = np.unique(np.concatenate([spouse_triples[:,0], spouse_triples[:,2], exp_entities],axis=0))\n",
    "# relations = np.unique(np.concatenate([spouse_triples[:,1], exp_relations],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_entities = len(entities)\n",
    "# num_relations = len(relations)\n",
    "\n",
    "# ent2idx = dict(zip(entities, range(num_entities)))\n",
    "# rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "# idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "# idx2rel = {idx:rel for rel,idx in rel2idx.items()}\n",
    "\n",
    "# train2idx = utils.array2idx(train,ent2idx,rel2idx)\n",
    "# test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "\n",
    "# trainexp2idx = utils.array2idx(train_exp,ent2idx,rel2idx)\n",
    "# testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0],train2idx[:,1],train2idx[:,2],\n",
    "#                                                  tuple((((traces2idx[:,i,:][:,0],traces2idx[:,i,:][:,1],\n",
    "#                                                           traces2idx[:,i,:][:,2]) for i in range(MAX_PADDING)))))).batch(2)\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0],train2idx[:,1],train2idx[:,2],\n",
    "#                                                 trainexp2idx[:,:,0],trainexp2idx[:,:,1],trainexp2idx[:,:,2])).batch(3)\n",
    "\n",
    "# for h,r,t,he,re,te in train_data:\n",
    "#     print(h,r,t,he,re,te)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size = 50\n",
    "# batch_size = 128\n",
    "# margin = 2\n",
    "# learning_rate = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "#model = transE.ExTransE(num_entities,num_relations,embedding_size,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_loss = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     for pos_head, rel, pos_tail, pos_head_exp,rel_exp, pos_tail_exp in train_data:\n",
    "\n",
    "#         neg_head, neg_tail = utils.get_negative_triples(\n",
    "#             head=pos_head, \n",
    "#             rel=rel, \n",
    "#             tail=pos_tail,\n",
    "#             num_entities=num_entities\n",
    "#             )\n",
    "\n",
    "#         neg_head_exp, neg_tail_exp = utils.get_negative_triples(\n",
    "#             head=pos_head_exp, \n",
    "#             rel=rel_exp, \n",
    "#             tail=pos_tail_exp,\n",
    "#             num_entities=num_entities\n",
    "#             )\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "\n",
    "#             pos_head_e, pos_tail_e, neg_head_e, neg_tail_e, rel_e = model([\n",
    "#                 pos_head,\n",
    "#                 pos_tail, \n",
    "#                 neg_head, \n",
    "#                 neg_tail, \n",
    "#                 rel\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#             pos_head_exp_e, pos_tail_exp_e, neg_head_exp_e, neg_tail_exp_e, rel_exp_e = model([\n",
    "#                 pos_head_exp,\n",
    "#                 pos_tail_exp, \n",
    "#                 neg_head_exp,  \n",
    "#                 neg_tail_exp, \n",
    "#                 rel_exp\n",
    "#                 ]\n",
    "#             )\n",
    "\n",
    "#             prediction_loss = transE.pred_loss(pos_head_e,pos_tail_e,neg_head_e,neg_tail_e,rel_e)\n",
    "#             #explain_loss = transE.exp_loss(pos_head_exp_e,pos_tail_exp_e,neg_head_exp_e, neg_tail_exp_e, rel_exp_e)\n",
    "            \n",
    "            \n",
    "#             #explain_loss = transE.exp_loss(pos_head_e,pos_tail_exp_e,pos_head_exp_e,pos_tail_exp_e,rel_e,rel_exp_e)\n",
    "#             explain_loss = transE.exp_loss(pos_head_exp_e,\n",
    "#                                            pos_tail_exp_e,\n",
    "#                                            neg_head_exp_e,\n",
    "#                                            neg_tail_exp_e,\n",
    "#                                            rel_exp_e,\n",
    "#                                            rel_exp_e)\n",
    "#             #print(f\"pred loss {prediction_loss}\")\n",
    "#             #print(f\"explain loss {explain_loss}\")\n",
    "#             total_loss = prediction_loss + explain_loss\n",
    "\n",
    "#         grads = tape.gradient(total_loss,model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "#     #if not epoch % 10:\n",
    "#     #print(f\"Loss at epoch {epoch}: {total_loss.numpy()} \")\n",
    "        \n",
    "#     epoch_loss.append(np.round(total_loss.numpy(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = utils.get_entity_embeddings(model)\n",
    "relation_embeddings = utils.get_relation_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 1\n",
    "pred_exp = []\n",
    "\n",
    "#testexp2idx[:,:,0].flatten()\n",
    "for i in range(len(testexp2idx)):\n",
    "\n",
    "    h_idx, r_idx, t_idx = test2idx[i]\n",
    "\n",
    "    triple_h_e = entity_embeddings[h_idx]\n",
    "    triple_r_e = relation_embeddings[r_idx]\n",
    "    triple_t_e = entity_embeddings[t_idx]\n",
    "\n",
    "    h_e = entity_embeddings[testexp2idx[:,:,0].flatten()]\n",
    "    r_e = relation_embeddings[testexp2idx[:,:,1].flatten()]\n",
    "    t_e = entity_embeddings[testexp2idx[:,:,2].flatten()]\n",
    "    \n",
    "    squared_diff = np.square(triple_h_e - h_e) + np.square(triple_r_e-r_e) + np.square(triple_t_e-t_e)\n",
    "\n",
    "    l2_dist = np.sqrt(np.sum(squared_diff,axis=1))\n",
    "\n",
    "    closest_l2 = np.argsort(l2_dist)[:top_k]\n",
    "\n",
    "    k_closest = testexp2idx[closest_l2]\n",
    "\n",
    "    pred_exp.append(k_closest) \n",
    "\n",
    "pred_exp = np.array(pred_exp).reshape(-1,top_k,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.square(triple_h_e - h_e).shape\n",
    "#testexp2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_ttl(file_name,max_padding):\n",
    "    \n",
    "#     lines = []\n",
    "\n",
    "#     with open(file_name, 'r') as f:\n",
    "#         for line in f:\n",
    "#             lines.append(line)\n",
    "\n",
    "#     ground_truth = []\n",
    "#     traces = []\n",
    "\n",
    "#     for idx in range(len(lines)):\n",
    "\n",
    "#         if \"graph us:construct\" in lines[idx]:\n",
    "\n",
    "#             source_tup = utils.get_tup(lines[idx+1])\n",
    "\n",
    "#         exp_triples = []\n",
    "\n",
    "#         if 'graph us:where' in lines[idx]:\n",
    "\n",
    "#             while lines[idx+1] != '} \\n':\n",
    "\n",
    "#                 exp_tup = utils.get_tup(lines[idx+1])\n",
    "#                 exp_triples.append(np.array(exp_tup))\n",
    "\n",
    "#                 idx+=1\n",
    "\n",
    "#         if len(source_tup) != 0 and len(exp_triples) != 0:\n",
    "            \n",
    "#             no_name_entity = False\n",
    "            \n",
    "#             if (\"no_name_entry\" in source_tup[0]) or (\"no_name_entry\" in source_tup[2]):\n",
    "#                 no_name_entity = True\n",
    "            \n",
    "#             for h,r,t in exp_triples:\n",
    "#                 if (\"no_name_entry\" in h) or (\"no_name_entry\" in t):\n",
    "#                     no_name_entity = True\n",
    "            \n",
    "#             if not no_name_entity:\n",
    "                \n",
    "#                 if len(exp_triples) < max_padding:\n",
    "                    \n",
    "#                     while len(exp_triples) != max_padding:\n",
    "                        \n",
    "#                         #pad = np.zeros((3))\n",
    "#                         #pad[:] = None\n",
    "#                         pad = np.array(['UNK_ENT', 'UNK_REL', 'UNK_ENT'])\n",
    "#                         exp_triples.append(pad)\n",
    "                        \n",
    "#                 ground_truth.append(np.array(source_tup))\n",
    "#                 traces.append(np.array(exp_triples))\n",
    "\n",
    "#     return np.array(ground_truth), np.array(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spouse_triples,spouse_traces = parse_ttl(os.path.join('.','data','traces','spouse.ttl'),max_padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jaccard_score(np.array([np.array(i) for i in spouse_triples]),np.array([np.array(i) for i in spouse_triples]))\n",
    "# X_train, X_test, train_exp, test_exp = train_test_split(spouse_triples,\n",
    "#     spouse_traces,test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.concatenate([X_train[:,0],X_train[:,2],train_exp[:,0][:,0],train_exp[:,0][:,2]],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_exp_entities = np.concatenate([train_exp[:,0][:,0],train_exp[:,0][:,2],train_exp[:,1][:,0],train_exp[:,1][:,2]])\n",
    "\n",
    "# train_exp_entities = np.array([i for i in train_exp_entities if i != '0.0'])\n",
    "# train_entities = np.unique(np.concatenate([X_train[:,0],X_train[:,2],train_exp_entities]))\n",
    "\n",
    "#update 2idx functions (array2idx,idx2train)\n",
    "# #update train data variable\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0], train2idx[:,1], train2idx[:,2],\n",
    "#                                                 exp2idx[:,0],exp2idx[:,1],exp2idx[:,2])).batch(batch_size)\n",
    "#train_data = tf.data.Dataset.from_tensor_slices(spouse_traces[:])\n",
    "\n",
    "#spouse_traces[:,0]\n",
    "\n",
    "#np.concatenate([spouse_triples[:,0], spouse_triples[:,2]],axis=0).shape\n",
    "\n",
    "# max_traces = 2\n",
    "# exp_entities = np.array([[spouse_traces[:,i,:][:,0],spouse_traces[:,i,:][:,2]] for i in range(max_traces)]).flatten()\n",
    "# #exp_entities = np.array([i for i in exp_entities if i != \"0.0\"])\n",
    "\n",
    "# exp_relations = np.array([spouse_traces[:,i][:,1] for i in range(max_traces)]).flatten()\n",
    "# #exp_relations = np.array([i for i in exp_relations if i != \"0.0\"])\n",
    "\n",
    "# entities = np.unique(np.concatenate([spouse_triples[:,0], spouse_triples[:,2], exp_entities],axis=0))\n",
    "# relations = np.unique(np.concatenate([spouse_triples[:,1], exp_relations],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_entities = len(entities)\n",
    "# num_relations = len(relations)\n",
    "\n",
    "# ent2idx = dict(zip(entities, range(num_entities)))\n",
    "# rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "# idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "# idx2rel = {idx:rel for rel,idx in rel2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train2idx = array2idx(spouse_triples,ent2idx,rel2idx)\n",
    "# train2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0],train2idx[:,1],train2idx[:,2],\n",
    "                                                 tuple((((traces2idx[:,i,:][:,0],traces2idx[:,i,:][:,1],\n",
    "                                                          traces2idx[:,i,:][:,2]) for i in range(2)))))).batch(2)\n",
    "\n",
    "#train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0],train2idx[:,1],train2idx[:,2],traces2idx)).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces2idx = array2idx(spouse_traces,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = tf.data.Dataset.from_tensor_slices((traces2idx[:,0,:][:,0],traces2idx[:,0,:][:,1],traces2idx[:,0,:][:,2])).batch(1)\n",
    "#tuple(((traces2idx[:,i,:][:,0],traces2idx[:,i,:][:,1],traces2idx[:,i,:][:,2]) for i in range(2)))\n",
    "\n",
    "# data = tf.data.Dataset.from_tensor_slices(traces2idx).batch(1)\n",
    "# for d in data:\n",
    "#     print(d)\n",
    "# data = tf.data.Dataset.from_tensor_slices(\n",
    "#     ).batch(2)\n",
    "\n",
    "# for d in data:\n",
    "#     print(d)\n",
    "#     break\n",
    "#     for i in d:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h,r,t,ex in train_data:\n",
    "    print('h',h,'r',r,'t',t)\n",
    "    exps = []\n",
    "    for h1,r1,t1 in ex:\n",
    "        #print(\"h ex\",h1,\"r ex\",r1,\"t ex\",t1)\n",
    "        exps.append((h1,r1,t1))\n",
    "    print(exps)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard_score(true_exp,pred_exp):\n",
    "\n",
    "#     scores = []\n",
    "\n",
    "#     for i in range(len(true_exp)):\n",
    "\n",
    "#         pred_i = pred_exp[i]\n",
    "#         true_i = true_exp[i]\n",
    "\n",
    "#         if isinstance(true_i,np.ndarray):\n",
    "#             num_true_traces = min(true_i.ndim,true_i.shape[0])\n",
    "\n",
    "#         elif isinstance(true_i,list):\n",
    "#             num_true_traces = len(true_i)\n",
    "\n",
    "#         if isinstance(pred_i,np.ndarray):\n",
    "#             num_pred_traces = min(pred_i.ndim,pred_i.shape[0])\n",
    "        \n",
    "#         elif isinstance(pred_i,list):\n",
    "#             num_pred_traces = len(pred_i)\n",
    "        \n",
    "#         bool_array = (pred_i == true_i).reshape(num_true_traces,3)\n",
    "\n",
    "#         count = 0\n",
    "\n",
    "#         for row in bool_array:\n",
    "#             if row.all():\n",
    "#                 count +=1\n",
    "\n",
    "#         score = count / (num_true_traces+num_pred_traces-count)\n",
    "\n",
    "#         scores.append(score)\n",
    "\n",
    "#     return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_exp = np.array([[[136,0,932],[136,0,932]],[[502,0,972],[972,0,502]]])\n",
    "# true_exp = traces2idx[0:2]\n",
    "# def jaccard_score(true_exp,pred_exp):\n",
    "\n",
    "#     assert len(true_exp) == len(pred_exp)\n",
    "\n",
    "#     scores = []\n",
    "\n",
    "#     for i in range(len(true_exp)):\n",
    "\n",
    "#         true_i = true_exp[i][true_exp[i] != np.array([-1, -1, -1])].reshape(-1,3)    \n",
    "#         pred_i = pred_exp[i]#[pred_exp[i] != np.array([-1, -1, -1])].reshape(-1,3)    \n",
    "\n",
    "#         num_true_traces = true_i.shape[0]\n",
    "#         num_pred_traces = pred_i.shape[0]\n",
    "\n",
    "#         if num_true_traces < num_pred_traces:\n",
    "\n",
    "#             pred_i = pred_i[:num_true_traces]\n",
    "#             num_pred_traces = pred_i.shape[0]\n",
    "\n",
    "#         bool_array = (pred_i == true_i)\n",
    "\n",
    "#         count = 0\n",
    "#         for row in bool_array:\n",
    "#             if row.all():\n",
    "#                 count += 1\n",
    "\n",
    "#         score = count / (num_true_traces + num_pred_traces-count)\n",
    "\n",
    "#         scores.append(score)\n",
    "        \n",
    "#     return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    #print(pred_i)\n",
    "    \n",
    "#     pred_i = pred_exp[i][pred_exp[i] != np.array([-1, -1, -1])].reshape(-1,3)\n",
    "\n",
    "#     num_true_traces = true_i.shape[0]\n",
    "#     num_pred_traces = pred_i.shape[0]\n",
    "    \n",
    "#     print(pred_i == true_i)\n",
    "    \n",
    "#     count = 0\n",
    "    \n",
    "#     for row in bool_array:\n",
    "#         if row.all():\n",
    "#             count +=1\n",
    "#     print(count, num_true_traces, num_pred_traces)        \n",
    "#     score = count / (num_true_traces+num_pred_traces - count)\n",
    "    \n",
    "#     scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2idx(dataset, ent2idx,rel2idx):\n",
    "    \n",
    "    if dataset.ndim == 2:\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        for head, rel, tail in dataset:\n",
    "            \n",
    "            head_idx = ent2idx[head]\n",
    "            tail_idx = ent2idx[tail]\n",
    "            rel_idx = rel2idx[rel]\n",
    "            \n",
    "            data.append((head_idx, rel_idx, tail_idx))\n",
    "\n",
    "        data = np.array(data)\n",
    "\n",
    "    elif dataset.ndim == 3:\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        for i in range(len(dataset)):\n",
    "            \n",
    "            temp_array = []\n",
    "        \n",
    "            for head,rel,tail in dataset[i,:,:]:\n",
    "#                 if (head == '0.0') or (tail == '0.0') or (rel == '0.0'):\n",
    "#                     temp_array.append((-1,-1,-1))\n",
    "#                     continue\n",
    "\n",
    "                head_idx = ent2idx[head]\n",
    "                tail_idx = ent2idx[tail]\n",
    "                rel_idx = rel2idx[rel]\n",
    "\n",
    "                temp_array.append((head_idx,rel_idx,tail_idx))\n",
    "\n",
    "            data.append(temp_array)\n",
    "            \n",
    "        data = np.array(data).reshape(-1,dataset.shape[1],3)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_traces = spouse_traces.shape[1]\n",
    "\n",
    "# data = []\n",
    "# #spouse_traces[-1,:,:]\n",
    "\n",
    "# #for i in range(len(spouse_traces)):\n",
    "# for i in range(2):\n",
    "#     temp_array = []\n",
    "#     for head,rel,tail in spouse_traces[i,:,:]:\n",
    "        \n",
    "#         if (head == '0.0') or (tail == '0.0') or (rel == '0.0'):\n",
    "#             temp_array.append((-1,-1,-1))\n",
    "#             continue\n",
    "            \n",
    "#         head_idx = ent2idx[head]\n",
    "#         tail_idx = ent2idx[tail]\n",
    "#         rel_idx = rel2idx[rel]\n",
    "        \n",
    "#         temp_array.append((head_idx,rel_idx,tail_idx))\n",
    "        \n",
    "#     data.append(temp_array)\n",
    "\n",
    "# for i in range(num_traces):\n",
    "\n",
    "#     temp_array = []\n",
    "\n",
    "#     for head,rel,tail in spouse_traces[:,i]:\n",
    "\n",
    "#         if head == '0.0' or tail == '0.0':\n",
    "#             temp_array.append((-1,-1,-1))\n",
    "#             continue\n",
    "\n",
    "#         head_idx = ent2idx[head]\n",
    "#         tail_idx = ent2idx[tail]\n",
    "#         rel_idx = rel2idx[rel]\n",
    "\n",
    "#         temp_array.append((head_idx,rel_idx,tail_idx))\n",
    "\n",
    "#     data.append(temp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lines = []\n",
    "\n",
    "# with open(os.path.join('.' ,'data','traces','spouse.ttl'), 'r') as f:\n",
    "#     for line in f:\n",
    "#         lines.append(line)\n",
    "        \n",
    "# ground_truth = []\n",
    "# traces = []\n",
    "\n",
    "# for idx in range(len(lines)):\n",
    "    \n",
    "#     if \"graph us:construct\" in lines[idx]:\n",
    "\n",
    "#         source_tup = utils.get_tup(lines[idx+1])            \n",
    "    \n",
    "#     exp_triples = []\n",
    "    \n",
    "#     if 'graph us:where' in lines[idx]:\n",
    "        \n",
    "#         while lines[idx+1] != '} \\n':\n",
    "#             exp_tup = utils.get_tup(lines[idx+1])\n",
    "#             exp_triples.append(exp_tup)\n",
    "\n",
    "#             idx+=1\n",
    "        \n",
    "#     if len(source_tup) != 0 and len(exp_triples) != 0:\n",
    "        \n",
    "#         no_name_entity = False\n",
    "        \n",
    "#         if (\"no_name_entry\" in source_tup[0]) or (\"no_name_entry\" in source_tup[2]):\n",
    "#             no_name_entity = True\n",
    "        \n",
    "#         for h,r,t in exp_triples:\n",
    "#             if (\"no_name_entry\" in h) or (\"no_name_entry\" in t):\n",
    "#                 no_name_entity = True\n",
    "        \n",
    "#         if not no_name_entity:\n",
    "#             ground_truth.append(source_tup)\n",
    "#             traces.append(exp_triples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#[tf.convert_to_tensor(i) for i in spouse_traces]\n",
    "#tf.convert_to_tensor(spouse_traces[0])\n",
    "# for i in spouse_traces:\n",
    "#     print(i.shape)\n",
    "# for i in range(2):\n",
    "    \n",
    "#     if (tf.convert_to_tensor(spouse_traces)[1][i]).all():\n",
    "#         print(tf.convert_to_tensor(spouse_traces)[1][i])\n",
    "\n",
    "# for t1,t2 in spouse_traces:\n",
    "    \n",
    "    \n",
    "#     e1,r1,e2 = t1\n",
    "#     e3,r2,e4 = t2\n",
    "\n",
    "#     print(e1,r1,e2)\n",
    "#     print(e3,r2,e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# traces = defaultdict(list)\n",
    "\n",
    "# for idx in range(len(lines)):\n",
    "        \n",
    "#     if \"graph us:construct\" in lines[idx] and 'dbe' in lines[idx+1]:\n",
    "\n",
    "#         source_tup = utils.get_tup(lines[idx+1])\n",
    "                 \n",
    "#         assert len(source_tup) == 3\n",
    "        \n",
    "#         traces[source_tup] = []\n",
    "\n",
    "# for i,j in traces.items():\n",
    "    \n",
    "#     i = eval(i)\n",
    "\n",
    "#     if (i[0] != j[0][2]) or (i[2] != j[0][0]):\n",
    "#         print(i,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in truth:\n",
    "#     if i not in all_spouses:\n",
    "#         print(i)\n",
    "# for i,j,k in all_spouses:\n",
    "#     if i == 'Elena_Cuza' or k =='Elena_Cuza':\n",
    "#         print(i,j,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "# traces = defaultdict(list)\n",
    "\n",
    "# for idx,line in enumerate(lines[0:100]):\n",
    "#     print(line)\n",
    "#     if ('dbe' in line or 'dbo' in line):\n",
    "#         #print(line)\n",
    "#         source_tup = utils.get_tup(line)\n",
    "        \n",
    "#         traces[source_tup] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
