{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from rdflib import Graph\n",
    "import utils\n",
    "import transE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb15k_237 = np.load('./data/fb15k_237.npz', allow_pickle=True)\n",
    "# fb_train = fb15k_237['train']\n",
    "# fb_valid = fb15k_237['valid']\n",
    "# fb_test = fb15k_237['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "data = np.load(os.path.join('.','data','royalty.npz'))\n",
    "\n",
    "train = data['X_train']\n",
    "test = data['X_test']\n",
    "\n",
    "train_exp = data['train_exp']\n",
    "test_exp = data['test_exp']\n",
    "\n",
    "#full_train = np.concatenate((train,train_exp), axis=0)\n",
    "\n",
    "entities = data['entities'].tolist()\n",
    "relations = data['relations'].tolist()\n",
    "\n",
    "num_entities = len(entities)\n",
    "num_relations = len(relations)\n",
    "\n",
    "ent2idx = dict(zip(entities, range(num_entities)))\n",
    "rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "# idx2ent = dict(zip(range(num_entities),entities))\n",
    "# idx2rel = dict(zip(range(num_relations),relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2idx = utils.array2idx(train,ent2idx,rel2idx)\n",
    "exp2idx = utils.array2idx(train_exp,ent2idx,rel2idx)\n",
    "\n",
    "test2idx = utils.array2idx(test,ent2idx,rel2idx)\n",
    "testexp2idx = utils.array2idx(test_exp,ent2idx,rel2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "batch_size = 3\n",
    "num_epochs = 2\n",
    "margin = 2\n",
    "learning_rate = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transE.transE(num_entities,num_relations,embedding_size,random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0], train2idx[:,1], train2idx[:,2],\n",
    "                                                exp2idx[:,0],exp2idx[:,1],exp2idx[:,2])).batch(batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 18.410451889038086 \n",
      "Loss at epoch 1: 10.564764022827148 \n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for pos_head, rel, pos_tail, pos_head_exp,rel_exp, pos_tail_exp in train_data:\n",
    "\n",
    "        neg_head, neg_tail = utils.get_negative_triples(\n",
    "            head=pos_head, \n",
    "            rel=rel, \n",
    "            tail=pos_tail,\n",
    "            num_entities=num_entities,\n",
    "            random_state=SEED\n",
    "            )\n",
    "\n",
    "        neg_head_exp, neg_tail_exp = utils.get_negative_triples(\n",
    "            head=pos_head_exp, \n",
    "            rel=rel_exp, \n",
    "            tail=pos_tail_exp,\n",
    "            num_entities=num_entities,\n",
    "            random_state=SEED\n",
    "            )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            pos_head_e, pos_tail_e, neg_head_e, neg_tail_e, rel_e = model([\n",
    "                pos_head,\n",
    "                pos_tail, \n",
    "                neg_head, \n",
    "                neg_tail, \n",
    "                rel\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            pos_head_exp_e, pos_tail_exp_e, neg_head_exp_e, neg_tail_exp_e, rel_exp_e = model([\n",
    "                pos_head_exp,\n",
    "                pos_tail_exp,\n",
    "                neg_head_exp,  \n",
    "                neg_tail_exp, \n",
    "                rel_exp\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            prediction_loss = transE.pred_loss(pos_head_e,pos_tail_e,neg_head_e,neg_tail_e,rel_e)\n",
    "            #explain_loss = transE.exp_loss(pos_head_exp_e,pos_tail_exp_e,neg_head_exp_e, neg_tail_exp_e, rel_exp_e)\n",
    "            explain_loss = transE.exp_loss(pos_head_e,pos_tail_exp_e,pos_head_exp_e,pos_tail_exp_e,rel_e,rel_exp_e)\n",
    "            \n",
    "            total_loss = prediction_loss + explain_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "    #if not epoch % 10:\n",
    "    print(f\"Loss at epoch {epoch}: {total_loss.numpy()} \")\n",
    "        \n",
    "    epoch_loss.append(np.round(total_loss.numpy(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = utils.get_entity_embeddings(model)\n",
    "relation_embeddings = utils.get_relation_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1403,    0, 1460])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1403,    0, 1460],\n",
       "       [ 531,    0, 1219]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def exp_score(triple,k,data,entity_embeddings,relation_embeddings):\n",
    "    \n",
    "#     triple_h_e = entity_embeddings[triple[0]]\n",
    "#     triple_r_e = relation_embeddings[triple[1]]\n",
    "#     triple_t_e = entity_embeddings[triple[2]]\n",
    "\n",
    "#     h_e = entity_embeddings[data[:,0]]\n",
    "#     r_e = relation_embeddings[data[:,1]]\n",
    "#     t_e = entity_embeddings[data[:,2]]\n",
    "\n",
    "#     squared_diff = np.square(triple_h_e - h_e) + np.square(triple_r_e-r_e) + np.square(triple_t_e-t_e)\n",
    "\n",
    "#     l2_dist = np.sqrt(np.sum(squared_diff,axis=1))\n",
    "\n",
    "#     closest_l2 = np.argsort(l2_dist)[:k]\n",
    "    \n",
    "#     return data[closest_l2]\n",
    "\n",
    "transE.exp_score(test2idx[0],k=2,data=test2idx,entity_embeddings=entity_embeddings,relation_embeddings=relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transE.exp_loss(pos_head_e,pos_tail_e,pos_head_exp_e,pos_tail_exp_e,rel_e,rel_exp_e)\n",
    "\n",
    "# triple = (pos_head_e[0], rel_e[0], pos_tail_e[0])\n",
    "\n",
    "# triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data = np.load('./data/human_data.npz')\n",
    "# train = data['X']\n",
    "\n",
    "# g=Graph()\n",
    "# g.parse(\"../CORESE-DATA/human-data.rdf\", format=\"xml\")\n",
    "\n",
    "# triples = []\n",
    "\n",
    "# for i,j,k in g:\n",
    "    \n",
    "#     head = str(i).split('#')\n",
    "#     rel = str(j).split('#')\n",
    "#     tail = str(k).split('#')\n",
    "    \n",
    "#     if head[0] == 'http://www.inria.fr/2015/humans-instances':\n",
    "        \n",
    "#         triples.append((head[-1], rel[-1], tail[-1]))\n",
    "\n",
    "# triples = [('Eve', 'type', 'Lecturer'),\n",
    "#            #('Eve', 'type', 'Person'), \n",
    "#            ('Lecturer', 'subClassOf', 'Person'), \n",
    "#            #('David', 'type', 'Person'),\n",
    "#            ('David', 'type', 'Researcher'),\n",
    "#            ('Researcher', 'subClassOf', 'Person'),\n",
    "#            ('Flora', 'hasSpouse', 'Gaston'),\n",
    "#            ('Gaston', 'type', 'Person'),\n",
    "#            #('Flora', 'type', 'Person'),\n",
    "#           ]\n",
    "# g=Graph()\n",
    "# g.parse(\"/Users/nhalliwe/Desktop/CORESE-DATA/human-data.rdf\", format=\"xml\")\n",
    "# triples = []\n",
    "\n",
    "# for i,j,k in g:\n",
    "    \n",
    "#     head = str(i).split('#')\n",
    "#     rel = str(j).split('#')\n",
    "#     tail = str(k).split('#')\n",
    "    \n",
    "#     if head[0] == 'http://www.inria.fr/2015/humans-instances':\n",
    "        \n",
    "#         triples.append((head[-1], rel[-1], tail[-1]))\n",
    "\n",
    "# train = np.array(triples)\n",
    "# traces = utils.parse_traces(file_name='../traces/entailment.ttl')\n",
    "# exp_triples = utils.get_exp_triples(train,traces)\n",
    "\n",
    "# entities = np.unique(np.concatenate((train[:,0], train[:,2], exp_triples[:,0], exp_triples[:,2]), axis=0)).tolist()\n",
    "# relations = np.unique(np.concatenate((train[:,1], exp_triples[:,1])), axis=0).tolist()\n",
    "\n",
    "# num_entities = len(entities)\n",
    "# num_relations = len(relations)\n",
    "\n",
    "# ent2idx = dict(zip(entities, range(num_entities)))\n",
    "# rel2idx = dict(zip(relations, range(num_relations)))\n",
    "\n",
    "# idx2ent = {idx:ent for ent,idx in ent2idx.items()}\n",
    "# idx2rel = {idx:rel for rel,idx in rel2idx.items()}\n",
    "\n",
    "# train2idx = utils.train2idx(train,ent2idx,rel2idx)\n",
    "# idx2train = utils.idx2train(train2idx,idx2ent,idx2rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train:\n",
    "#     if tuple(i) in traces:\n",
    "#         print(tuple(i),traces[tuple(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h,r,t in traces[('Eve', 'type', 'Person')]:\n",
    "#     print(ent2idx[h],rel2idx[r],ent2idx[t])\n",
    "\n",
    "\n",
    "        \n",
    "        #exp_triples.append(list(traces[tuple(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = tf.convert_to_tensor([6,4])\n",
    "#tf.concat([tf.gather_nd(train2idx,tf.where(train2idx[:,0] == idx)), tf.gather_nd(train2idx,tf.where(train2idx[:,2] == idx))], axis=0)\n",
    "\n",
    "#tf.where(train2idx[:,0]==idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = np.zeros(shape=(num_entities,num_entities,num_relations))\n",
    "\n",
    "# for h,r,t in train:\n",
    "    \n",
    "#     h_idx = entities.index(h)\n",
    "#     r_idx = relations.index(r)\n",
    "#     t_idx = entities.index(t)\n",
    "    \n",
    "#     A[h_idx,t_idx,r_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## valid2idx = []\n",
    "\n",
    "# for head, rel, tail in valid:\n",
    "    \n",
    "#     head_idx = ent2idx[head]\n",
    "#     tail_idx = ent2idx[tail]\n",
    "#     rel_idx = rel2idx[rel]\n",
    "\n",
    "#     valid2idx.append([head_idx, rel_idx, tail_idx])\n",
    "    \n",
    "# valid2idx = np.array(valid2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transE\n",
    "# EMBEDDING_SIZE = 30\n",
    "# BATCH_SIZE = 2\n",
    "# NUM_EPOCHS = 200\n",
    "# MARGIN = 2\n",
    "# SQRT_SIZE = 6 / np.sqrt(EMBEDDING_SIZE)\n",
    "\n",
    "# model = transE.build_model(\n",
    "#     embedding_size=EMBEDDING_SIZE,\n",
    "#     num_entities=num_entities,\n",
    "#     num_relations=num_relations,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     num_epochs=NUM_EPOCHS,\n",
    "#     margin=MARGIN,\n",
    "#     sqrt_size=SQRT_SIZE,\n",
    "#     seed=SEED\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex\n",
    "# EMBEDDING_SIZE = 30\n",
    "# BATCH_SIZE = 3\n",
    "# NUM_EPOCHS = 200\n",
    "# MARGIN = 1\n",
    "# SQRT_SIZE = 6 / np.sqrt(EMBEDDING_SIZE)\n",
    "\n",
    "# real_head_input = tf.keras.layers.Input(shape=(1,), name='real_head_input')\n",
    "# img_head_input = tf.keras.layers.Input(shape=(1,), name='img_head_input')\n",
    "# real_tail_input = tf.keras.layers.Input(shape=(1,), name='real_tail_input')\n",
    "# img_tail_input = tf.keras.layers.Input(shape=(1,), name='img_tail_input')\n",
    "# real_rel_input = tf.keras.layers.Input(shape=(1,), name='real_rel_input')\n",
    "# img_rel_input = tf.keras.layers.Input(shape=(1,), name='img_rel_input')\n",
    "\n",
    "# real_entity_embeddings = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_entities,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='real_entity_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED))\n",
    "#     )\n",
    "\n",
    "# img_entity_embeddings = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_entities,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='img_entity_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED))\n",
    "#     )\n",
    "\n",
    "# real_relation_embedding = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_relations,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='real_relation_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED)),\n",
    "#     )\n",
    "\n",
    "# img_relation_embedding = tf.keras.layers.Embedding(\n",
    "#     input_dim=num_relations,\n",
    "#     output_dim=EMBEDDING_SIZE,\n",
    "#     name='img_relation_embeddings',\n",
    "#     embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-SQRT_SIZE, maxval=SQRT_SIZE, \n",
    "#                                                                seed=tf.random.set_seed(SEED)),\n",
    "#     )\n",
    "\n",
    "# real_head = real_entity_embeddings(real_head_input)\n",
    "# img_head = img_entity_embeddings(img_head_input)\n",
    "# real_tail = real_entity_embeddings(real_tail_input)\n",
    "# img_tail = img_entity_embeddings(img_tail_input)\n",
    "# real_rel = real_relation_embedding(real_rel_input)\n",
    "# img_rel = real_relation_embedding(img_rel_input)\n",
    "\n",
    "# model = tf.keras.models.Model(\n",
    "#     inputs=[\n",
    "#         real_head_input,\n",
    "#         img_head_input, \n",
    "#         real_tail_input, \n",
    "#         img_tail_input, \n",
    "#         real_rel_input,\n",
    "#         img_rel_input\n",
    "#         ], \n",
    "#     outputs=[\n",
    "#         real_head,\n",
    "#         img_head, \n",
    "#         real_tail, \n",
    "#         img_tail, \n",
    "#         real_rel,\n",
    "#         img_rel\n",
    "#         ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_negative_triples(head, rel, tail, num_entities, seed):\n",
    "    \n",
    "#     cond = tf.random.uniform(head.shape, 0, 2, dtype=tf.int64, seed=seed) #1 means keep entity\n",
    "#     rnd = tf.random.uniform(head.shape, 0, num_entities-1, dtype=tf.int64, seed=seed)\n",
    "    \n",
    "#     neg_head = tf.where(cond == 1, head, rnd)\n",
    "#     neg_tail = tf.where(cond == 1, rnd, tail)   \n",
    "    \n",
    "#     return neg_head, neg_tail\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((train2idx[:,0], train2idx[:,1], train2idx[:,2])).batch(BATCH_SIZE)\n",
    "# #train_data = train_data.shuffle(buffer_size=50000, seed=tf.random.set_seed(SEED)).batch(BATCH_SIZE)\n",
    "\n",
    "# #exp_decay = tf.keras.optimizers.schedules.ExponentialDecay(.01, 1000, .05)\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score(h,r,t):\n",
    "#     return tf.reduce_sum(tf.square(h + r - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "#     for head, rel, tail in train_data:\n",
    "                \n",
    "#         neg_head, neg_tail = get_negative_triples(head, rel, tail,seed=tf.random.set_seed(SEED))\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "            \n",
    "#             real_head_e,img_head_e, real_tail_e, img_tail_e, real_rel_e,img_rel_e = model([head, \n",
    "#                                                                            neg_head, tail, neg_tail, rel, rel])\n",
    "\n",
    "            \n",
    "#             dot1 = tf.reduce_sum(tf.multiply(real_rel_e, tf.multiply(real_head_e, real_tail_e)),1)\n",
    "#             dot2 = tf.reduce_sum(tf.multiply(real_rel_e, tf.multiply(img_head_e, img_tail_e)),1)\n",
    "#             dot3 = tf.reduce_sum(tf.multiply(img_rel_e, tf.multiply(real_head_e, img_tail_e)),1)\n",
    "#             dot4 = tf.reduce_sum(tf.multiply(img_rel_e, tf.multiply(img_head_e, real_tail_e)),1)\n",
    "            \n",
    "#             embedding_loss = tf.reduce_sum(dot1+dot2+dot3-dot4)\n",
    "\n",
    "#         grads = tape.gradient(embedding_loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "#     if not epoch % 10:\n",
    "        \n",
    "#         print('Current loss' , embedding_loss.numpy(),'at epoch', epoch)\n",
    "    \n",
    "#     losses.append(embedding_loss.numpy())\n",
    "#train2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for head, rel, tail in train_data:\n",
    "#     print(head, rel, tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "#     for head, rel, tail in train_data:\n",
    "                \n",
    "#         neg_head, neg_tail = utils.get_negative_triples(head, rel, tail,num_entities=num_entities,seed=SEED)\n",
    "        \n",
    "#         with tf.GradientTape() as tape:\n",
    "                        \n",
    "# #             pos_head_e,neg_head_e, pos_tail_e, neg_tail_e, rel_e= model([head, \n",
    "# #                                                                            neg_head, tail, neg_tail, rel])\n",
    "            \n",
    "#             pos_head_0,neg_head_0, pos_tail_0, neg_tail_0, rel_0 = model([head[0], \n",
    "#                                                                           neg_head[0], tail[0], \n",
    "#                                                                           neg_tail[0], rel[0]])\n",
    "            \n",
    "#             pos_head_1,neg_head_1, pos_tail_1, neg_tail_1, rel_1 = model([head[1], \n",
    "#                                                                           neg_head[1], tail[1], neg_tail[1], rel[1]])\n",
    "            \n",
    "# #             pos_head_2,neg_head_2, pos_tail_2, neg_tail_2, rel_2 = model([head[2], \n",
    "# #                                                                           neg_head[2], tail[2], neg_tail[2], rel[2]])\n",
    "            \n",
    "#             #pos = score(pos_head_0,rel_0, pos_tail_0) + \\\n",
    "#             #score(pos_head_1,rel_1, pos_tail_1) #+score(pos_head_2,rel_2, pos_tail_2)\n",
    "            \n",
    "#             #neg = score(neg_head_0,rel_0, neg_tail_0) + \\\n",
    "#             #score(neg_head_1,rel_1, neg_tail_1) #+\\score(neg_head_2,rel_2, neg_tail_2)\n",
    "            \n",
    "            \n",
    "#             pos = tf.reduce_sum(tf.square(pos_head_e + rel_e - pos_tail_e), axis=1)\n",
    "#             neg = tf.reduce_sum(tf.square(neg_head_e + rel_e - neg_tail_e), axis=1)    \n",
    "            \n",
    "#             embedding_loss = tf.reduce_sum(tf.maximum(pos - neg + MARGIN, 0))\n",
    "#             #pred_loss = tf.reduce_sum(tf.maximum(pos - neg + MARGIN, 0))\n",
    "            \n",
    " \n",
    "#         grads = tape.gradient(embedding_loss, model.trainable_variables)\n",
    "#         optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "#     if not epoch % 10:\n",
    "        \n",
    "#         print('Current loss' , embedding_loss.numpy(),'at epoch', epoch)\n",
    "    \n",
    "#     losses.append(embedding_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_names = np.concatenate((entities,relations))\n",
    "\n",
    "# all_entities = model.get_layer('entity_embeddings').get_weights()[0]\n",
    "# all_relations = model.get_layer('relation_embeddings').get_weights()[0]\n",
    "\n",
    "# all_embeddings = np.concatenate((all_entities,all_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# np.load(os.path.join('.','data','transE_embeddings.npz'))['entity_embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_2d = PCA(n_components=2, random_state=SEED).fit_transform(all_embeddings)\n",
    "# fig, ax = plt.subplots(figsize=(12,12))\n",
    "# ax.scatter(embeddings_2d[:,0], embeddings_2d[:,1])\n",
    "# for i, txt in enumerate(all_names):\n",
    "#     ax.annotate(txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_names = list(all_names)\n",
    "# all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_head = 'Eve'\n",
    "# true_tail = 'Person'\n",
    "\n",
    "# head_idx = all_names.index(true_head)\n",
    "# tail_idx = all_names.index(true_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_score = -100000000\n",
    "# min_idx = -100000000\n",
    "\n",
    "# for rel in relations:\n",
    "    \n",
    "#     rel_idx = all_names.index(rel)\n",
    "    \n",
    "#     current_score = -score(all_embeddings[head_idx],\n",
    "#                            all_embeddings[rel_idx],\n",
    "#                            all_embeddings[tail_idx]).numpy()\n",
    "    \n",
    "#     if current_score > min_score:\n",
    "#         min_score = current_score\n",
    "#         min_idx = rel_idx\n",
    "        \n",
    "#     print(all_names[rel_idx], current_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_head, all_names[min_idx], true_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for h,r,t in train:\n",
    "    \n",
    "#     h_idx = all_names.index(h)\n",
    "#     r_idx = all_names.index(r)\n",
    "#     t_idx = all_names.index(t)\n",
    "    \n",
    "#     print(h,r,t,-score(all_embeddings[h_idx], all_embeddings[r_idx], all_embeddings[t_idx]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def closest_l2(source_head, source_rel, source_tail,k, data, entity_rel_names):\n",
    "    \n",
    "#     for h,r,t in data:\n",
    "        \n",
    "#         h_idx = entity_rel_names.index(h)\n",
    "#         r_idx = entity_rel_names.index(r)\n",
    "#         t_idx = entity_rel_names.index(t)\n",
    "    \n",
    "#     l2 = np.sqrt(np.sum((source_head - all_embeddings[h_idx])**2 + (rel- all_embeddings[r_idx])**2\n",
    "#                 + (person-all_embeddings[t_idx])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find closest l2 triple\n",
    "# eve = all_embeddings[head_idx]\n",
    "# person = all_embeddings[tail_idx]\n",
    "# rel = all_embeddings[min_idx]\n",
    "# scores = []\n",
    "\n",
    "# for h,r,t in train:\n",
    "    \n",
    "#     h_idx = all_names.index(h)\n",
    "#     r_idx = all_names.index(r)\n",
    "#     t_idx = all_names.index(t)\n",
    "    \n",
    "#     l2 = np.sqrt(np.sum((eve - all_embeddings[h_idx])**2 + (rel- all_embeddings[r_idx])**2\n",
    "#                 + (person-all_embeddings[t_idx])**2))\n",
    "#     scores.append(((h,r,t), l2))\n",
    "#     print(h,r,t,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace = set([('Eve', 'type', 'Lecturer'), ('Lecturer', 'subClassOf', 'Person')])\n",
    "# gen = set([('Eve', 'type', 'Lecturer')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(scores, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eve = all_embeddings[all_names.index('Eve')]\n",
    "# person = all_embeddings[all_names.index('Person')]\n",
    "# rel = all_embeddings[all_names.index('type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head_idx, rel_idx, tail_idx = train2idx[0]\n",
    "\n",
    "# def get_grad(head_idx, tail_idx, rel_idx, A):\n",
    "    \n",
    "#     with tf.GradientTape(persistent=True) as g:\n",
    "\n",
    "# #         head_idx = tf.convert_to_tensor(head_idx)\n",
    "# #         tail_idx = tf.convert_to_tensor(tail_idx)\n",
    "# #         rel_idx = tf.convert_to_tensor(rel_idx)    \n",
    "#         A = tf.convert_to_tensor(A)\n",
    "\n",
    "#         head = tf.argmax(A[:,tail_idx, rel_idx])\n",
    "#         tail = tf.argmax(A[head_idx,:, rel_idx])\n",
    "#         rel = tf.argmax(A[head_idx,tail_idx,:])\n",
    " \n",
    "#         head_e,_,tail_e,_,rel_e= model([head,head,tail,tail,rel])\n",
    "\n",
    "#         get_score = score(head_e,rel_e,tail_e)\n",
    "        \n",
    "        \n",
    "#     nabla = g.gradient(get_score, head_e)\n",
    "#     return nabla\n",
    "#get_grad(head_idx, tail_idx, rel_idx, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('entity2wikidata.json','r') as f:\n",
    "    \n",
    "#     entities_dict = json.load(f)\n",
    "\n",
    "# for k, d in entities_dict.items():\n",
    "    \n",
    "#     if 'France' in d['label']:\n",
    "        \n",
    "#         print(k)\n",
    "\n",
    "# embeddings = model.get_layer('entity_embeddings').get_weights()[0]\n",
    "# relations = model.get_layer('relation_embeddings').get_weights()[0]\n",
    "\n",
    "# paris = '/m/05qtj'\n",
    "# france = '/m/0f8l9c'\n",
    "\n",
    "# paris_idx = ent2idx[paris]\n",
    "# france_idx = ent2idx[france]\n",
    "# capital = rel2idx['/location/country/capital']\n",
    "\n",
    "# head, tail = embeddings[[paris_idx, france_idx], :]\n",
    "# rel = relations[capital]\n",
    "\n",
    "# #-np.linalg.norm((head+rel - tail),ord=2)\n",
    "\n",
    "# scores = []\n",
    "\n",
    "# for i in range(len(relations)):\n",
    "    \n",
    "#     temp_rel = relations[i]\n",
    "    \n",
    "#     score = -np.linalg.norm((head+temp_rel - tail),ord=2)\n",
    "    \n",
    "#     scores.append(score)\n",
    "\n",
    "# idx2rel[np.argmax(scores)]\n",
    "\n",
    "# for i in np.argsort(scores)[-10:]:\n",
    "    \n",
    "#     print(idx2rel[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
